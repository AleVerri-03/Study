\documentclass[8pt, a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} 
\usepackage{amsmath, amssymb, amsfonts, amsthm} 
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{graphicx} 
\usepackage{hyperref} 
\usepackage{xcolor}
\usepackage{listings}
\usepackage{bm} % For bold math symbols


\theoremstyle{plain} 
\newtheorem{theorem}{Theorem}[section] % Numbered by section
\newtheorem{property}[theorem]{Property} % Shares numbering with theorem

% Style for Definitions (Upright/Roman text)
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition} % Shares numbering with theorem

% --- Python Code Highlighting Configuration ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstset{
    language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\small,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single
}

% --- Machine Learning Notation Commands ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\grad}{\nabla}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\renewcommand{\vec}[1]{\mathbf{#1}} % Bold vectors
\newcommand{\mat}[1]{\mathbf{#1}}  % Bold matrices

\title{Numerical Analysis for Machine Learning: \\ Complete Lecture Notes}
\author{}
\date{}

\begin{document}
\maketitle

\tableofcontents

\chapter{Lesson 23-09}

\section{Fundamental Problems in Linear Algebra for Machine Learning}
Linear algebra, and more specifically numerical linear algebra, forms the foundation for many algorithms in machine learning and data science. We will explore several core families of problems that are particularly relevant in these fields.

The primary topics to be covered include:
\begin{enumerate}
    \item \textbf{Linear Systems}: The problem of solving equations of the form $A\mathbf{x} = \mathbf{b}$. Key questions involve the conditions for solvability, which are intrinsically linked to the fundamental vector spaces associated with the matrix $A$.

    \item \textbf{Eigenvalue Decomposition}: For a square matrix $A$, finding the scalars $\lambda$ and vectors $\mathbf{x}$ that satisfy the equation $A\mathbf{x} = \lambda\mathbf{x}$. This decomposition is crucial for understanding the behavior of linear transformations and is used in algorithms like Principal Component Analysis (PCA).

    \item \textbf{Singular Value Decomposition (SVD)}: A powerful generalization of eigenvalue decomposition, expressed as $A\mathbf{v} = \sigma\mathbf{u}$. SVD is arguably the most important matrix decomposition for machine learning, applicable to any rectangular matrix and essential for tasks like dimensionality reduction and recommendation systems.

    \item \textbf{Minimization}: Particularly the method of \textbf{Least Squares (ls)}, which provides a way to find approximate solutions to overdetermined linear systems. This is a fundamental regression technique.

    \item \textbf{Matrix Factorizations}: General methods for decomposing a matrix into a product of simpler matrices (e.g., LU, QR), which are instrumental in developing efficient numerical algorithms.
\end{enumerate}

\section{The Data Matrix: A Linear Algebra View of Data}
The central role of linear algebra in data science stems from the representation of datasets as matrices. A standard dataset can be organized into a data matrix $A$ of size $m \times n$.
\begin{itemize}
    \item $m$: The number of rows, corresponding to the number of \textbf{samples} or observations in the dataset.
    \item $n$: The number of columns, corresponding to the number of \textbf{features} or variables measured for each sample.
\end{itemize}
For instance, a dataset of $m=1000$ grayscale images, each of size $1000 \times 1000$ pixels, can be represented as a matrix where each row is a flattened image. In this case, the number of features $n$ would be $1000 \times 1000 = 1,000,000$. The matrix $A$ would thus be of size $1000 \times 1,000,000$. Even categorical data can be converted into a numerical format (e.g., via one-hot encoding) to be included in such a matrix.

\section{Matrix-Vector Multiplication and the Column Space}
Understanding matrix-vector multiplication from multiple perspectives is key to grasping more advanced concepts. The product $\mathbf{c} = A\mathbf{x}$ can be viewed in two fundamental ways.

\subsection{Two Perspectives on Matrix-Vector Multiplication}
Consider a matrix $A_1$ of size $3 \times 2$ and a vector $\mathbf{x} \in \mathbb{R}^2$:
\begin{equation}
    \mathbf{c} = A_1 \mathbf{x} = 
    \begin{bmatrix} 1 & 3 \\ 2 & 1 \\ -1 & -1 \end{bmatrix}
    \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\end{equation}

\paragraph{1. The Row-Wise Perspective (Dot Products):}
The conventional method computes each element of the resulting vector $\mathbf{c}$ as the dot product of a row of $A_1$ with the vector $\mathbf{x}$:
\begin{equation}
    \mathbf{c} = \begin{bmatrix} 1 \cdot x_1 + 3 \cdot x_2 \\ 2 \cdot x_1 + 1 \cdot x_2 \\ -1 \cdot x_1 + (-1) \cdot x_2 \end{bmatrix}
\end{equation}

\paragraph{2. The Column-Wise Perspective (Linear Combination):}
A more insightful interpretation is to view the product as a \textbf{linear combination of the columns of the matrix}, where the coefficients are the elements of the vector $\mathbf{x}$:
\begin{equation}
    \mathbf{c} = x_1 \underbrace{\begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}}_{\mathbf{a}_1} + x_2 \underbrace{\begin{bmatrix} 3 \\ 1 \\ -1 \end{bmatrix}}_{\mathbf{a}_2}
\end{equation}
This perspective reveals that the resulting vector $\mathbf{c}$ is a vector that lies in the space spanned by the columns of $A_1$.

\subsection{The Column Space of a Matrix}
This leads to the formal definition of the column space.
\begin{center}
    \textit{The space spanned by the columns of a matrix $A$ is denoted by $C(A)$ and is called the \textbf{Column Space} of $A$.}
\end{center}
The product $A\mathbf{x}$ is always a vector that belongs to the column space of $A$.

\subsection{The Rank of a Matrix}
The rank of a matrix is a fundamental property that describes the dimensionality of its column space.
\begin{equation}
\begin{cases}
    r = \text{rank}(A) = \text{the number of linearly independent columns of } A \\
    r = \text{dim}(C(A)) = \text{the dimension of the column space of } A
\end{cases}
\end{equation}

\paragraph{Examples:}
\begin{itemize}
    \item For matrix $A_1$ (size $3 \times 2$), its two columns are linearly independent. Therefore, $\text{rank}(A_1) = 2$, and its column space $C(A_1)$ is a \textbf{plane} in $\mathbb{R}^3$.

    \item Consider the matrix $A_2 = \begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}$. The third column $\mathbf{a}_3$ is a linear combination of the first two: $\mathbf{a}_3 = 2\mathbf{a}_2 - \mathbf{a}_1$. Since there are only two linearly independent columns, $\text{rank}(A_2) = 2$. The column space $C(A_2)$ is also a \textbf{plane} in $\mathbb{R}^3$.
    
    \item For the matrix $A_3 = \begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 10 \end{bmatrix}$, all three columns are linearly independent. Thus, $\text{rank}(A_3) = 3$, and its column space $C(A_3)$ spans the entire $\mathbb{R}^3$.

    \item For the matrix $A_4 = \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}$, the second and third columns are multiples of the first. There is only one linearly independent column. Therefore, $\text{rank}(A_4) = 1$, and its column space $C(A_4)$ is a \textbf{line} in $\mathbb{R}^3$.
\end{itemize}

\section{Solvability of Linear Systems}
The concept of the column space provides a clear and elegant condition for the solvability of a linear system.
\begin{equation}
    A\mathbf{x} = \mathbf{b}
\end{equation}
From our understanding of matrix-vector multiplication, we know that the vector $A\mathbf{x}$ is a linear combination of the columns of $A$ and therefore must lie in the column space $C(A)$. This leads to a fundamental conclusion:
\begin{itemize}
    \item If the vector $\mathbf{b}$ \textbf{belongs} to the column space of $A$ ($\mathbf{b} \in C(A)$), then the system is \textbf{solvable}. There exists at least one vector $\mathbf{x}$ whose components are the coefficients of the linear combination of columns that equals $\mathbf{b}$.
    \item If the vector $\mathbf{b}$ \textbf{does not belong} to the column space of $A$ ($\mathbf{b} \notin C(A)$), then the system is \textbf{not solvable}. No linear combination of the columns of $A$ can produce $\mathbf{b}$.
\end{itemize}

\section{Column Space Basis and CR Factorization}
\subsection{Finding a Basis for the Column Space}
To find a basis for the column space $C(\mat{A})$, we can use the following iterative algorithm:
\begin{enumerate}
    \item Initialize the basis set with the first non-zero column of $\mat{A}$. Let this basis vector be $\vec{c}_1$. For instance, $\vec{c}_1 = \vec{a}_1$.
    \item For each subsequent column $\vec{a}_k$ of $\mat{A}$, check if it is a linear combination of the vectors currently in the basis set.
    \item If $\vec{a}_k$ is \textbf{not} a linear combination of the existing basis vectors (i.e., it is linearly independent), add it to the basis set.
    \item If $\vec{a}_k$ \textbf{is} a linear combination, discard it and move to the next column.
    \item The process terminates when all columns of $\mat{A}$ have been checked. The resulting set of vectors forms a basis for $C(\mat{A})$.
\end{enumerate}

Applying this to matrix $\mat{A}_2 = \begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}$:
\begin{itemize}
    \item Let $\vec{c}_1 = \vec{a}_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$. The basis set is $\{\vec{c}_1\}$.
    \item Consider $\vec{a}_2 = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}$. We check if $\vec{a}_2 = \alpha \vec{c}_1$ for some scalar $\alpha$. This is clearly not the case. Thus, $\vec{a}_2$ is linearly independent of $\vec{c}_1$. We add it to the basis: $\vec{c}_2 = \vec{a}_2$. The basis set is now $\{\vec{c}_1, \vec{c}_2\}$.
    \item Consider $\vec{a}_3 = \begin{bmatrix} 7 \\ 8 \\ 9 \end{bmatrix}$. We check if $\vec{a}_3$ is a linear combination of $\vec{c}_1$ and $\vec{c}_2$. We find that:
    \begin{equation}
    \vec{a}_3 = 2\vec{a}_2 - \vec{a}_1 = 2\begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 8-1 \\ 10-2 \\ 12-3 \end{bmatrix} = \begin{bmatrix} 7 \\ 8 \\ 9 \end{bmatrix}
    \end{equation}
    Since $\vec{a}_3$ is linearly dependent on the previous columns, we discard it.
\end{itemize}
The basis for the column space $C(\mat{A}_2)$ is therefore given by the first two columns:
\begin{equation}
\text{Basis}(C(\mat{A}_2)) = \left\{ \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix} \right\}
\end{equation}

\subsection{CR Decomposition}
The process of identifying a basis for the column space naturally leads to a form of matrix factorization. Any matrix $\mat{A}$ can be decomposed into the product of two matrices, $\mat{C}$ and $\mat{R}$, such that $\mat{A} = \mat{C}\mat{R}$.
\begin{itemize}
    \item $\mat{C}$ is a matrix whose columns form a basis for the column space of $\mat{A}$.
    \item $\mat{R}$ is a matrix that contains the coefficients expressing each column of $\mat{A}$ as a linear combination of the basis vectors in $\mat{C}$.
\end{itemize}

For our example matrix $\mat{A}_2$, the decomposition is:
\begin{equation}
\underbrace{\begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}}_{\mat{A}_2 (3\times3)} = \underbrace{\begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}}_{\mat{C} (3\times2)} \underbrace{\begin{bmatrix} 1 & 0 & -1 \\ 0 & 1 & 2 \end{bmatrix}}_{\mat{R} (2\times3)}
\end{equation}
Here, the columns of $\mat{C}$ are the basis vectors we found. The columns of $\mat{R}$ represent how to reconstruct the columns of $\mat{A}_2$:
\begin{itemize}
    \item $\vec{a}_1 = 1 \cdot \vec{c}_1 + 0 \cdot \vec{c}_2$
    \item $\vec{a}_2 = 0 \cdot \vec{c}_1 + 1 \cdot \vec{c}_2$
    \item $\vec{a}_3 = -1 \cdot \vec{c}_1 + 2 \cdot \vec{c}_2$
\end{itemize}
This factorization is also known as the \textbf{full rank factorization} and the matrix $\mat{R}$ is the \textbf{row-reduced echelon form (rref)} of $\mat{A}$.

\subsection{Rank of a Matrix and its Transpose}
An important property of the rank is that the rank of a matrix is equal to the rank of its transpose. This implies that the number of linearly independent columns is equal to the number of linearly independent rows.
\begin{equation}
\text{rank}(\mat{A}^T) = \text{rank}(\mat{A})
\end{equation}
In terms of dimensions of column spaces:
\begin{equation}
\dim(C(\mat{A}^T)) = \dim(C(\mat{A})) = r
\end{equation}

\section{Perspectives on Matrix-Matrix Multiplication}
The standard way to compute the product of two matrices $\mat{A}$ (size $m \times n$) and $\mat{B}$ (size $n \times p$) is the row-column multiplication. However, an alternative and insightful perspective is to view the product as a sum of outer products.

\subsection{Column-Row Expansion}
The product $\mat{A}\mat{B}$ can be expressed as the sum of the outer products of the columns of $\mat{A}$ and the rows of $\mat{B}$. If we denote the columns of $\mat{A}$ as $\vec{c}_{\mat{A}i}$ and the rows of $\mat{B}$ as $\vec{r}_{\mat{B}i}$, the product is:
\begin{equation}
\mat{A}\mat{B} = \sum_{i=1}^{n} \vec{c}_{\mat{A}i} \vec{r}_{\mat{B}i}
\end{equation}
Each term $\vec{c}_{\mat{A}i} \vec{r}_{\mat{B}i}$ is an $m \times p$ matrix of rank one. This decomposition is a foundational concept for many advanced factorization techniques used in machine learning, such as Singular Value Decomposition (SVD).

\section{The Four Fundamental Subspaces of Linear Algebra}
For any given matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$, there are four fundamental subspaces that provide deep insights into its properties and the linear systems it defines.

\subsection{The Column Space and Row Space}
\begin{enumerate}
    \item \textbf{The Column Space of A}, denoted $C(\mathbf{A})$, is the subspace spanned by the columns of $\mathbf{A}$. The dimension of this space is the rank of the matrix, $r$. Since each column vector has $m$ components, the column space is a subspace of $\mathbb{R}^m$.
    \begin{align*}
        \dim(C(\mathbf{A})) &= r \\
        C(\mathbf{A}) &\subset \mathbb{R}^m
    \end{align*}

    \item \textbf{The Row Space of A}, which is equivalent to the column space of its transpose, is denoted $C(\mathbf{A}^T)$. It is the subspace spanned by the rows of $\mathbf{A}$. A fundamental theorem of linear algebra states that the dimension of the row space is also equal to the rank, $r$. Since each row vector has $n$ components, the row space is a subspace of $\mathbb{R}^n$.
    \begin{align*}
        \dim(C(\mathbf{A}^T)) &= r \\
        C(\mathbf{A}^T) &\subset \mathbb{R}^n
    \end{align*}
\end{enumerate}

\subsection{The Null Space and Left Null Space}
\begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{The Null Space of A} (or Kernel), denoted $N(\mathbf{A})$, is the set of all vectors $\mathbf{x}$ that are mapped to the zero vector by $\mathbf{A}$.
    \begin{equation}
        N(\mathbf{A}) = \ker(\mathbf{A}) = \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{A}\mathbf{x} = \mathbf{0}\}
    \end{equation}
    The null space is a subspace of $\mathbb{R}^n$.

    \item \textbf{The Left Null Space of A}, denoted $N(\mathbf{A}^T)$, is the null space of the transpose of $\mathbf{A}$. It consists of all vectors $\mathbf{y}$ that, when multiplied by $\mathbf{A}$ from the left, result in the zero vector.
    \begin{equation}
        N(\mathbf{A}^T) = \{\mathbf{y} \in \mathbb{R}^m \mid \mathbf{A}^T\mathbf{y} = \mathbf{0}\}
    \end{equation}
    The left null space is a subspace of $\mathbb{R}^m$.
\end{enumerate}

\subsection{Orthogonality of the Subspaces}
A critical relationship exists between these subspaces. The equation $\mathbf{A}\mathbf{x} = \mathbf{0}$ can be interpreted as a system of scalar products. If we denote the rows of $\mathbf{A}$ as $\mathbf{r}_1, \dots, \mathbf{r}_m$, then:
\begin{equation}
\mathbf{A}\mathbf{x} = \begin{bmatrix} \mathbf{r}_1 \cdot \mathbf{x} \\ \mathbf{r}_2 \cdot \mathbf{x} \\ \vdots \\ \mathbf{r}_m \cdot \mathbf{x} \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
\end{equation}
This shows that any vector $\mathbf{x}$ in the null space of $\mathbf{A}$ must be orthogonal to every row of $\mathbf{A}$. Since the rows of $\mathbf{A}$ span the row space $C(\mathbf{A}^T)$, we arrive at a profound conclusion:
\begin{equation}
C(\mathbf{A}^T) \perp N(\mathbf{A})
\end{equation}
The row space and the null space are orthogonal complements. Similarly, by considering the equation $\mathbf{A}^T\mathbf{y} = \mathbf{0}$, we can deduce the symmetric relationship:
\begin{equation}
C(\mathbf{A}) \perp N(\mathbf{A}^T)
\end{equation}

\subsection{The Fundamental Theorem of Linear Algebra}
These orthogonality relationships, combined with the dimensions of the subspaces, form the Fundamental Theorem of Linear Algebra. The theorem states that for a matrix $\mathbf{A} \in \mathbb{R}^{m \times n}$ with rank $r$:
\begin{itemize}
    \item The space $\mathbb{R}^n$ is decomposed into two orthogonal subspaces: the row space $C(\mathbf{A}^T)$ and the null space $N(\mathbf{A})$. Their dimensions sum to $n$:
    \begin{equation}
        \dim(C(\mathbf{A}^T)) + \dim(N(\mathbf{A})) = r + (n-r) = n
    \end{equation}
    \item The space $\mathbb{R}^m$ is decomposed into two orthogonal subspaces: the column space $C(\mathbf{A})$ and the left null space $N(\mathbf{A}^T)$. Their dimensions sum to $m$:
    \begin{equation}
        \dim(C(\mathbf{A})) + \dim(N(\mathbf{A}^T)) = r + (m-r) = m
    \end{equation}
\end{itemize}

\subsection{Proof of the Rank-Nullity Theorem}
Let $\mathbf{A} \in \mathbb{R}^{m \times n}$ be a matrix with $\text{rank}(\mathbf{A}) = r$. Without loss of generality, we can reorder the columns of $\mathbf{A}$ such that the first $r$ columns are linearly independent. We can then partition $\mathbf{A}$ as:
\begin{equation}
    \mathbf{A} = \begin{bmatrix} \mathbf{A}_1 & \mathbf{A}_2 \end{bmatrix}
\end{equation}
where $\mathbf{A}_1 \in \mathbb{R}^{m \times r}$ contains the $r$ linearly independent columns, and $\mathbf{A}_2 \in \mathbb{R}^{m \times (n-r)}$ contains the remaining $n-r$ columns.

Since the columns of $\mathbf{A}_1$ form a basis for $C(\mathbf{A})$, each column in $\mathbf{A}_2$ can be expressed as a linear combination of the columns of $\mathbf{A}_1$. This relationship can be represented by a matrix $\mathbf{B} \in \mathbb{R}^{r \times (n-r)}$ such that $\mathbf{A}_2 = \mathbf{A}_1 \mathbf{B}$.

Our goal is to find a basis for the null space $N(\mathbf{A})$. We propose a candidate matrix $\mathbf{K}$ whose columns form this basis:
\begin{equation}
    \mathbf{K} = \begin{bmatrix} -\mathbf{B} \\ \mathbf{I}_{n-r} \end{bmatrix} \in \mathbb{R}^{n \times (n-r)}
\end{equation}
The columns of $\mathbf{K}$ are in the null space because $\mathbf{A}\mathbf{K} = \begin{bmatrix} \mathbf{A}_1 & \mathbf{A}_1 \mathbf{B} \end{bmatrix} \begin{bmatrix} -\mathbf{B} \\ \mathbf{I} \end{bmatrix} = -\mathbf{A}_1\mathbf{B} + \mathbf{A}_1\mathbf{B} = \mathbf{0}$. The columns are linearly independent due to the identity matrix $\mathbf{I}_{n-r}$ in the lower block.

To show the columns span $N(\mathbf{A})$, let $\mathbf{v} \in N(\mathbf{A})$ be an arbitrary vector, partitioned as $\mathbf{v} = \begin{bmatrix} \mathbf{v}_1 \\ \mathbf{v}_2 \end{bmatrix}$. From $\mathbf{A}\mathbf{v}=\mathbf{0}$, we have $\mathbf{A}_1\mathbf{v}_1 + \mathbf{A}_1\mathbf{B}\mathbf{v}_2 = \mathbf{A}_1(\mathbf{v}_1 + \mathbf{B}\mathbf{v}_2) = \mathbf{0}$. Since $\mathbf{A}_1$ has linearly independent columns, this implies $\mathbf{v}_1 + \mathbf{B}\mathbf{v}_2 = \mathbf{0}$, or $\mathbf{v}_1 = -\mathbf{B}\mathbf{v}_2$. Substituting this back into $\mathbf{v}$:
\begin{equation}
    \mathbf{v} = \begin{bmatrix} \mathbf{v}_1 \\ \mathbf{v}_2 \end{bmatrix} = \begin{bmatrix} -\mathbf{B}\mathbf{v}_2 \\ \mathbf{v}_2 \end{bmatrix} = \begin{bmatrix} -\mathbf{B} \\ \mathbf{I} \end{bmatrix} \mathbf{v}_2 = \mathbf{K}\mathbf{v}_2
\end{equation}
This shows that any vector $\mathbf{v}$ in the null space can be written as a linear combination of the columns of $\mathbf{K}$. Since the columns of $\mathbf{K}$ are linearly independent and span $N(\mathbf{A})$, they form a basis. The number of basis vectors is $n-r$, so $\text{dim}(N(\mathbf{A})) = n-r$.

\section{Orthogonal Matrices and Geometric Transformations}

\subsection{Definition and Properties of Orthogonal Matrices}
An orthogonal matrix is a square matrix $\mathbf{Q}$ whose columns are mutually orthonormal. A key property of such matrices is that their transpose is equal to their inverse: $\mathbf{Q}^T = \mathbf{Q}^{-1}$. This leads to the defining property:
\begin{equation}
    \mathbf{Q}^T \mathbf{Q} = \mathbf{I}
\end{equation}
Orthogonal matrices represent geometric transformations, such as rotations and reflections, that preserve lengths and angles. To see this, consider a vector $\mathbf{x}$ and its transformed counterpart $\mathbf{y} = \mathbf{Q}\mathbf{x}$. The squared Euclidean norm of $\mathbf{y}$ is:
\begin{align*}
    \|\mathbf{y}\|^2 &= \mathbf{y}^T\mathbf{y} = (\mathbf{Q}\mathbf{x})^T(\mathbf{Q}\mathbf{x}) \\
    &= \mathbf{x}^T\mathbf{Q}^T\mathbf{Q}\mathbf{x} \\
    &= \mathbf{x}^T\mathbf{I}\mathbf{x} = \mathbf{x}^T\mathbf{x} = \|\mathbf{x}\|^2
\end{align*}
This shows that the transformation preserves the length of the vector, which ensures numerical stability in algorithms.

\subsection{Examples of Orthogonal Transformations}

\subsubsection{Rotation Matrix}
A classic example is the 2D rotation matrix, which rotates a vector by an angle $\theta$ counter-clockwise around the origin:
\begin{equation}
    \mathbf{R} = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}
\end{equation}
This matrix is orthogonal and preserves the norm of any vector it transforms.

\subsubsection{Reflection Matrix}
Another important orthogonal transformation is reflection across a plane. Consider a plane $\pi$ passing through the origin, defined by its unit normal vector $\mathbf{n}$ (i.e., $\|\mathbf{n}\|=1$). We want to find the matrix that reflects a vector $\mathbf{v}$ across this plane to produce a vector $\mathbf{w}$.

The reflection $\mathbf{w}$ can be found by starting at $\mathbf{v}$ and moving along the direction opposite to $\mathbf{n}$ by twice the length of the projection of $\mathbf{v}$ onto $\mathbf{n}$.\\
The projection of $\mathbf{v}$ onto the normal $\mathbf{n}$ is the vector component $\mathbf{v}_\perp$:
\begin{equation}
    \mathbf{v}_\perp = (\mathbf{v} \cdot \mathbf{n}) \mathbf{n}
\end{equation}
The reflected vector $\mathbf{w}$ is then:
\begin{equation}
    \mathbf{w} = \mathbf{v} - 2\mathbf{v}_\perp = \mathbf{v} - 2(\mathbf{v} \cdot \mathbf{n})\mathbf{n}
\end{equation}
Using the property that the dot product $\mathbf{v} \cdot \mathbf{n}$ can be written as $\mathbf{n}^T\mathbf{v}$, we can express this as a matrix-vector product:
\begin{align*}
    \mathbf{w} &= \mathbf{v} - 2\mathbf{n}(\mathbf{n}^T\mathbf{v}) \\
    &= (\mathbf{I} - 2\mathbf{n}\mathbf{n}^T)\mathbf{v}
\end{align*}
The matrix $\mathbf{H} = \mathbf{I} - 2\mathbf{n}\mathbf{n}^T$ is known as the Householder reflection matrix. It is an orthogonal matrix that performs the desired reflection.

\subsection{Projection Matrix}
A closely related matrix is the projection matrix. Consider a matrix with a structure similar to the Householder matrix, but without the factor of 2:
\begin{equation}
\mathbf{P} = \mathbf{I} - \mathbf{n}\mathbf{n}^T
\end{equation}
Applying this matrix to a vector $\mathbf{v}$ yields:
\begin{equation}
\mathbf{P}\mathbf{v} = (\mathbf{I} - \mathbf{n}\mathbf{n}^T)\mathbf{v} = \mathbf{v} - \mathbf{n}(\mathbf{n}^T\mathbf{v}) = \mathbf{v} - (\mathbf{v}\cdot\mathbf{n})\mathbf{n}
\end{equation}
Geometrically, this operation subtracts the component of $\mathbf{v}$ orthogonal to the plane $\pi$, effectively yielding the orthogonal projection of $\mathbf{v}$ onto the plane. For this reason, $\mathbf{P}$ is called an \textbf{orthogonal projection matrix}. Unlike reflection matrices, projection matrices are singular and have a determinant of zero.

\subsection{Determinant of Orthogonal Matrices}
We can derive a general property for the determinant of any orthogonal matrix $\mathbf{Q}$. Starting from the definition $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$, we take the determinant of both sides:
\begin{equation}
\det(\mathbf{Q}^T\mathbf{Q}) = \det(\mathbf{I})
\end{equation}
Using the properties $\det(\mathbf{A}\mathbf{B}) = \det(\mathbf{A})\det(\mathbf{B})$ and $\det(\mathbf{A}^T) = \det(\mathbf{A})$, we get:
\begin{align}
\det(\mathbf{Q}^T)\det(\mathbf{Q}) &= 1 \\
[\det(\mathbf{Q})]^2 &= 1
\end{align}
This implies that the determinant of any orthogonal matrix must be:
\begin{equation}
\det(\mathbf{Q}) = \pm 1
\end{equation}
This confirms our findings for the two main types of isometries:
\begin{itemize}
    \item For rotation matrices, $\det(\mathbf{R}) = +1$, as rotations preserve the orientation of the coordinate system.
    \item For reflection matrices, $\det(\mathbf{H}) = -1$, as reflections reverse the orientation.
\end{itemize}

\newpage
\chapter{Lesson 26-09}

\section{Decompositions for Square Matrices}

Matrix decompositions are fundamental tools in numerical analysis and machine learning. They allow us to break down a matrix into constituent parts that reveal its underlying properties and simplify complex operations. Here, we focus on decompositions primarily applicable to square matrices.

\subsection{Eigenvalue Decomposition (Spectral Decomposition)}

For a square matrix $\mat{A} \in \R^{n \times n}$, the eigenvalue decomposition is a factorization that reveals its fundamental properties in terms of its eigenvalues and eigenvectors.

An eigenvector $\vec{x}_i$ of a matrix $\mat{A}$ is a non-zero vector that, when multiplied by $\mat{A}$, is simply scaled by a scalar factor $\lambda_i$, known as the eigenvalue. This relationship is expressed as:
\begin{equation}
    \mat{A}\vec{x}_i = \lambda_i \vec{x}_i, \quad \text{for } i = 1, \dots, n
\end{equation}

If the matrix $\mat{A}$ has $n$ linearly independent eigenvectors, we can assemble them as columns of a matrix $\mat{X} = [\vec{x}_1, \vec{x}_2, \dots, \vec{x}_n]$. The corresponding eigenvalues can be placed on the diagonal of a diagonal matrix $\mat{\Lambda}$. This allows us to express the decomposition in matrix form:
\begin{equation}
    \mat{A}\mat{X} = \mat{X}\mat{\Lambda}
\end{equation}
Assuming $\mat{X}$ is invertible (which it is if the eigenvectors are linearly independent), we can write the decomposition as:
\begin{equation}
    \mat{X}^{-1}\mat{A}\mat{X} = \mat{\Lambda}
\end{equation}
This is also known as the \textbf{spectral decomposition} of $\mat{A}$.

\subsubsection{Properties of Eigenvalue Decomposition}
\begin{itemize}
    \item \textbf{Powers of a Matrix:} The eigenvalues of $\mat{A}^k$ are $\lambda_i^k$, while the eigenvectors remain the same. This can be seen by repeated application of the eigenvalue definition:
    \begin{align*}
        \mat{A}^2\vec{x}_i &= \mat{A}(\mat{A}\vec{x}_i) = \mat{A}(\lambda_i\vec{x}_i) \\
        &= \lambda_i(\mat{A}\vec{x}_i) = \lambda_i(\lambda_i\vec{x}_i) = \lambda_i^2\vec{x}_i
    \end{align*}
    This property extends to any analytic function $f(\mat{A})$ of a matrix, such as the matrix exponential, where $f(\mat{A})\vec{x}_i = f(\lambda_i)\vec{x}_i$. For example:
    \begin{equation}
        e^{\mat{A}t}\vec{x}_i = e^{\lambda_i t}\vec{x}_i
    \end{equation}

    \item \textbf{Similar Matrices:} Two square matrices $\mat{A}$ and $\mat{B}$ are considered \textbf{similar} if there exists an invertible matrix $\mat{M}$ such that:
    \begin{equation}
        \mat{B} = \mat{M}^{-1}\mat{A}\mat{M}
    \end{equation}
    Similar matrices share the same eigenvalues. If $\vec{y}$ is an eigenvector of $\mat{B}$ with eigenvalue $\lambda$, then $\vec{w} = \mat{M}\vec{y}$ is an eigenvector of $\mat{A}$ with the same eigenvalue $\lambda$:
    \begin{align*}
        \mat{B}\vec{y} &= \lambda\vec{y} \\
        \mat{M}^{-1}\mat{A}\mat{M}\vec{y} &= \lambda\vec{y} \\
        \mat{A}(\mat{M}\vec{y}) &= \lambda(\mat{M}\vec{y}) \\
        \mat{A}\vec{w} &= \lambda\vec{w}
    \end{align*}
\end{itemize}

\section{QR Decomposition}

The QR decomposition is a factorization of a matrix into an orthogonal matrix and an upper triangular matrix. Unlike eigenvalue decomposition, it is applicable to any rectangular matrix $\mat{A} \in \R^{m \times n}$.

The decomposition is given by:
\begin{equation}
    \mat{A} = \mat{Q}\mat{R}
\end{equation}
where:
\begin{itemize}
    \item $\mat{Q}$ is an $m \times m$ orthogonal matrix, meaning its columns are orthonormal vectors and it satisfies $\mat{Q}^T\mat{Q} = \mat{I}$.
    \item $\mat{R}$ is an $m \times n$ upper triangular matrix.
\end{itemize}

\subsubsection{Full vs. Reduced QR Decomposition}
There are two forms of QR decomposition:
\begin{itemize}
    \item \textbf{Full QR Decomposition:} For $\mat{A}_{m \times n}$, $\mat{Q}$ is a square $m \times m$ matrix and $\mat{R}$ is a rectangular $m \times n$ matrix. If $m > n$, the bottom $m-n$ rows of $\mat{R}$ will be all zeros.
    \[
    \underset{(m \times n)}{\mat{A}} = \underset{(m \times m)}{\mat{Q}} \begin{bmatrix}
        \hat{\mat{R}} \\
        \mat{0}
    \end{bmatrix}
    \]
    where $\hat{\mat{R}}$ is an $n \times n$ upper triangular matrix.

    \item \textbf{Reduced (or Thin) QR Decomposition:} This is a more compact and computationally efficient form. Here, $\mat{Q}$ is an $m \times n$ matrix (denoted $\hat{\mat{Q}}$) and $\mat{R}$ is an $n \times n$ upper triangular matrix (denoted $\hat{\mat{R}}$). This form only considers the first $n$ columns of the full $\mat{Q}$ matrix, which form an orthonormal basis for the column space of $\mat{A}$.
    \[
    \underset{(m \times n)}{\mat{A}} = \underset{(m \times n)}{\hat{\mat{Q}}} \underset{(n \times n)}{\hat{\mat{R}}}
    \]
\end{itemize}

\subsection{Construction via Gram-Schmidt Orthogonalization}

The orthogonal matrix $\mat{Q}$ can be constructed from the columns of $\mat{A}$ using the Gram-Schmidt process. This algorithm iteratively builds an orthonormal set of vectors $\{\vec{q}_1, \vec{q}_2, \dots, \vec{q}_n\}$ from the columns of $\mat{A} = [\vec{a}_1, \vec{a}_2, \dots, \vec{a}_n]$.

The procedure is as follows:
\begin{enumerate}
    \item \textbf{Initialize with the first vector:}
    The first orthonormal vector $\vec{q}_1$ is obtained by normalizing the first column of $\mat{A}$, $\vec{a}_1$:
    \begin{equation}
        \vec{q}_1 = \frac{\vec{a}_1}{\|\vec{a}_1\|}
    \end{equation}

    \item \textbf{Construct subsequent vectors:}
    For each subsequent column $\vec{a}_k$, we first subtract its projection onto the subspace spanned by the previously constructed orthonormal vectors $\{\vec{q}_1, \dots, \vec{q}_{k-1}\}$. This creates a vector, $\tilde{\vec{q}}_k$, that is orthogonal to this subspace.

    For the second vector, $\vec{a}_2$:
    \begin{equation}
        \tilde{\vec{q}}_2 = \vec{a}_2 - (\vec{a}_2 \cdot \vec{q}_1)\vec{q}_1
    \end{equation}
    This vector $\tilde{\vec{q}}_2$ is orthogonal to $\vec{q}_1$. We then normalize it to get the next orthonormal vector:
    \begin{equation}
        \vec{q}_2 = \frac{\tilde{\vec{q}}_2}{\|\tilde{\vec{q}}_2\|}
    \end{equation}

    For the third vector, $\vec{a}_3$, we subtract its projections onto both $\vec{q}_1$ and $\vec{q}_2$:
    \begin{equation}
        \tilde{\vec{q}}_3 = \vec{a}_3 - (\vec{a}_3 \cdot \vec{q}_1)\vec{q}_1 - (\vec{a}_3 \cdot \vec{q}_2)\vec{q}_2
    \end{equation}
    and then normalize:
    \begin{equation}
        \vec{q}_3 = \frac{\tilde{\vec{q}}_3}{\|\tilde{\vec{q}}_3\|}
    \end{equation}
\end{enumerate}

This process continues for all columns of $\mat{A}$. The resulting vectors $\vec{q}_1, \dots, \vec{q}_n$ form the columns of the matrix $\hat{\mat{Q}}$ in the reduced QR decomposition. The entries of the upper triangular matrix $\hat{\mat{R}}$ are the coefficients computed during the projection steps.

\section{Spectral Properties of Symmetric Matrices}

Symmetric matrices possess special properties that are fundamental in many areas of machine learning and optimization. For a general square matrix $\mat{A} \in \R^{n \times n}$ with a full set of linearly independent eigenvectors, the eigenvalue decomposition is given by:
\begin{equation}
    \mat{A} = \mat{X} \mat{\Lambda} \mat{X}^{-1}
\end{equation}
where $\mat{X}$ is the matrix whose columns are the eigenvectors of $\mat{A}$, and $\mat{\Lambda}$ is a diagonal matrix containing the corresponding eigenvalues.

\subsection{Eigenvalue Decomposition of Symmetric Matrices}
When a matrix is symmetric, its eigenvalue decomposition takes on a more structured and powerful form. Let $\mat{S}$ be a square, symmetric matrix ($\mat{S} = \mat{S}^T$). Its decomposition is:
\begin{equation}
    \mat{S} = \mat{Q} \mat{\Lambda} \mat{Q}^T
\end{equation}
The key difference is that the matrix of eigenvectors, denoted by $\mat{Q}$, is an \textbf{orthogonal matrix}. This means its columns are not just linearly independent, but are mutually orthogonal and have unit length. Consequently, its inverse is simply its transpose:
\begin{equation}
    \mat{Q}^{-1} = \mat{Q}^T
\end{equation}

This property simplifies many theoretical derivations and numerical computations involving symmetric matrices.

\subsection{Orthogonality of Eigenvectors}
\begin{proof}
Let's prove that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal. Consider two distinct eigenvalues, $\lambda$ and $\alpha$, with their corresponding eigenvectors $\vec{x}$ and $\vec{y}$:
\begin{align*}
    \mat{S}\vec{x} &= \lambda \vec{x} \\
    \mat{S}\vec{y} &= \alpha \vec{y}
\end{align*}
From these definitions, we can make two observations about the fundamental subspaces of $\mat{S}$:
\begin{itemize}
    \item The eigenvector $\vec{x}$ belongs to the column space of $\mat{S}$, $\vec{x} \in C(\mat{S})$, provided $\lambda \neq 0$.
    \item If we consider a zero eigenvalue, such that $\mat{S}\vec{y} = 0\vec{y} = 0$, then the eigenvector $\vec{y}$ belongs to the null space of $\mat{S}$, $\vec{y} \in N(\mat{S})$.
\end{itemize}
For any real symmetric matrix, its column space is the orthogonal complement of its null space:
\begin{equation}
    C(\mat{S}) = C(\mat{S}^T) \perp N(\mat{S})
\end{equation}
Since $\vec{x} \in C(\mat{S})$ and $\vec{y} \in N(\mat{S})$, it follows directly that $\vec{x}$ and $\vec{y}$ are orthogonal, i.e., $\vec{x}^T \vec{y} = 0$.

More generally, for any two distinct eigenvalues $\lambda_i \neq \lambda_j$:
\begin{align*}
    \lambda_i \vec{x}_j^T \vec{x}_i &= \vec{x}_j^T (\lambda_i \vec{x}_i) = \vec{x}_j^T (\mat{S} \vec{x}_i) \\
    &= (\vec{x}_j^T \mat{S}) \vec{x}_i = (\mat{S}^T \vec{x}_j)^T \vec{x}_i = (\mat{S} \vec{x}_j)^T \vec{x}_i \\
    &= (\lambda_j \vec{x}_j)^T \vec{x}_i = \lambda_j \vec{x}_j^T \vec{x}_i
\end{align*}
This leads to $(\lambda_i - \lambda_j) \vec{x}_j^T \vec{x}_i = 0$. Since $\lambda_i \neq \lambda_j$, we must have $\vec{x}_j^T \vec{x}_i = 0$.
\end{proof}

\subsection{Reality of Eigenvalues}
\begin{proof}
The eigenvalues of a real symmetric matrix are always real. This can be shown using the Rayleigh quotient. The eigenvalue $\lambda$ is given by:
\begin{equation}
    \lambda = \frac{\bar{\vec{x}}^T \mat{S} \vec{x}}{\bar{\vec{x}}^T \vec{x}}
\end{equation}
where $\bar{\vec{x}}$ is the complex conjugate of the eigenvector $\vec{x}$. The denominator $\bar{\vec{x}}^T \vec{x}$ is the squared Euclidean norm of $\vec{x}$, which is always a real, positive number.
\[ \bar{\vec{x}}^T \vec{x} = \sum_{k=1}^n \bar{x}_k x_k = \sum_{k=1}^n (a_k - ib_k)(a_k + ib_k) = \sum_{k=1}^n (a_k^2 + b_k^2) \in \R \]
Now, let's examine the numerator, which is the quadratic form $\bar{\vec{x}}^T \mat{S} \vec{x}$. Let $x_k = a_k + ib_k$.
\begin{align*}
    \bar{\vec{x}}^T \mat{S} \vec{x} &= \sum_{i,j} a_{ij} \bar{x}_i x_j \\
    &= \sum_{i,j} a_{ij} (a_i - ib_i)(a_j + ib_j) \\
    &= \sum_{i,j} a_{ij} (a_i a_j + i a_i b_j - i b_i a_j + b_i b_j)
\end{align*}
Since $\mat{S}$ is symmetric ($a_{ij} = a_{ji}$) and real, the imaginary terms will cancel out, leaving a real number. For example, the imaginary part of the sum is $\sum_{i,j} a_{ij}(a_i b_j - b_i a_j)$. Due to symmetry, the term $a_{ij}(a_i b_j - b_i a_j)$ and the term $a_{ji}(a_j b_i - b_j a_i)$ will cancel each other out. Thus, the numerator is real. Since $\lambda$ is the ratio of two real numbers, it must be real.
\end{proof}

\section{Symmetric Positive Definite (SPD) Matrices}

A special and important class of symmetric matrices are Symmetric Positive Definite (SPD) matrices. They have several equivalent characterizations:
\begin{enumerate}
    \item \textbf{Eigenvalues}: All eigenvalues are strictly positive: $\lambda_i > 0$ for all $i$.
    \item \textbf{Energy / Quadratic Form}: The quadratic form is positive for any non-zero vector $\vec{v} \in \R^n$:
    \[ \vec{v}^T \mat{S} \vec{v} > 0 \quad \text{for all } \vec{v} \neq 0 \]
    (Note: if the condition is $\vec{v}^T \mat{S} \vec{v} \ge 0$, the matrix is positive semi-definite).
    \item \textbf{Cholesky Decomposition}: The matrix $\mat{S}$ can be factored as $\mat{S} = \mat{B}^T \mat{B}$, where $\mat{B}$ is an invertible matrix (i.e., has linearly independent columns).
\end{enumerate}

\subsection{Proof of Equivalence (Partial)}
Let's show that property (1) implies property (2).
\begin{proof}[Proof that (1) $\implies$ (2)]
Assume all eigenvalues $\lambda_i$ of $\mat{S}$ are positive. We use the spectral decomposition $\mat{S} = \mat{Q} \mat{\Lambda} \mat{Q}^T$ to analyze the quadratic form $\vec{v}^T \mat{S} \vec{v}$:
\begin{equation}
    \vec{v}^T \mat{S} \vec{v} = \vec{v}^T (\mat{Q} \mat{\Lambda} \mat{Q}^T) \vec{v} = (\mat{Q}^T \vec{v})^T \mat{\Lambda} (\mat{Q}^T \vec{v})
\end{equation}
Let $\vec{y} = \mat{Q}^T \vec{v}$. The expression becomes:
\begin{equation}
    \vec{y}^T \mat{\Lambda} \vec{y} = \sum_{i=1}^n \lambda_i y_i^2
\end{equation}
Since $\lambda_i > 0$ and $y_i^2 \ge 0$, the sum $\sum_{i=1}^n \lambda_i y_i^2$ is always greater than or equal to zero. This sum is equal to zero only if every term $\lambda_i y_i^2$ is zero. As $\lambda_i > 0$, this requires that $y_i^2 = 0$ for all $i$, which means $\vec{y}=0$.

Because $\mat{Q}$ is orthogonal (and thus invertible), the condition $\vec{y} = \mat{Q}^T \vec{v} = 0$ implies that $\vec{v}$ must be the zero vector. Therefore, for any non-zero vector $\vec{v}$, $\vec{y}$ will be non-zero, and the quadratic form $\vec{v}^T \mat{S} \vec{v} = \sum \lambda_i y_i^2$ will be strictly positive.
\end{proof}

\section{Singular Value Decomposition (SVD)}

Singular Value Decomposition (SVD) is arguably one of the most important matrix factorizations in numerical linear algebra and machine learning. It generalizes the concept of eigendecomposition to any rectangular matrix.

\subsection{Definition of Full SVD}
Any real matrix $\mat{A} \in \R^{m \times n}$ can be decomposed into the product of three matrices:
\begin{equation}
    \mat{A} = \mat{U} \mat{\Sigma} \mat{V}^T
\end{equation}
where:
\begin{itemize}
    \item $\mat{U} \in \R^{m \times m}$ is an orthogonal matrix ($\mat{U}^T \mat{U} = \mat{I}$). Its columns, $\vec{u}_i$, are called the \textbf{left-singular vectors}.
    \item $\mat{V} \in \R^{n \times n}$ is an orthogonal matrix ($\mat{V}^T \mat{V} = \mat{I}$). Its columns, $\vec{v}_i$, are called the \textbf{right-singular vectors}.
    \item $\mat{\Sigma} \in \R^{m \times n}$ is a rectangular diagonal matrix. Its diagonal entries, $\sigma_i$, are the \textbf{singular values} of $\mat{A}$. They are non-negative and are typically ordered in descending order: $\sigma_1 \geq \sigma_2 \geq \dots \geq 0$.
\end{itemize}
The structure of $\mat{\Sigma}$ is composed of a diagonal matrix $\mat{D}$ and zero-padding. For instance, if $m > n$, then:
\begin{equation}
    \mat{\Sigma} = \begin{bmatrix}
        \mat{D} \\
        0
    \end{bmatrix}, \quad \text{where} \quad \mat{D} = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_n)
\end{equation}

\subsection{Economy SVD}
In practice, if $m > n$, the last $m-n$ columns of $\mat{U}$ are multiplied by the zero block of $\mat{\Sigma}$. This suggests a more compact, or "economy," version of SVD:
\begin{equation}
    \mat{A} = \hat{\mat{U}} \hat{\mat{\Sigma}} \hat{\mat{V}}^T
\end{equation}
where:
\begin{itemize}
    \item $\hat{\mat{U}} \in \R^{m \times n}$ contains the first $n$ columns of $\mat{U}$.
    \item $\hat{\mat{\Sigma}} \in \R^{n \times n}$ is a square diagonal matrix containing the singular values.
    \item $\hat{\mat{V}} \in \R^{n \times n}$ is identical to $\mat{V}$. (Often written as just $\mat{V}$).
\end{itemize}
This form is more memory-efficient and is the default in many numerical libraries.

\subsection{Relationship with Eigendecomposition}
SVD is deeply connected to the eigendecomposition of the symmetric matrices $\mat{A}^T \mat{A}$ and $\mat{A} \mat{A}^T$.

Consider the matrix $\mat{A}^T \mat{A}$:
\begin{align}
    \mat{A}^T \mat{A} &= (\mat{U} \mat{\Sigma} \mat{V}^T)^T (\mat{U} \mat{\Sigma} \mat{V}^T) \\
    &= (\mat{V} \mat{\Sigma}^T \mat{U}^T) (\mat{U} \mat{\Sigma} \mat{V}^T) \\
    &= \mat{V} \mat{\Sigma}^T (\mat{U}^T \mat{U}) \mat{\Sigma} \mat{V}^T
\end{align}
Since $\mat{U}$ is orthogonal, $\mat{U}^T \mat{U} = \mat{I}$. The equation becomes:
\begin{equation}
    \mat{A}^T \mat{A} = \mat{V} (\mat{\Sigma}^T \mat{\Sigma}) \mat{V}^T
\end{equation}
This is precisely the eigendecomposition of the symmetric matrix $\mat{A}^T \mat{A}$. The matrix $\mat{\Sigma}^T \mat{\Sigma}$ is an $n \times n$ diagonal matrix with the squared singular values on its diagonal: $\text{diag}(\sigma_1^2, \sigma_2^2, \dots, \sigma_n^2)$. This tells us:
\begin{itemize}
    \item The columns of $\mat{V}$ (the right-singular vectors of $\mat{A}$) are the eigenvectors of $\mat{A}^T \mat{A}$.
    \item The squares of the singular values of $\mat{A}$ ($\sigma_i^2$) are the eigenvalues of $\mat{A}^T \mat{A}$.
\end{itemize}
Similarly, for $\mat{A} \mat{A}^T$:
\begin{equation}
    \mat{A} \mat{A}^T = \mat{U} (\mat{\Sigma} \mat{\Sigma}^T) \mat{U}^T
\end{equation}
This tells us:
\begin{itemize}
    \item The columns of $\mat{U}$ (the left-singular vectors of $\mat{A}$) are the eigenvectors of $\mat{A} \mat{A}^T$.
\end{itemize}
Since $\mat{A}^T \mat{A}$ and $\mat{A} \mat{A}^T$ are symmetric positive semi-definite, their eigenvalues are non-negative, confirming that the singular values $\sigma_i$ are real numbers.

The fundamental relationship between the singular vectors and values can be expressed column-wise. From $\mat{A}\mat{V} = \mat{U}\mat{\Sigma}$, we can write for each column $i$:
\begin{equation}
    \mat{A} \vec{v}_i = \sigma_i \vec{u}_i
\end{equation}
This equation provides a powerful geometric interpretation: the matrix $\mat{A}$ transforms the orthonormal input basis $\{\vec{v}_i\}$ into the axes of a hyperellipse, where the $i$-th axis is aligned with $\vec{u}_i$ and has a length of $\sigma_i$.

\subsection{SVD and the Null Space}
A key insight from SVD arises when considering a rank-deficient matrix $\mat{A}$, where its rank, denoted by $r$, is less than the minimum of its dimensions, i.e., $r < \min(m, n)$.

By convention, the singular values $\sigma_i$ in the diagonal matrix $\mat{\Sigma}$ are arranged in descending order. For a rank-$r$ matrix, only the first $r$ singular values are non-zero:
\begin{equation}
    \sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0
\end{equation}
while the remaining singular values are zero:
\begin{equation}
    \sigma_{r+1} = \sigma_{r+2} = \dots = \sigma_{\min(m,n)} = 0
\end{equation}

This has a direct implication for the relationship $\mat{A}\vec{v}_i = \sigma_i \vec{u}_i$:
\begin{itemize}
    \item For $i = 1, \dots, r$: $\mat{A}\vec{v}_i = \sigma_i \vec{u}_i \neq \vec{0}$. These vectors span the column space of $\mat{A}$.
    \item For $i = r+1, \dots, n$: $\mat{A}\vec{v}_i = 0 \cdot \vec{u}_i = \vec{0}$.
\end{itemize}

The second case is particularly important. By definition, any vector $\vec{x}$ for which $\mat{A}\vec{x} = \vec{0}$ belongs to the null space of $\mat{A}$, denoted $\mathcal{N}(\mat{A})$. Therefore, the vectors $\{\vec{v}_{r+1}, \dots, \vec{v}_n\}$ are members of the null space of $\mat{A}$.

Since these vectors are columns of the orthogonal matrix $\mat{V}$, they are orthonormal. Consequently, the set $\{\vec{v}_{r+1}, \dots, \vec{v}_n\}$ forms an orthonormal basis for the null space of $\mat{A}$. The SVD provides a direct and numerically stable method for finding a basis for both the column space and the null space of any matrix.

\subsection{The Outer Product Expansion of a Matrix}
The SVD also allows us to express a matrix as a weighted sum of rank-one matrices. This is known as the outer product expansion.

Let us consider a simple $3 \times 3$ matrix $\mat{A}$ for clarity. Its SVD is $\mat{A} = \mat{U}\mat{\Sigma}\mat{V}^T$. Writing this out explicitly:
\begin{equation}
    \mat{A} = 
    \begin{bmatrix}
        | & | & | \\
        \vec{u}_1 & \vec{u}_2 & \vec{u}_3 \\
        | & | & |
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_1 & 0 & 0 \\
        0 & \sigma_2 & 0 \\
        0 & 0 & \sigma_3
    \end{bmatrix}
    \begin{bmatrix}
        - & \vec{v}_1^T & - \\
        - & \vec{v}_2^T & - \\
        - & \vec{v}_3^T & -
    \end{bmatrix}
\end{equation}
First, multiplying $\mat{U}$ by $\mat{\Sigma}$ scales each column of $\mat{U}$:
\begin{equation}
    \mat{U}\mat{\Sigma} = 
    \begin{bmatrix}
        | & | & | \\
        \sigma_1\vec{u}_1 & \sigma_2\vec{u}_2 & \sigma_3\vec{u}_3 \\
        | & | & |
    \end{bmatrix}
\end{equation}
Next, multiplying this result by $\mat{V}^T$ yields the sum of outer products:
\begin{equation}
    \mat{A} = (\mat{U}\mat{\Sigma})\mat{V}^T = \sigma_1 \vec{u}_1 \vec{v}_1^T + \sigma_2 \vec{u}_2 \vec{v}_2^T + \sigma_3 \vec{u}_3 \vec{v}_3^T
\end{equation}
Each term $\vec{u}_i \vec{v}_i^T$ is an outer product of two vectors, resulting in a rank-one matrix.

\subsubsection{General Formulation and Low-Rank Approximation}
This concept generalizes to any $m \times n$ matrix $\mat{A}$ of rank $r$. The matrix can be perfectly reconstructed as a sum of $r$ rank-one matrices, each weighted by a singular value:
\begin{equation}
    \mat{A} = \sum_{i=1}^{r} \sigma_i \vec{u}_i \vec{v}_i^T
    \label{eq:svd_expansion}
\end{equation}
Since the singular values are ordered by magnitude ($\sigma_1 \ge \sigma_2 \ge \dots$), the first term $\sigma_1 \vec{u}_1 \vec{v}_1^T$ is the most significant rank-one component of the matrix, the second term is the next most significant, and so on.

\subsubsection{Application: Image Compression}
This expansion is the foundation for low-rank approximation, a powerful technique used in applications like image compression. A grayscale image can be represented as a matrix $\mat{A}$, where each entry corresponds to a pixel's intensity.

The key idea is that most of the "energy" or important information in the image is captured by the first few terms in the SVD expansion (those corresponding to the largest singular values). We can create a compressed, low-rank approximation of the image, $\mat{A}_k$, by truncating the sum in Equation \ref{eq:svd_expansion} after $k$ terms, where $k < r$:
\begin{equation}
    \mat{A} \approx \mat{A}_k = \sum_{i=1}^{k} \sigma_i \vec{u}_i \vec{v}_i^T
\end{equation}
For a large image, such as one with $1000 \times 1000$ pixels, a visually faithful reconstruction can often be achieved with a small number of terms (e.g., $k=20$ or $k=30$). Instead of storing all $m \times n$ pixel values, one only needs to store the first $k$ singular values $\sigma_i$ and the corresponding vectors $\vec{u}_i$ and $\vec{v}_i$. This results in significant data compression, as the storage required for $k(\sigma_i, \vec{u}_i, \vec{v}_i)$ is much less than for the full matrix $\mat{A}$.

\newpage
\chapter{Lesson 29-09}

\section{Singular Value Decomposition (SVD)}

Singular Value Decomposition (SVD) is a fundamental matrix factorization technique in linear algebra with wide-ranging applications in machine learning, statistics, and numerical analysis. It asserts that any real matrix $\mat{A} \in \R^{m \times n}$ can be decomposed into the product of three specific matrices.

\subsection{Formal Definition}
The decomposition is expressed as:
\begin{equation}
\mat{A} = \mat{U} \mat{\Sigma} \mat{V}^T
\end{equation}
where the components have the following properties and dimensions:
\begin{itemize}
    \item $\mat{U}$: An $m \times m$ orthogonal matrix ($\mat{U}^T\mat{U} = \mat{I}$). Its columns are the \textbf{left-singular vectors}.
    \item $\mat{\Sigma}$: An $m \times n$ pseudo-diagonal matrix. Its only non-zero entries are on the main diagonal, known as the \textbf{singular values}, $\sigma_i$. These values are non-negative and are conventionally arranged in descending order:
    \begin{equation}
        \sigma_1 \ge \sigma_2 \ge \sigma_3 \ge \dots \ge \sigma_r > 0
    \end{equation}
    where $r = \text{rank}(\mat{A})$.
    \item $\mat{V}^T$: An $n \times n$ orthogonal matrix ($\mat{V}^T\mat{V} = \mat{I}$). Its columns (the rows of $\mat{V}^T$) are the \textbf{right-singular vectors}.
\end{itemize}

\subsection{Types of SVD}
Depending on the application and matrix dimensions, different forms of SVD are used.

\subsubsection{Full SVD}
This is the complete decomposition as defined above, where $\mat{U}$ and $\mat{V}$ are square orthogonal matrices.
\[
\underset{(m \times n)}{\mat{A}} = \underset{(m \times m)}{\mat{U}} \quad \underset{(m \times n)}{\mat{\Sigma}} \quad \underset{(n \times n)}{\mat{V}^T}
\]

\subsubsection{Economy (Thin) SVD}
In many practical cases, particularly when $m > n$, the matrix $\mat{\Sigma}$ contains a block of all-zero rows. These rows nullify the corresponding last $m-n$ columns of $\mat{U}$ in the multiplication. The Economy SVD eliminates this redundancy.

We partition $\mat{\Sigma}$ into an $n \times n$ diagonal matrix $\mat{\Sigma}_{\text{econ}}$ and a zero matrix below it. We similarly partition $\mat{U}$ to only keep its first $n$ columns, resulting in $\mat{U}_{\text{econ}}$:
\[
\underset{(m \times n)}{\mat{A}} = \underset{(m \times n)}{\mat{U}_{\text{econ}}} \quad \underset{(n \times n)}{\mat{\Sigma}_{\text{econ}}} \quad \underset{(n \times n)}{\mat{V}^T}
\]
This form is more memory-efficient as it stores smaller matrices while preserving the exact reconstruction of $\mat{A}$.

\subsubsection{Reduced SVD}
If the rank of the matrix $\mat{A}$ is $r < \min(m, n)$, then only the first $r$ singular values in $\mat{\Sigma}$ are non-zero. The decomposition can be further reduced to only include the components corresponding to these non-zero singular values. This is achieved by selecting:
\begin{itemize}
    \item The first $r$ columns of $\mat{U}$ (denoted $\mat{U}_r$).
    \item The first $r$ columns of $\mat{V}$ (or first $r$ rows of $\mat{V}^T$, denoted $\mat{V}_r^T$).
    \item The top-left $r \times r$ diagonal matrix of singular values, $\mat{\Sigma}_r$.
\end{itemize}
The reconstruction remains exact:
\[
\underset{(m \times n)}{\mat{A}} = \underset{(m \times r)}{\mat{U}_r} \quad \underset{(r \times r)}{\mat{\Sigma}_r} \quad \underset{(r \times n)}{\mat{V}_r^T}
\]

\subsection{Geometric Interpretation}
The SVD provides a powerful geometric interpretation of a linear transformation represented by matrix $\mat{A}$. The transformation of a vector $\vec{x}$ can be seen as a sequence of three fundamental geometric operations:
\begin{enumerate}
    \item \textbf{Rotation/Reflection ($\mat{V}^T$):} The vector $\vec{x}$ is first rotated by the orthogonal matrix $\mat{V}^T$. This aligns the basis vectors of the domain space with the principal axes of the transformation.
    \item \textbf{Scaling ($\mat{\Sigma}$):} The components of the rotated vector are then scaled along these new axes by the corresponding singular values $\sigma_i$. A unit circle in 2D (or a hypersphere in higher dimensions) is transformed into an ellipse (or hyper-ellipse).
    \item \textbf{Rotation/Reflection ($\mat{U}$):} The scaled vector is finally rotated by the orthogonal matrix $\mat{U}$ into the final position in the codomain space.
\end{enumerate}
This decomposition reveals that any linear transformation can be broken down into a rotation, a scaling, and another rotation.

\section{Application: Image Compression via Truncated SVD}

One of the most intuitive applications of SVD is in lossy image compression. This is achieved through the **Truncated SVD**, which provides the best low-rank approximation of a matrix.

\subsection{The Eckart-Young-Mirsky Theorem}
The theorem states that if we want to find a matrix $\mat{A}_k$ of a lower rank $k < r$ that is closest to our original matrix $\mat{A}$, the best choice is the truncated SVD. "Best" is defined as minimizing the Frobenius norm (or spectral norm) of the difference:
\begin{equation}
\mat{A}_k = \arg\min_{\text{rank}(\mat{B})=k} \|\mat{A} - \mat{B}\|_F
\end{equation}
This optimal approximation $\mat{A}_k$ is constructed by keeping only the first $k$ singular values and their corresponding singular vectors:
\begin{equation}
\mat{A}_k = \mat{U}_k \mat{\Sigma}_k \mat{V}_k^T
\end{equation}
The magnitude of the approximation error is directly related to the discarded singular values:
\begin{equation}
\|\mat{A} - \mat{A}_k\|_F^2 = \sum_{i=k+1}^{r} \sigma_i^2
\end{equation}
Since singular values are ordered, this technique discards the "least important" information first.

\subsection{Practical Demonstration}
An image can be represented as a matrix $\mat{A} \in \R^{m \times n}$, where each entry corresponds to a pixel's intensity. For a color image, this can be done for each color channel (e.g., Red, Green, Blue) separately.

By applying truncated SVD, we can store an approximation of the image with significantly less data. Instead of storing the full $m \times n$ matrix, we store the first $k$ columns of $\mat{U}$ ($m \times k$), the first $k$ singular values, and the first $k$ rows of $\mat{V}^T$ ($k \times n$). The total storage required is proportional to $k(m+n+1)$, which leads to significant reduction when $k \ll \min(m, n)$.

The quality of the compressed image depends on the decay rate of its singular values. Images with significant geometric structure or repetitive patterns (e.g., paintings by Mondrian) have rapidly decaying singular values. This means a small number of singular values can capture a large portion of the image's "energy," allowing for high compression ratios with minimal visual loss. In contrast, images with high entropy and little coherent structure, such as random noise, have a very slow decay of singular values, making SVD-based compression ineffective. Natural images, like landscapes or portraits, fall in between these two extremes. Increasing the number of singular values, $k$, progressively improves the reconstruction quality.

\section{Orientation Sensitivity of SVD}
A crucial property of SVD is its sensitivity to the orientation of the data. The decomposition is not rotation-invariant, which means that rotating an image will change its singular value spectrum. This has important implications for machine learning applications.

\subsection{Python Implementation: Rotating a Square}
To demonstrate this property, we can generate a simple image of a square and observe how its singular value spectrum changes as we rotate it.

\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
import numpy as np
import skimage.transform

# Setup plot parameters
plt.rcParams['figure.figsize'] = [12, 6]
plt.rcParams.update({'font.size': 18})

# Create a simple square image
n = 1000
q = n // 4
X = np.zeros((n, n))
X[(q-1):(3*q), (q-1):(3*q)] = 1

# Compute and plot SVD of the original square
U, S, VT = np.linalg.svd(X, full_matrices=0)
fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.imshow(X)
ax2.semilogy(S, '-o')
plt.show()

# Rotate the image and re-compute SVD for different angles
fig, axs = plt.subplots(1, 2)
nAngles = 12
colors = plt.cm.jet(np.linspace(0, 1, nAngles))

for j in range(nAngles):
    # Rotate by theta = j*4 degrees
    theta = j * 4
    Xrot = skimage.transform.rotate(X, theta)
    Xrot[np.nonzero(Xrot)] = j # Assign a value for visualization
    
    # Compute SVD of the rotated image
    U, S, VT = np.linalg.svd(Xrot)
    
    # Plotting
    axs[0].imshow(np.ma.masked_where(Xrot == 0, Xrot), cmap='jet', vmin=0, vmax=nAngles)
    axs[1].semilogy(S, '-o', color=tuple(colors[j]))

axs[0].set_axis_off()
axs[1].grid(True)
plt.show()
\end{lstlisting}

When the square is axis-aligned, its matrix representation is extremely low-rank. The singular value spectrum decays almost instantly. However, as the square is rotated, the singular value spectrum decays much more slowly, indicating that more singular values are required to capture the same amount of information. The most "complex" representation occurs at a rotation of 45 degrees.

\subsection{Implications for Machine Learning: Data Augmentation}
This orientation sensitivity is fundamental to the concept of \textbf{data augmentation} in machine learning. When training a model on a limited dataset of images, one can artificially expand the dataset by creating rotated, scaled, or shifted versions of the existing images. From a human perspective, a rotated cat is still a cat. However, from the model's perspective, the underlying data matrix is different. By training on these augmented images, the model learns to become invariant to such transformations, leading to better generalization and robustness.

\section{Manual SVD Calculation: Numerical Examples}

To solidify the understanding of SVD, we can manually compute the decomposition for small matrices.

\subsection{Example 1: Full-Rank Matrix}
Let $\mat{A} = \begin{pmatrix} 5 & 5 \\ -2 & 2 \end{pmatrix}$. This is a full-rank matrix ($\text{rank}(\mat{A}) = 2$).

\begin{enumerate}
    \item \textbf{Compute $\mat{A}^T \mat{A}$ and its spectral decomposition:}
    The matrix $\mat{V}$ in SVD consists of the eigenvectors of $\mat{A}^T \mat{A}$.
    \begin{equation}
    \mat{X} = \mat{A}^T \mat{A} = \begin{pmatrix} 5 & -2 \\ 5 & 2 \end{pmatrix} \begin{pmatrix} 5 & 5 \\ -2 & 2 \end{pmatrix} = \begin{pmatrix} 29 & 21 \\ 21 & 29 \end{pmatrix}
    \end{equation}
    The eigenvalues of $\mat{X}$ are $\lambda_1 = 50$ and $\lambda_2 = 8$. The corresponding normalized eigenvectors are:
    \begin{equation}
    \vec{v}_1 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}, \quad \vec{v}_2 = \begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}
    \end{equation}
    Thus, $\mat{V} = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{2} \\ 1/\sqrt{2} & 1/\sqrt{2} \end{pmatrix}$.

    \item \textbf{Compute $\mat{\Sigma}$:}
    The singular values are the square roots of the eigenvalues of $\mat{A}^T \mat{A}$.
    \begin{equation}
    \sigma_1 = \sqrt{50} = 5\sqrt{2}, \quad \sigma_2 = \sqrt{8} = 2\sqrt{2}
    \end{equation}
    The matrix $\mat{\Sigma}$ is $\begin{pmatrix} 5\sqrt{2} & 0 \\ 0 & 2\sqrt{2} \end{pmatrix}$.

    \item \textbf{Compute $\mat{A} \mat{A}^T$ and its eigenvectors for $\mat{U}$:}
    The matrix $\mat{U}$ consists of the eigenvectors of $\mat{A} \mat{A}^T$.
    \begin{equation}
    \mat{Y} = \mat{A} \mat{A}^T = \begin{pmatrix} 5 & 5 \\ -2 & 2 \end{pmatrix} \begin{pmatrix} 5 & -2 \\ 5 & 2 \end{pmatrix} = \begin{pmatrix} 50 & 0 \\ 0 & 8 \end{pmatrix}
    \end{equation}
    Since $\mat{Y}$ is diagonal, its eigenvalues are $\mu_1 = 50, \mu_2 = 8$. The eigenvectors are:
    \begin{equation}
    \vec{u}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}, \quad \vec{u}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
    \end{equation}
    Thus, $\mat{U} = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} = \mat{I}$.
\end{enumerate}

\subsection{Example 2: Rank-Deficient Matrix}
Let $\mat{A} = \begin{pmatrix} 4 & 3 \\ 8 & 6 \end{pmatrix}$. This matrix has $\text{rank}(\mat{A})=1$.

\begin{enumerate}
    \item \textbf{Compute $\mat{A}^T \mat{A}$ and its spectral decomposition:}
    \begin{equation}
    \mat{X} = \mat{A}^T \mat{A} = \begin{pmatrix} 4 & 8 \\ 3 & 6 \end{pmatrix} \begin{pmatrix} 4 & 3 \\ 8 & 6 \end{pmatrix} = \begin{pmatrix} 80 & 60 \\ 60 & 45 \end{pmatrix}
    \end{equation}
    The eigenvalues are $\lambda_1 = 125, \lambda_2 = 0$. The normalized eigenvectors are:
    \begin{equation}
    \vec{v}_1 = \begin{pmatrix} 4/5 \\ 3/5 \end{pmatrix}, \quad \vec{v}_2 = \begin{pmatrix} -3/5 \\ 4/5 \end{pmatrix}
    \end{equation}
    Thus, $\mat{V} = \begin{pmatrix} 4/5 & -3/5 \\ 3/5 & 4/5 \end{pmatrix}$.

    \item \textbf{Compute $\mat{\Sigma}$:}
    \begin{equation}
    \sigma_1 = \sqrt{125} = 5\sqrt{5}, \quad \sigma_2 = \sqrt{0} = 0
    \end{equation}
    So, $\mat{\Sigma} = \begin{pmatrix} 5\sqrt{5} & 0 \\ 0 & 0 \end{pmatrix}$.

    \item \textbf{Compute $\mat{U}$:}
    We use the relationship $\vec{u}_1 = \frac{1}{\sigma_1} \mat{A} \vec{v}_1$.
    \begin{equation}
    \mat{A} \vec{v}_1 = \begin{pmatrix} 4 & 3 \\ 8 & 6 \end{pmatrix} \begin{pmatrix} 4/5 \\ 3/5 \end{pmatrix} = \begin{pmatrix} 5 \\ 10 \end{pmatrix}
    \end{equation}
    \begin{equation}
    \vec{u}_1 = \frac{1}{5\sqrt{5}} \begin{pmatrix} 5 \\ 10 \end{pmatrix} = \begin{pmatrix} 1/\sqrt{5} \\ 2/\sqrt{5} \end{pmatrix}
    \end{equation}
    To complete the basis, we choose a vector $\vec{u}_2$ orthogonal to $\vec{u}_1$:
    \begin{equation}
    \vec{u}_2 = \begin{pmatrix} -2/\sqrt{5} \\ 1/\sqrt{5} \end{pmatrix}
    \end{equation}
    Thus, $\mat{U} = \begin{pmatrix} 1/\sqrt{5} & -2/\sqrt{5} \\ 2/\sqrt{5} & 1/\sqrt{5} \end{pmatrix}$.
\end{enumerate}

\section{Properties and Special Cases of SVD}

Examining the SVD for special classes of matrices reveals deeper insights.

\subsection{SVD of an Orthogonal Matrix}
A special case arises when the matrix $\mat{A}$ is square and orthogonal. An orthogonal matrix is defined by the property $\mat{A}^T \mat{A} = \mat{A} \mat{A}^T = \mat{I}$. To find its SVD, we analyze the matrix $\mat{A}^T \mat{A} = \mat{I}$. The eigenvalues of the identity matrix are all 1. Therefore, the singular values of an orthogonal matrix, which are the square roots of these eigenvalues, are also all 1:
\begin{equation}
\sigma_1 = \sigma_2 = \dots = \sigma_n = 1
\end{equation}
This implies that the matrix $\mat{\Sigma}$ is the identity matrix, $\mat{\Sigma} = \mat{I}$. The SVD of an orthogonal matrix $\mat{A}$ is therefore given by $\mat{A} = \mat{U} \mat{I} \mat{V}^T = \mat{U} \mat{V}^T$.

\subsection{SVD for Square and Symmetric Matrices: The Polar Decomposition}
Let $\mat{A}$ be a square and symmetric matrix. The SVD of $\mat{A}$ is given by $\mat{A} = \mat{U\Sigma V}^T$. We can manipulate this expression:
\begin{align*}
    \mat{A} &= \mat{U\Sigma V}^T = (\mat{U}\mat{V}^T)(\mat{V\Sigma V}^T)
\end{align*}
This refactoring is known as the **Polar Decomposition** of $\mat{A}$, where $\mat{A} = \mat{Q}\mat{S}$.
\begin{enumerate}
    \item $\mat{Q} = \mat{U}\mat{V}^T$: An orthogonal matrix, representing a pure \textbf{rotation}.
    \item $\mat{S} = \mat{V\Sigma V}^T$: A symmetric matrix, representing a pure \textbf{scaling} or \textbf{stretching} operation.
\end{enumerate}
The polar decomposition thus provides a powerful geometric interpretation: any linear transformation represented by a symmetric matrix $\mat{A}$ can be decomposed into a rotation ($\mat{Q}$) followed by a scaling ($\mat{S}$).

\subsection{Relationship Between Eigenvalues and Singular Values}
\begin{theorem}
Let $\mat{A}$ be a square matrix with eigenvalues $\lambda_i$ and singular values $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_n$. Then, for any eigenvalue $\lambda$ of $\mat{A}$, its magnitude is bounded as follows:
\begin{equation}
    |\lambda| \le \sigma_1
\end{equation}
\end{theorem}

\begin{proof}
Let $\vec{x}$ be an eigenvector of $\mat{A}$ corresponding to the eigenvalue $\lambda$. By definition, $\mat{A}\vec{x} = \lambda\vec{x}$. Taking the Euclidean norm of both sides:
\begin{equation}
    \|\mat{A}\vec{x}\| = \|\lambda\vec{x}\| = |\lambda| \|\vec{x}\|
\end{equation}
Using the SVD of $\mat{A} = \mat{U\Sigma V}^T$, and since $\mat{U}$ preserves norms:
\begin{equation}
    \|\mat{A}\vec{x}\| = \|\mat{U\Sigma V}^T\vec{x}\| = \|\mat{\Sigma V}^T\vec{x}\|
\end{equation}
Let $\vec{w} = \mat{V}^T\vec{x}$. Since $\mat{V}^T$ is orthogonal, $\|\vec{w}\| = \|\vec{x}\|$. The expression becomes:
\begin{equation}
    \|\mat{\Sigma w}\| \le \sigma_1 \|\vec{w}\| = \sigma_1 \|\vec{x}\|
\end{equation}
The inequality holds because $\sigma_1$ is the largest scaling factor in the diagonal matrix $\mat{\Sigma}$. Combining our results:
\begin{equation}
    |\lambda| \|\vec{x}\| = \|\mat{A}\vec{x}\| \le \sigma_1 \|\vec{x}\|
\end{equation}
Dividing by $\|\vec{x}\|$ gives the final result: $|\lambda| \le \sigma_1$. This establishes that all eigenvalues of a matrix lie within a circle in the complex plane with a radius equal to the largest singular value.
\end{proof}

\section{SVD for Large-Scale Data}
In machine learning, we often work with a data matrix $\mat{A}$ of size $n \times d$, where $n$ is the number of samples and $d$ is the number of features. The computation of the SVD typically involves forming either $\mat{A}\mat{A}^T$ ($n \times n$) or $\mat{A}^T\mat{A}$ ($d \times d$). The choice depends on which dimension is smaller. However, for massive datasets where both $n$ and $d$ are very large, computing the full SVD is often computationally infeasible.

\subsection{Randomized SVD (rSVD)}
To handle large-scale matrices, approximate methods are necessary. A key algorithm is the **Randomized SVD (rSVD)**. This technique provides an approximate SVD that is much faster to compute by projecting the high-dimensional data onto a lower-dimensional subspace that captures most of the "action" of the matrix. It is widely used in practice for large-scale data analysis.

\subsection{Low-Rank Approximation as a Sum of Rank-One Matrices}
The theoretical foundation for using SVD for dimensionality reduction is its ability to provide the best low-rank approximation. A matrix $\mat{A}$ can be expressed as a sum of rank-one matrices:
\begin{equation}
    \mat{A} = \mat{U\Sigma V}^T = \sum_{i=1}^{r} \sigma_i \vec{u}_i \vec{v}_i^T
\end{equation}
where $r$ is the rank of $\mat{A}$, and $\vec{u}_i, \vec{v}_i$ are the $i$-th left and right singular vectors, respectively. The Eckart-Young-Mirsky theorem guarantees that the best rank-$k$ approximation to $\mat{A}$ is obtained by keeping only the first $k$ terms of this sum:
\begin{equation}
    \mat{A}_k = \sum_{i=1}^{k} \sigma_i \vec{u}_i \vec{v}_i^T
\end{equation}
This principle is the cornerstone of methods like Principal Component Analysis (PCA) and is what makes SVD so effective for data compression and noise reduction.

\newpage
\chapter{Lesson 30-09}

\section{SVD and Low-Rank Approximation}
The Singular Value Decomposition (SVD) is a fundamental tool for low-rank matrix approximation. The Eckart-Young theorem states that the best rank-$k$ approximation of a matrix $\mat{A}$ is found by truncating its SVD, retaining only the top $k$ singular values and their corresponding singular vectors.

This truncated reconstruction, denoted as $\mat{A}_k$, is given by the sum of $k$ rank-one matrices:
\begin{equation}
\mat{A}_k = \sum_{i=1}^{k} \sigma_i \vec{u}_i \vec{v}_i^T
\end{equation}
where $\sigma_i$ are the singular values, and $\vec{u}_i$ and $\vec{v}_i$ are the left and right singular vectors, respectively.

\subsection{A Hidden Assumption: The Problem of Noisy Data}
A critical assumption in the standard truncated SVD approach is that the matrix $\mat{A}$ is "clean," meaning it is free from noise. In many real-world applications, such as image processing or signal analysis, our data is corrupted.

Let's formalize this problem by considering a matrix $\mat{Y}$ which is a noisy version of a true, low-rank signal matrix $\mat{X}$:
\begin{equation}
\mat{Y} = \mat{X} + \mat{E}
\end{equation}
where $\mat{E}$ is a matrix of random noise (e.g., Gaussian noise).

The central question then becomes: if we compute the SVD of the noisy matrix $\mat{Y}$ and truncate it at some rank $k$, can we effectively recover the original clean matrix $\mat{X}$?

\section{The Effect of Noise on Singular Values}
The presence of noise perturbs both the singular values and singular vectors of the original signal. Consequently, the singular values of the noisy matrix $\mat{Y}$ are not the same as those of the clean matrix $\mat{X}$. A simple truncation of the SVD of $\mat{Y}$ at the rank of the original signal, rank$(\mat{X})$, is suboptimal because it will invariably retain a significant amount of the noise captured in the first $k$ singular components.

The primary goal is to find a principled method to separate the singular values that represent the underlying signal from those that are merely artifacts of the noise.

\subsection{The Core Idea: Hard Thresholding}
Instead of choosing a rank $k$ for truncation, we can define a threshold $\tau$. With this approach, we keep all singular values above this threshold and discard (set to zero) all singular values below it. This technique is known as **Hard Thresholding**.

The new, thresholded singular values, $\hat{\sigma}_i$, are defined as:
\begin{equation}
\hat{\sigma}_i = 
\begin{cases} 
\sigma_i & \text{if } \sigma_i \ge \tau \\
0 & \text{if } \sigma_i < \tau 
\end{cases}
\end{equation}
The denoised matrix, $\hat{\mat{X}}$, is then reconstructed using these thresholded singular values:
\begin{equation}
\hat{\mat{X}} = \mat{U} \hat{\mathbf{\Sigma}} \mat{V}^T = \sum_{i} \hat{\sigma}_i \vec{u}_i \vec{v}_i^T
\end{equation}
This raises the main question: How do we choose the threshold $\tau$ in a non-arbitrary, optimal way?

\subsection{Insight from Random Matrix Theory}
The key insight for selecting an optimal threshold comes from Random Matrix Theory (RMT), which studies the properties of matrices with random entries.

\begin{itemize}
    \item \textbf{Marchenko-Pastur Law:} For a large $m \times n$ matrix with i.i.d. random entries, the distribution of its singular values is predictable. Crucially, the largest singular value of a pure random noise matrix is not itself random; it converges to a specific upper bound. This implies that the singular values originating from noise are confined to a predictable bulk distribution.
    \item \textbf{Theorem (Donoho \& Gavish, 2014):} For a large rectangular matrix corrupted by Gaussian noise, a sharp phase transition occurs. The singular values corresponding to the underlying signal "pop out" above the bulk distribution of singular values generated by the noise. This separation allows for the determination of a theoretically optimal threshold.
\end{itemize}

\section{The Optimal Hard Threshold}
For a noisy matrix $\mat{Y} \in \R^{m \times n}$ (assuming $m \ge n$), the optimal hard threshold $\tau$ is given by the formula:
\begin{equation}
\tau = \omega(\beta) \cdot \sigma_{\text{med}}
\end{equation}
where:
\begin{itemize}
    \item $\beta = m/n$ is the aspect ratio of the matrix.
    \item $\sigma_{\text{med}}$ is the \textbf{median singular value} of the noisy matrix $\mat{Y}$. This serves as a robust estimator of the noise level.
    \item $\omega(\beta)$ is a value that depends only on the aspect ratio. It can be approximated by a polynomial fit. A commonly used approximation is:
    \begin{equation}
    \omega(\beta) \approx 0.56\beta^3 - 0.95\beta^2 + 1.82\beta + 1.43
    \end{equation}
\end{itemize}
This threshold is optimal in the sense that it minimizes the squared error between the denoised matrix and the true signal matrix in the limit of large dimensions.

\subsection{Special Case: Square Matrices with Known Noise}
In the simpler case where we have a noisy square matrix $\mat{Y} \in \R^{n \times n}$ and the magnitude of the noise $\gamma$ is known, the optimal threshold simplifies to:
\begin{equation}
\tau = \frac{4}{\sqrt{3}}\sqrt{n}\gamma
\end{equation}

\section{The Denoising Algorithm}
Given a noisy data matrix $\mat{Y} \in \R^{m \times n}$, the denoising process follows these steps:
\begin{enumerate}
    \item \textbf{Compute the SVD} of the noisy matrix: $\mat{Y} = \mat{U\Sigma V}^T$.
    \item \textbf{Estimate Noise Level:} Find the median of the singular values from the diagonal of $\mathbf{\Sigma}$: $\sigma_{\text{med}} = \text{median}(\text{diag}(\mathbf{\Sigma}))$.
    \item \textbf{Calculate Optimal Threshold:} Compute the aspect ratio $\beta = m/n$ and find the corresponding $\omega(\beta)$. Calculate the threshold $\tau = \omega(\beta) \sigma_{\text{med}}$.
    \item \textbf{Apply Thresholding:} Create a new diagonal matrix $\hat{\mathbf{\Sigma}}$ where $\hat{\sigma}_i = \sigma_i$ if $\sigma_i \ge \tau$ and 0 otherwise. The number of non-zero singular values is the estimated rank $\hat{k}$.
    \item \textbf{Reconstruct Denoised Matrix:} Compute the denoised matrix $\hat{\mat{X}} = \mat{U} \hat{\mathbf{\Sigma}} \mat{V}^T$.
\end{enumerate}

\section{Python Implementation Example}
Let's demonstrate this with an example. First, we construct a true, low-rank matrix $\mat{X}$ and visualize it.

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

# --- Generate the true signal matrix X ---
t = np.arange(-3, 3, 0.01)
Utrue = np.array([np.cos(17*t) * np.exp(-t**2), np.sin(11*t)]).T
Strue = np.array([[2, 0], [0, 0.5]])
Vtrue = np.array([np.sin(5*t) * np.exp(-t**2), np.cos(13*t)]).T
X = Utrue @ Strue @ Vtrue.T

plt.imshow(X)
plt.set_cmap('gray')
plt.axis('off')
plt.show()
\end{lstlisting}
The original matrix `X` shows a clear, structured wave-like pattern. Next, we add Gaussian noise to simulate a corrupted observation.

\begin{lstlisting}[language=Python]
# --- Add noise to create the noisy matrix Xnoisy ---
sigma = 1
Xnoisy = X + sigma * np.random.randn(*X.shape)

plt.imshow(Xnoisy)
plt.set_cmap('gray')
plt.axis('off')
plt.show()
\end{lstlisting}
The resulting image `Xnoisy` shows the original pattern heavily obscured by random noise. Now, we apply the optimal hard thresholding algorithm. For this square matrix example with known noise `sigma`, we use the simplified threshold formula.

\begin{lstlisting}[language=Python]
# --- Denoising Algorithm ---
U, S, VT = np.linalg.svd(Xnoisy, full_matrices=0)
N = Xnoisy.shape[0]

# Calculate the optimal hard threshold
cutoff = (4/np.sqrt(3)) * np.sqrt(N) * sigma

# Find the rank by keeping modes with singular values > cutoff
r = np.max(np.where(S > cutoff)) 

# Reconstruct the clean matrix
Xclean = U[:, :(r+1)] @ np.diag(S[:(r+1)]) @ VT[:(r+1), :]

plt.imshow(Xclean)
plt.set_cmap('gray')
plt.axis('off')
plt.show()
\end{lstlisting}
The reconstructed matrix `Xclean` successfully recovers the underlying structure of the original signal, with the majority of the noise removed.

\section{Alternative Rank Selection and Basis Comparison}

\subsection{Capturing Cumulative Energy with Singular Values}
The singular values of a matrix, typically denoted by $\sigma_i$ and arranged in descending order, represent the magnitude of the principal components of the data. The squared singular values are proportional to the variance captured by each corresponding singular vector. By computing the cumulative sum of these values (normalized by the total sum), we can determine how much of the original data's "energy" or information is retained when we use only a subset of the singular values and vectors for reconstruction.

This is visualized with a cumulative energy plot. The y-axis shows the fraction of total energy captured, and the x-axis shows the number of singular values used.

\begin{figure}[h!]
\centering
\includegraphics[width=0.3\textwidth]{cumulative_energy_plot.png}
\caption{A cumulative energy plot. To capture 90\% of the data's energy (y=0.9), we need to retain approximately 401 singular values, which is the rank of our approximation.}
\end{figure}

The Python implementation below demonstrates how to calculate the number of modes required to capture 90\% of the energy and generate this plot.

\begin{lstlisting}[language=Python]
# Assume 's' is the array of singular values from an SVD
import numpy as np
import matplotlib.pyplot as plt

# cdS is the cumulative energy
cdS = np.cumsum(s) / np.sum(s)

# r90 is the number of modes to capture 90% energy
r90 = np.min(np.where(cdS >= 0.9))

# The plot shows the cumulative energy and the threshold
fig3, ax3 = plt.subplots(1)
ax3.plot(cdS[:r90+1], 'o', color='b', linewidth=2)
ax3.plot(cdS[r90+1:], 'o', color='k', linewidth=2)
plt.xticks(np.array([0, 300, r90, 600]))
plt.yticks(np.array([0, 0.5, 0.9, 1]))

# Plot threshold lines
ax3.plot(np.array([r90, r90]), np.array([0, 0.9]), '--', color='b', linewidth=2)
ax3.plot(np.array([0, r90]), np.array([0.9, 0.9]), '--', color='b', linewidth=2)

ax3.grid()
plt.show()
\end{lstlisting}

\subsection{Comparison with Fourier Analysis: Data-Dependent vs. Fixed Basis}
It is insightful to compare the SVD basis with the basis used in Fourier analysis.

\begin{itemize}
    \item \textbf{Fourier Analysis}: A signal or function is represented as a sum of sine and cosine functions of varying frequencies and amplitudes. This basis of sines and cosines is \textbf{fixed} and universal; it does not depend on the specific data being analyzed.
    
    \item \textbf{SVD}: The basis vectors (columns of $\mat{U}$ and $\mat{V}$) are derived directly from the data matrix itself. This means the SVD provides a \textbf{data-dependent basis} that is optimally tailored to capture the variance within that specific dataset.
\end{itemize}
Because the SVD basis is specific to the data, it is often much more efficient for tasks like compression and dimensionality reduction compared to a general-purpose basis like the Fourier basis.

\section{The Challenge of Scale and Randomized Algorithms}
While SVD is powerful, its application to large-scale data presents significant computational challenges. This motivates the need for more efficient, approximate algorithms.

\subsection{The Johnson-Lindenstrauss (JL) Lemma}
The JL Lemma is a cornerstone result in dimensionality reduction, providing theoretical guarantees for random projections.

\begin{theorem}[Johnson-Lindenstrauss Lemma]
Let $0 < \epsilon < 1$. For any set $X$ of $N$ points in $\R^d$, there exists a linear map $f: \R^d \to \R^k$ where $k$ is on the order of
\begin{equation}
k = O\left(\frac{\log N}{\epsilon^2}\right)
\end{equation}
such that for all pairs of points $u, v \in X$:
\begin{equation}
(1 - \epsilon) \norm{u - v}_2^2 \leq \norm{f(u) - f(v)}_2^2 \leq (1 + \epsilon) \norm{u - v}_2^2
\end{equation}
\end{theorem}

\textbf{Key Idea:} The lemma guarantees that any set of high-dimensional points can be projected into a much lower-dimensional space while approximately preserving the pairwise distances between them. This is often described as preserving the "geometry" of the point cloud.

\textbf{The Surprising Part:} The target dimension $k$ is independent of the original dimension $d$. It only depends on the number of points $N$ and the desired error tolerance $\epsilon$. This is a profoundly counter-intuitive result that makes random projection so powerful for "big data".

\section{The Randomized SVD (rSVD) Algorithm}
The Randomized Singular Value Decomposition (rSVD) is a powerful and efficient algorithm for computing an approximate low-rank decomposition of a large matrix. It is significantly faster than traditional SVD for large, approximately low-rank matrices because it relies on matrix multiplications that can be highly optimized and performs the expensive SVD on a much smaller matrix.

The algorithm proceeds in two main stages.

\subsection{Stage 1: Finding an Approximate Basis for the Column Space}
The first stage aims to find a low-dimensional orthonormal basis that approximates the column space of the original matrix $\mat{A} \in \R^{m \times n}$.

\begin{enumerate}
    \item \textbf{Sketching:} We generate a random Gaussian matrix $\mathbf{\Omega} \in \R^{n \times k}$, where $k$ is the target rank of the approximation ($k \ll \min(m, n)$). This matrix is used to "sketch" the column space of $\mat{A}$.
    \item \textbf{Forming the Sketch Matrix:} We compute the matrix $\mat{Y} \in \R^{m \times k}$ by multiplying $\mat{A}$ by the random matrix $\mathbf{\Omega}$:
    \begin{equation}
        \mat{Y} = \mat{A} \mathbf{\Omega}
    \end{equation}
    The columns of $\mat{Y}$ are random linear combinations of the columns of $\mat{A}$. This "sketch" effectively captures the most important information in the column space of $\mat{A}$.
    \item \textbf{Orthonormalization:} To obtain an orthonormal basis, we perform an "economic" QR decomposition on $\mat{Y}$:
    \begin{equation}
        \mat{Y} = \mat{Q} \mat{R}
    \end{equation}
    where $\mat{Q} \in \R^{m \times k}$ is a matrix with orthonormal columns, and $\mat{R} \in \R^{k \times k}$ is an upper triangular matrix. The matrix $\mat{Q}$ serves as an approximate basis for the column space of $\mat{A}$.
\end{enumerate}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{image.png}
    \caption{Graphical representation of Stage 1 of the rSVD algorithm.}
\end{figure}

\subsection{Stage 2: Projecting onto the Low-Rank Subspace}
In the second stage, we project the original matrix $\mat{A}$ onto the low-dimensional subspace spanned by the columns of $\mat{Q}$ and then perform SVD on this smaller projected matrix.

\begin{enumerate}
    \item \textbf{Projection:} We project $\mat{A}$ onto the basis $\mat{Q}$ to create a much smaller matrix $\mat{B} \in \R^{k \times n}$:
    \begin{equation}
        \mat{B} = \mat{Q}^T \mat{A}
    \end{equation}
    \item \textbf{SVD of the Small Matrix:} We then compute the SVD of the smaller matrix $\mat{B}$:
    \begin{equation}
        \mat{B} = \tilde{\mat{U}} \mathbf{\Sigma} \mat{V}^T
    \end{equation}
    This step is computationally efficient because $\mat{B}$ is much smaller than $\mat{A}$. A key property of this method is that the singular values in $\mathbf{\Sigma}$ and the right singular vectors in $\mat{V}^T$ are also the approximate singular values and right singular vectors of the original matrix $\mat{A}$.
    
    \item \textbf{Recovering Left Singular Vectors:} The left singular vectors of $\mat{A}$, denoted by $\mat{U}$, can be recovered by "lifting" the left singular vectors of $\mat{B}$, $\tilde{\mat{U}}$, back into the original higher-dimensional space using $\mat{Q}$:
    \begin{equation}
        \mat{U} = \mat{Q} \tilde{\mat{U}}
    \end{equation}
\end{enumerate}
This yields the approximate low-rank SVD of the original matrix $\mat{A} \approx \mat{U} \mathbf{\Sigma} \mat{V}^T$.

\subsection{Key Advantages and Considerations}
\subsubsection{Advantages of rSVD}
\begin{itemize}
    \item \textbf{Speed:} rSVD is significantly faster than traditional SVD for large, approximately low-rank matrices. It involves highly optimized matrix multiplications and a much smaller SVD computation.
    \item \textbf{Scalability:} The algorithm can be adapted for matrices that do not fit into memory.
    \item \textbf{Accuracy:} It provides strong probabilistic guarantees on the quality of the approximation.
\end{itemize}

\subsubsection{Important Considerations}
\begin{itemize}
    \item \textbf{Oversampling:} For better accuracy, the sketch size $k$ is usually chosen to be slightly larger than the desired rank $r$. A common choice is $k = r + p$, where $p$ is a small oversampling parameter (e.g., $p=5$ or $p=10$). This helps to capture more of the action of the matrix.
    \item \textbf{Power Iterations:} Accuracy can be further improved by using a few power iterations to better capture the top singular vectors. Instead of using $\mat{Y} = \mat{A}\mathbf{\Omega}$, we can use:
    \begin{equation}
        \mat{Y} = (\mat{A}\mat{A}^T)^q \mat{A} \mathbf{\Omega}
    \end{equation}
    where $q$ is a small integer (e.g., 1 or 2). This technique amplifies the decay of singular values, making the dominant singular vectors more prominent and easier to capture.
\end{itemize}

\subsection{Python Implementation}
Here is a simple implementation of the rSVD algorithm in Python using NumPy, followed by a demonstration of its application.

\subsubsection{Standard SVD for Comparison}
First, let's compute the SVD of a small matrix using NumPy's standard SVD function to have a baseline for comparison.
\begin{lstlisting}
import numpy as np

A = np.array([[1, 3, 2],
              [5, 3, 1],
              [3, 4, 5]])

u, s, v = np.linalg.svd(A, full_matrices=0)
print('Left singular vectors:')
print(u)
print('Singular values:')
print(s)
print('Right singular vectors:')
print(v)
\end{lstlisting}
The output provides the true left singular vectors, singular values, and right singular vectors of matrix $\mat{A}$.

\subsubsection{rSVD Implementation}
The rSVD algorithm can be implemented in a few lines of code. The function below implements the two stages described previously.
\begin{lstlisting}
import numpy as np

def rsvd(A, Omega):
    Y = A @ Omega
    Q, _ = np.linalg.qr(Y)
    B = Q.T @ A
    u_tilde, s, v = np.linalg.svd(B, full_matrices=0)
    u = Q @ u_tilde
    return u, s, v
\end{lstlisting}
Now, let's apply this function to the same matrix $\mat{A}$ with a target rank of 2. We generate a random matrix $\mathbf{\Omega}$ for this purpose.
\begin{lstlisting}
np.random.seed(1000)

A = np.array([[1, 3, 2],
              [5, 3, 1],
              [3, 4, 5]])

rank = 2
Omega = np.random.randn(A.shape[1], rank)
u, s, v = rsvd(A, Omega)

print('Left singular vectors:')
print(u)
print('Singular values:')
print(s)
print('Right singular vectors:')
print(v)
\end{lstlisting}
Comparing the results, we can see that the singular values (9.34, 3.02) are very close to the top two singular values from the full SVD (9.34, 3.24). The singular vectors are also very similar, demonstrating the effectiveness of the approximation.

\subsubsection{Illustrating Power Iterations}
Power iterations can significantly improve the accuracy of the approximation by emphasizing the larger singular values. The following code snippet generates a random matrix and shows how the singular value spectrum from rSVD approaches the true SVD spectrum as the number of power iterations, $q$, increases. The plot clearly shows that as $q$ goes from 1 to 5, the curve for rSVD gets progressively closer to the black line representing the true SVD. This demonstrates that a small number of power iterations can drastically improve the approximation.

\section{Matrix Norms and the Eckart-Young-Mirsky Theorem}

\subsection{The Frobenius Norm}
\subsubsection{Relation to Singular Values}
The Frobenius norm of a matrix can be directly related to its singular values. This provides an alternative and powerful way to compute and interpret the norm.

\begin{theorem}
The squared Frobenius norm of a matrix $A$ is equal to the sum of the squares of its singular values.
\begin{equation}
    \norm{A}_F^2 = \sum_i \sigma_i^2
\end{equation}
\end{theorem}

\begin{proof}
The proof combines three fundamental properties of matrices.
\begin{enumerate}
    \item We start from the previous result relating the Frobenius norm to the trace:
    \begin{equation*}
        \norm{A}_F^2 = \operatorname{tr}(A^H A)
    \end{equation*}

    \item A fundamental property of the trace is that it equals the sum of the eigenvalues of the matrix. Let $\lambda_j$ be the eigenvalues of $A^H A$. Then:
    \begin{equation*}
        \operatorname{tr}(A^H A) = \sum_{j=1}^{n} \lambda_j(A^H A)
    \end{equation*}

    \item By definition, the eigenvalues of the matrix $A^H A$ are the squares of the singular values of $A$.
    \begin{equation*}
        \lambda_j(A^H A) = \sigma_j(A)^2
    \end{equation*}
\end{enumerate}

Combining these facts, we substitute the eigenvalues in the trace expression with the squared singular values:
\begin{equation}
    \norm{A}_F^2 = \operatorname{tr}(A^H A) = \sum_{j=1}^{n} \lambda_j(A^H A) = \sum_{j=1}^{\operatorname{rank}(A)} \sigma_j^2
\end{equation}
This completes the proof. Note that the sum is up to the rank of $A$, as any singular values beyond the rank will be zero.
\end{proof}

\subsubsection{Unitary Invariance}

\begin{property}
The Frobenius norm is invariant under multiplication by unitary (or orthogonal for real matrices) matrices.
\end{property}

\begin{theorem}
Let $A \in \mathbb{C}^{m \times n}$, and let $Q \in \mathbb{C}^{m \times m}$ and $P \in \mathbb{C}^{n \times n}$ be unitary matrices (i.e., $Q^H Q = I$ and $P^H P = I$). Then:
\begin{equation}
    \norm{QA}_F = \norm{A}_F \quad \text{and} \quad \norm{AP}_F = \norm{A}_F
\end{equation}
\end{theorem}

\paragraph{Geometric Interpretation}
This property means that rotations and reflections do not change the "size" of the matrix as measured by the Frobenius norm. Since unitary and orthogonal matrices represent these geometric transformations, their application (either pre- or post-multiplication) preserves the Frobenius norm.

\begin{proof}[Proof of Unitary Invariance]
We use the property $\norm{M}_F^2 = \operatorname{tr}(M^H M)$. Let's prove the case for pre-multiplication, i.e., $\norm{QA}_F = \norm{A}_F$.

We start with the squared norm:
\begin{equation*}
    \norm{QA}_F^2 = \operatorname{tr}((QA)^H (QA))
\end{equation*}
Using the property of the conjugate transpose of a product, $(XY)^H = Y^H X^H$, we get:
\begin{equation*}
    \norm{QA}_F^2 = \operatorname{tr}(A^H Q^H Q A)
\end{equation*}
Since $Q$ is unitary, we know that $Q^H Q = I$. Substituting this into the equation:
\begin{equation*}
    \operatorname{tr}(A^H Q^H Q A) = \operatorname{tr}(A^H I A) = \operatorname{tr}(A^H A)
\end{equation*}
By definition, $\operatorname{tr}(A^H A) = \norm{A}_F^2$. Therefore:
\begin{equation*}
    \norm{QA}_F^2 = \norm{A}_F^2 \implies \norm{QA}_F = \norm{A}_F
\end{equation*}
For the second part, $\norm{AP}_F = \norm{A}_F$, we use the cyclic property of the trace, which states that $\operatorname{tr}(XYZ) = \operatorname{tr}(ZXY)$.
\begin{equation*}
    \norm{AP}_F^2 = \operatorname{tr}((AP)^H (AP)) = \operatorname{tr}(P^H A^H A P)
\end{equation*}
Applying the cyclic property by letting $X=P^H$, $Y=A^H A$, and $Z=P$:
\begin{equation*}
    \operatorname{tr}(P^H A^H A P) = \operatorname{tr}(A^H A P P^H)
\end{equation*}
Since $P$ is unitary, $P P^H = I$. Thus:
\begin{equation*}
    \operatorname{tr}(A^H A P P^H) = \operatorname{tr}(A^H A I) = \operatorname{tr}(A^H A) = \norm{A}_F^2
\end{equation*}
This shows that $\norm{AP}_F^2 = \norm{A}_F^2$, completing the proof.
\end{proof}

\subsection{Induced (or Operator) p-Norms}
An induced matrix norm is a norm that is defined in terms of a vector norm.

\begin{definition}
Given a vector norm $\norm{\cdot}_p$, the corresponding induced matrix norm is defined as the maximum "stretching factor" that the matrix applies to any non-zero vector.
\begin{equation}
    \norm{A}_p = \sup_{x \neq 0} \frac{\norm{Ax}_p}{\norm{x}_p} = \sup_{\norm{x}_p=1} \norm{Ax}_p
\end{equation}
\end{definition}

\paragraph{Important Cases}
\begin{itemize}
    \item \textbf{p = 1}: Max absolute column sum.
    \item \textbf{p = 2}: The Spectral Norm.
    \item \textbf{p = $\infty$}: Max absolute row sum.
\end{itemize}

\begin{property}
All induced norms are sub-multiplicative by definition.
\end{property}

\begin{proof}[Proof of Sub-multiplicativity of Induced Norms]
We want to show that $\norm{AB}_p \leq \norm{A}_p \norm{B}_p$.
From the definition of the induced norm, for any vector $x$, we have:
\begin{equation}
    \norm{Mx}_p \leq \norm{M}_p \norm{x}_p
\end{equation}
Let's apply this to the matrix product $AB$ acting on a vector $x$:
\begin{equation*}
    \norm{(AB)x}_p = \norm{A(Bx)}_p
\end{equation*}
Let $M=A$ and treat $Bx$ as a vector. Using the inequality above, we get:
\begin{equation*}
    \norm{A(Bx)}_p \leq \norm{A}_p \norm{Bx}_p
\end{equation*}
Now, we apply the property again to $\norm{Bx}_p$:
\begin{equation*}
    \norm{Bx}_p \leq \norm{B}_p \norm{x}_p
\end{equation*}
Combining the inequalities:
\begin{equation*}
    \norm{(AB)x}_p \leq \norm{A}_p \norm{B}_p \norm{x}_p
\end{equation*}
Dividing by $\norm{x}_p$ (for $x \neq 0$) and taking the supremum over all such $x$ gives the desired result:
\begin{equation*}
    \sup_{x \neq 0} \frac{\norm{(AB)x}_p}{\norm{x}_p} \leq \norm{A}_p \norm{B}_p
\end{equation*}
\begin{equation}
    \norm{AB}_p \leq \norm{A}_p \norm{B}_p
\end{equation}
\end{proof}

\subsection{The Spectral Norm: $\norm{A}_2$}

\begin{definition}
The spectral norm is the induced 2-norm, corresponding to the standard Euclidean vector norm. It is one of the most important matrix norms in linear algebra and numerical analysis.
\begin{equation}
    \norm{A}_2 = \sup_{\norm{x}_2=1} \norm{Ax}_2
\end{equation}
\end{definition}

\begin{theorem}
The spectral norm of a matrix $A$ is equal to its largest singular value, $\sigma_{\max}(A)$.
\begin{equation}
    \norm{A}_2 = \sigma_1 = \sqrt{\lambda_{\max}(A^H A)}
\end{equation}
where $\lambda_{\max}(A^H A)$ is the largest eigenvalue of the matrix $A^H A$.
\end{theorem}

\paragraph{Geometric Interpretation} The spectral norm $\norm{A}_2$ represents the largest possible stretching (or scaling) of a unit vector under the linear transformation defined by $A$.

\begin{proof}[Proof: $\norm{A}_2 = \sigma_1$]
We start with the squared norm, based on the definition:
\begin{equation*}
    \norm{A}_2^2 = \sup_{\norm{x}_2=1} \norm{Ax}_2^2
\end{equation*}
We can write the squared 2-norm of the vector $Ax$ as a dot product:
\begin{equation*}
    \norm{Ax}_2^2 = (Ax)^H(Ax) = x^H A^H A x
\end{equation*}
Let $M = A^H A$. The matrix $M$ is Hermitian and positive semi-definite. Its eigenvalues $\lambda_i$ are real and non-negative, and it has an orthonormal basis of eigenvectors $\{v_i\}$. The expression $x^H M x$ is the Rayleigh quotient. By the Rayleigh-Ritz theorem, its maximum value over all unit vectors $x$ is the largest eigenvalue of $M$.
\begin{equation*}
    \sup_{\norm{x}_2=1} x^H A^H A x = \lambda_{\max}(A^H A)
\end{equation*}
By definition, the eigenvalues of $A^H A$ are the squares of the singular values of $A$. Let $\sigma_1$ be the largest singular value. Then:
\begin{equation*}
    \lambda_{\max}(A^H A) = \sigma_1(A)^2
\end{equation*}
Therefore,
\begin{equation*}
    \norm{A}_2^2 = \sigma_1^2
\end{equation*}
Taking the square root gives the final result:
\begin{equation}
    \norm{A}_2 = \sigma_1
\end{equation}
\end{proof}

\subsubsection{Application: Proof of the Eckart-Young-Mirsky Theorem (Spectral Norm)}
The Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation to a matrix $A$ under the spectral norm is given by its truncated SVD, $A_k$. The error of this approximation is $\norm{A - A_k}_2 = \sigma_{k+1}$.

To prove this, we must show two parts:
\begin{enumerate}
    \item \textbf{Upper Bound:} The error for $A_k$ is exactly $\sigma_{k+1}$. This is established by noting that $A - A_k = \sum_{i=k+1}^{r} \sigma_i u_i v_i^T$, and the spectral norm of this matrix is its largest singular value, which is $\sigma_{k+1}$.
    \item \textbf{Lower Bound:} For any arbitrary matrix $B$ with $\text{rank}(B) \le k$, the error $\norm{A - B}_2$ is at least $\sigma_{k+1}$.
\end{enumerate}
This section focuses on proving the lower bound.

\paragraph{Constructing the Test Vector}
Let $B$ be any matrix with $\text{rank}(B) \le k$. The dimension of the null space of $B$, denoted $\text{null}(B)$, is given by the rank-nullity theorem: $\dim(\text{null}(B)) = n - \text{rank}(B) \ge n-k$.

Let the right singular vectors of $A$ be $\{v_1, v_2, \dots, v_n\}$. Consider the subspace $S$ spanned by the first $k+1$ of these vectors:
\begin{equation}
S = \text{span}\{v_1, v_2, \dots, v_{k+1}\}
\end{equation}
The dimension of this subspace is $\dim(S) = k+1$. Since the sum of the dimensions of $\text{null}(B)$ and $S$ is $(\ge n-k) + (k+1) = n+1$, which is greater than the dimension of the ambient space $n$, their intersection must be non-trivial. That is, there must exist a non-zero vector $z$ such that:
\begin{equation}
z \in \text{null}(B) \cap S
\end{equation}
We can construct such a vector $z$ to be a unit vector, meaning $\norm{z}_2 = 1$, which can be expressed as:
\begin{equation}
z = \sum_{i=1}^{k+1} c_i v_i \quad \text{with} \quad \sum_{i=1}^{k+1} c_i^2 = 1
\end{equation}

\paragraph{Derivation of the Lower Bound}
The spectral norm of a matrix $M$ is defined as $\norm{M}_2 = \max_{\norm{x}_2=1} \norm{Mx}_2$. Therefore, for our specific unit vector $z$, the following inequality holds:
\begin{equation}
\norm{A - B}_2^2 \ge \norm{(A - B)z}_2^2
\end{equation}
Since $z$ is in the null space of $B$, we have $Bz = 0$. The expression simplifies to:
\begin{equation}
\norm{(A - B)z}_2^2 = \norm{Az - Bz}_2^2 = \norm{Az}_2^2
\end{equation}
Now, we compute $Az$ explicitly using the SVD of $A$:
\begin{equation}
Az = \left(\sum_{j=1}^{r} \sigma_j u_j v_j^T\right) \left(\sum_{i=1}^{k+1} c_i v_i\right) = \sum_{i=1}^{k+1} c_i \sigma_i u_i
\end{equation}
Due to the orthogonality of the singular vectors. Taking the squared 2-norm:
\begin{equation}
\norm{Az}_2^2 = \left\| \sum_{i=1}^{k+1} c_i \sigma_i u_i \right\|_2^2 = \sum_{i=1}^{k+1} (c_i \sigma_i)^2 = \sum_{i=1}^{k+1} c_i^2 \sigma_i^2
\end{equation}

\paragraph{Final Step of the Proof}
The singular values are sorted such that $\sigma_i \ge \sigma_{k+1}$ for all $i \le k+1$. Thus:
\begin{equation}
\sum_{i=1}^{k+1} c_i^2 \sigma_i^2 \ge \sum_{i=1}^{k+1} c_i^2 \sigma_{k+1}^2 = \sigma_{k+1}^2 \sum_{i=1}^{k+1} c_i^2
\end{equation}
Since $\sum_{i=1}^{k+1} c_i^2 = 1$, we get our result:
\begin{equation}
\norm{Az}_2^2 \ge \sigma_{k+1}^2
\end{equation}
Combining our results, we have shown that for any matrix $B$ with $\text{rank}(B) \le k$:
\begin{equation}
\norm{A - B}_2^2 \ge \sigma_{k+1}^2 \implies \norm{A-B}_2 \ge \sigma_{k+1}
\end{equation}
This completes the proof. The optimal rank-$k$ approximation $A_k$ achieves this lower bound, making it the best possible approximation under the spectral norm.

\chapter{Lesson 07-10}

\section*{Introduction to Numerical Linear Algebra for ML}

\section{Detailed Proofs of the Eckart-Young-Mirsky Theorem}
The Eckart-Young-Mirsky theorem provides the optimal low-rank approximation of a matrix. This result is fundamental in numerical linear algebra and has widespread applications in machine learning, particularly in dimensionality reduction and data compression. The theorem holds for any unitarily invariant norm, but its most important applications concern the spectral norm (2-norm) and the Frobenius norm.

\subsection{Statement of the Theorem}

\subsubsection{Setup}
Let $\mat{A} \in \R^{m \times n}$ be a matrix of rank $r$. Its Singular Value Decomposition (SVD) is given by:
\begin{equation}
\mat{A} = \mat{U} \mat{\Sigma} \mat{V}^T
\end{equation}
with singular values ordered non-increasingly: $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$.

\subsubsection{Low-Rank Approximation $\mat{A}_k$}
For any integer $k < r$, the truncated SVD gives the rank-$k$ matrix $\mat{A}_k$:
\begin{equation}
\mat{A}_k = \sum_{i=1}^{k} \sigma_i \vec{u}_i \vec{v}_i^T
\end{equation}
where $\vec{u}_i$ and $\vec{v}_i$ are the $i$-th columns of $\mat{U}$ and $\mat{V}$ respectively.

\begin{theorem}[Eckart-Young-Mirsky]
For any matrix $\mat{B} \in \R^{m \times n}$ with $\text{rank}(\mat{B}) \leq k$, the matrix $\mat{A}_k$ is the best rank-$k$ approximation to $\mat{A}$ in both the spectral and Frobenius norms.

\begin{enumerate}
    \item \textbf{Spectral Norm:}
    \[
    \norm{\mat{A} - \mat{A}_k}_2 = \min_{\text{rank}(\mat{B}) \leq k} \norm{\mat{A} - \mat{B}}_2 = \sigma_{k+1}
    \]
    \item \textbf{Frobenius Norm:}
    \[
    \norm{\mat{A} - \mat{A}_k}_F = \min_{\text{rank}(\mat{B}) \leq k} \norm{\mat{A} - \mat{B}}_F = \sqrt{\sum_{i=k+1}^{r} \sigma_i^2}
    \]
\end{enumerate}
\end{theorem}

\section{Proof for the Frobenius Norm}
The proof for the Frobenius norm relies on a powerful result from matrix analysis known as Weyl's Inequality for singular values.

\subsection{Key Tool: Weyl's Inequality}
\begin{theorem}[Weyl's Inequality for Singular Values]
For any two matrices $\mat{X}, \mat{Y} \in \R^{m \times n}$, the following inequality holds for their singular values:
\begin{equation}
\sigma_{i+j-1}(\mat{X} + \mat{Y}) \leq \sigma_i(\mat{X}) + \sigma_j(\mat{Y})
\end{equation}
\end{theorem}

\subsubsection{Our Application of Weyl's Inequality}
Let $\mat{B}$ be any matrix of rank $k$. This implies that its $(k+1)$-th singular value and all subsequent ones are zero: $\sigma_{k+1}(\mat{B}) = 0$. We apply the inequality with $\mat{X} = \mat{A} - \mat{B}$ and $\mat{Y} = \mat{B}$. Let $j = k+1$. Then for any $i \geq 1$:
\[
\sigma_{i+(k+1)-1}(\mat{A}) \leq \sigma_i(\mat{A} - \mat{B}) + \sigma_{k+1}(\mat{B})
\]
Since $\sigma_{k+1}(\mat{B}) = 0$, we get a crucial relationship:
\begin{equation}
\sigma_{i+k}(\mat{A}) \leq \sigma_i(\mat{A} - \mat{B})
\end{equation}

\subsection{Step 1: Expressing the Error of $\mat{A}_k$}
First, let's explicitly write the squared Frobenius norm of the error for our optimal matrix $\mat{A}_k$. The Frobenius norm of a matrix is the square root of the sum of squares of its singular values. The difference matrix $\mat{A} - \mat{A}_k$ is:
\[
\mat{A} - \mat{A}_k = \left(\sum_{i=1}^{r} \sigma_i \vec{u}_i \vec{v}_i^T\right) - \left(\sum_{i=1}^{k} \sigma_i \vec{u}_i \vec{v}_i^T\right) = \sum_{i=k+1}^{r} \sigma_i \vec{u}_i \vec{v}_i^T
\]
The singular values of the matrix $(\mat{A} - \mat{A}_k)$ are precisely the "neglected" singular values of $\mat{A}$:
\[
\{\sigma_{k+1}(\mat{A}), \sigma_{k+2}(\mat{A}), \dots, \sigma_r(\mat{A})\}
\]
Therefore, its squared Frobenius norm is:
\begin{equation}
\norm{\mat{A} - \mat{A}_k}_F^2 = \sum_{i=k+1}^{r} \sigma_i(\mat{A})^2
\end{equation}

\subsection{Step 2: Connecting the Errors with Weyl's Inequality}
Now we use the result from Weyl's inequality, $\sigma_{i+k}(\mat{A}) \leq \sigma_i(\mat{A} - \mat{B})$, to relate the error of $\mat{A}_k$ to the error of any other rank-$k$ matrix $\mat{B}$. Let's re-index the sum for $\norm{\mat{A} - \mat{A}_k}_F^2$ by letting $j = i - k$ (so $i = j + k$):
\[
\norm{\mat{A} - \mat{A}_k}_F^2 = \sum_{i=k+1}^{r} \sigma_i(\mat{A})^2 = \sum_{j=1}^{r-k} \sigma_{j+k}(\mat{A})^2
\]
Now, apply Weyl's inequality to each term in the sum:
\[
\sum_{j=1}^{r-k} \sigma_{j+k}(\mat{A})^2 \leq \sum_{j=1}^{r-k} \sigma_j(\mat{A} - \mat{B})^2
\]
The sum on the right is a sum of some of the squared singular values of the matrix $(\mat{A} - \mat{B})$.

\subsection{Step 3: Finalizing the Proof}
We have the inequality:
\[
\norm{\mat{A} - \mat{A}_k}_F^2 \leq \sum_{j=1}^{r-k} \sigma_j(\mat{A} - \mat{B})^2
\]
The full squared Frobenius norm of $(\mat{A} - \mat{B})$ is the sum of \textit{all} its squared singular values:
\[
\norm{\mat{A} - \mat{B}}_F^2 = \sum_{j=1}^{\min(m, n)} \sigma_j(\mat{A} - \mat{B})^2
\]
Since the terms in the sum (squared singular values) are non-negative, we have:
\[
\sum_{j=1}^{r-k} \sigma_j(\mat{A} - \mat{B})^2 \leq \sum_{j=1}^{\min(m, n)} \sigma_j(\mat{A} - \mat{B})^2 = \norm{\mat{A} - \mat{B}}_F^2
\]

\subsubsection{Conclusion}
Combining the steps, we get:
\[
\norm{\mat{A} - \mat{A}_k}_F^2 \leq \norm{\mat{A} - \mat{B}}_F^2
\]
Taking the square root of both sides gives the final result:
\[
\norm{\mat{A} - \mat{A}_k}_F \leq \norm{\mat{A} - \mat{B}}_F
\]
This proves that $\mat{A}_k$ is the best rank-$k$ approximation to $\mat{A}$ in the Frobenius norm.

\section*{Principal Component Analysis (PCA)}

\section{The Goal of PCA: An Intuitive View}
\subsection{Core Idea}
Principal Component Analysis is a dimensionality reduction technique used to transform a high-dimensional dataset into a lower-dimensional one, while retaining as much of the original "information" as possible. The "information" is quantified by the variance in the data.

\begin{itemize}
    \item PCA finds a new set of coordinates (or axes), called \textbf{Principal Components}.
    \item These components are ordered such that the first component captures the largest possible variance in the data.
    \item Each subsequent component is orthogonal to the previous ones and captures the largest possible remaining variance.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{pca_figure.png}
    \caption{PCA finds the direction of maximum variance (PC1) and then subsequent orthogonal directions (PC2).}
    \label{fig:pca}
\end{figure}
\textit{Description of Figure \ref{fig:pca}: The figure displays a scatter plot of data points that are correlated and elongated along a diagonal axis. The first principal component (PCA 1st Dimension) is an arrow aligned with this direction of maximum variance. The second principal component (PCA 2nd Dimension) is an arrow orthogonal to the first, capturing the remaining variance.}

\subsection{The Mathematics of Maximizing Variance}
The process of finding these principal components can be broken down into three main mathematical steps:
\begin{enumerate}
    \item \textbf{Center the Data:} Start with a data matrix $\mat{X} \in \R^{n \times p}$ ($n$ samples, $p$ features). First, make each feature (column) have a mean of zero by subtracting the column mean from each element in that column.

    \item \textbf{Covariance Matrix:} The goal is to find directions that maximize variance. This information is captured by the sample covariance matrix $\mat{C}$:
    \begin{equation}
    \mat{C} = \frac{1}{n-1} \mat{X}^T \mat{X} \in \R^{p \times p}
    \end{equation}
    The diagonal entries $C_{ii}$ are the variances of each feature, and the off-diagonal entries $C_{ij}$ are the covariances between features.

    \item \textbf{Eigendecomposition:} PCA solves this problem by finding the eigenvectors and eigenvalues of the covariance matrix.
    \begin{equation}
    \mat{C} \vec{v}_j = \lambda_j \vec{v}_j
    \end{equation}
    \begin{itemize}
        \item The eigenvectors $\vec{v}_j$ are the \textbf{Principal Components}. They give the directions of maximum variance.
        \item The eigenvalues $\lambda_j$ give the amount of variance captured by each principal component.
    \end{itemize}
\end{enumerate}

\section{A More Stable Approach: The SVD-PCA Connection}
Computing the covariance matrix $\mat{X}^T \mat{X}$ can be numerically unstable, especially for high-dimensional or ill-conditioned data. A more robust method is to use the Singular Value Decomposition (SVD) of the centered data matrix $\mat{X}$ directly.

\subsection{Deriving Eigendecomposition from SVD}
Let $\mat{X} \in \R^{n \times p}$ be the centered data matrix. The SVD of $\mat{X}$ is $\mat{X} = \mat{U\Sigma V}^T$. By substituting this into the covariance matrix formula, we can establish a direct relationship between SVD and the eigendecomposition of $\mat{C}$.

\begin{align*}
    \mat{C} &= \frac{1}{n-1} \mat{X}^T \mat{X} \\
    &= \frac{1}{n-1} (\mat{U\Sigma V}^T)^T (\mat{U\Sigma V}^T) \\
    &= \frac{1}{n-1} (\mat{V\Sigma}^T \mat{U}^T) (\mat{U\Sigma V}^T)
\end{align*}
Since the columns of $\mat{U}$ are orthonormal, $\mat{U}^T\mat{U} = \mat{I}$. This simplifies the expression:
\begin{align*}
    \mat{C} &= \frac{1}{n-1} \mat{V\Sigma}^T (\mat{U}^T\mat{U}) \mat{\Sigma V}^T \\
    &= \frac{1}{n-1} \mat{V} (\mat{\Sigma}^T \mat{\Sigma}) \mat{V}^T
\end{align*}
Let $\mat{\Lambda} = \frac{1}{n-1} \mat{\Sigma}^T \mat{\Sigma}$. The matrix $\mat{\Sigma}^T \mat{\Sigma}$ is a diagonal matrix of size $p \times p$ whose diagonal entries are the squared singular values $\sigma_j^2$. Therefore, the final expression is:
\begin{equation}
    \mat{C} = \mat{V} \left( \frac{\mat{\Sigma}^2}{n-1} \right) \mat{V}^T
\end{equation}
This is precisely the eigendecomposition of the covariance matrix $\mat{C}$, where $\mat{C} = \mat{V\Lambda V}^T$.

The derivation reveals two fundamental connections between SVD and PCA:
\begin{itemize}
    \item The columns of $\mat{V}$ (the right singular vectors of the centered data matrix $\mat{X}$) are the principal components of the data. These are the eigenvectors of the covariance matrix $\mat{C}$.
    \item The eigenvalues $\lambda_j$ of the covariance matrix $\mat{C}$ are directly related to the singular values $\sigma_j$ of $\mat{X}$ by the formula:
    \begin{equation*}
        \lambda_j = \frac{\sigma_j^2}{n-1}
    \end{equation*}
\end{itemize}
This means we can find the principal components and their corresponding variances (eigenvalues) by performing an SVD on the data matrix, which is often numerically more stable than forming and decomposing the covariance matrix.

\subsection{Computational Considerations for Large Datasets}
For very large data matrices, computing the full SVD can be computationally expensive. In such cases, a practical alternative is to use a \textbf{randomized SVD}. This method provides an approximation of the singular vectors and values, which in turn gives an approximation of the principal components and eigenvalues. While this introduces a level of approximation, the error is controllable, offering a trade-off between computational cost and accuracy.

\section{A Crucial Distinction: PCA vs. Linear Regression}
Although both PCA and Linear Regression can be used to fit a line to a dataset, their objectives and the errors they minimize are fundamentally different. Understanding this distinction is crucial for applying the correct method.

\subsection{Different Objectives and Error Metrics}
The core difference lies in their goals and the definition of error.
\begin{itemize}
    \item \textbf{Principal Component Analysis (PCA):}
    \begin{itemize}
        \item \textbf{Goal:} To describe the data and find the best low-dimensional representation by identifying the directions of maximum variance.
        \item \textbf{Error:} Minimizes the \textbf{orthogonal distance} (or projection error) from each data point to the principal component line (or hyperplane).
        \item \textbf{Symmetry:} Treats all variables equally. There is no distinction between dependent and independent variables.
    \end{itemize}
    \item \textbf{Linear Regression (Least Squares):}
    \begin{itemize}
        \item \textbf{Goal:} To predict a specific dependent variable ($y$) from a set of independent variables ($x$).
        \item \textbf{Error:} Minimizes the \textbf{vertical distance} (the residual) from each data point to the regression line.
        \item \textbf{Asymmetry:} Assumes an explicit relationship where $x$ is a predictor and $y$ is a response. Swapping these roles changes the resulting model.
    \end{itemize}
\end{itemize}

\subsection{Visualizing the Difference}
The difference in the error metric leads to different optimal lines. For a 2D dataset, linear regression minimizes the sum of squared vertical distances (the $y$-residuals), fitting a line that best predicts $y$ from $x$. In contrast, PCA finds the line that best captures the total variance in the data, which is achieved by minimizing the sum of squared orthogonal distances to that line.

The line derived from PCA and the line from Linear Regression are generally \textbf{not the same}. They solve different problems and answer different questions about the data.

\section{PCA Summary and Python Implementation}
PCA is a powerful tool for dimensionality reduction that finds the directions of maximum variance in a dataset.
\begin{itemize}
    \item These directions, the \textbf{principal components}, are the eigenvectors of the data's covariance matrix.
    \item A more stable way to find them is by computing the SVD of the centered data matrix; the principal components are the columns of the right singular vector matrix $\mat{V}$.
    \item PCA is fundamentally different from Linear Regression as it minimizes orthogonal projection errors, not vertical prediction errors.
\end{itemize}
The following Python code demonstrates how to compute and visualize principal components on a synthetic 2D dataset using SVD.

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

# 1. Generate synthetic data
nPoints = 10000
xC = np.array([2, 1])  # Center of data
sig = np.array([2, 0.5]) # Principal axes variances
theta = np.pi / 3 # Rotate cloud by pi/3
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta), np.cos(theta)]]) # Rotation matrix

# Create 10,000 points
X = R @ np.diag(sig) @ np.random.randn(2, nPoints) + np.diag(xC) @ np.ones((2, nPoints))

# 2. Center the data
Xavg = np.mean(X, axis=1)
B = X - np.tile(Xavg, (nPoints, 1)).T  # Mean-subtracted data

# 3. Find principal components via SVD
# We compute SVD on the normalized mean-subtracted data
U, S, VT = np.linalg.svd(B / np.sqrt(nPoints - 1), full_matrices=0)

# The principal components are the columns of U in this formulation
# because B is p x n. If B were n x p, they would be in VT.
# Let's plot the data and the principal components.

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))

# Original data
ax1.plot(X[0, :], X[1, :], 'k.')
ax1.grid()
ax1.set_xlim(-6, 8)
ax1.set_ylim(-6, 8)
ax1.set_title('Original Data Cloud')

# Data with principal components
ax2.plot(X[0, :], X[1, :], 'k.')
ax2.grid()
ax2.set_xlim(-6, 8)
ax2.set_ylim(-6, 8)
ax2.set_title('Data with Principal Components and Variance Ellipses')

# Plot principal components (cyan vectors)
ax2.plot(np.array([Xavg[0], Xavg[0] + U[0, 0] * S[0]]),
         np.array([Xavg[1], Xavg[1] + U[1, 0] * S[0]]), '-', color='cyan', linewidth=5)
ax2.plot(np.array([Xavg[0], Xavg[0] + U[0, 1] * S[1]]),
         np.array([Xavg[1], Xavg[1] + U[1, 1] * S[1]]), '-', color='cyan', linewidth=5)

# Plot 1, 2, and 3-std confidence intervals (ellipses)
theta_ellipse = 2 * np.pi * np.arange(0, 1, 0.01)
for i in [1, 2, 3]:
    Xstd = U @ np.diag(i*S) @ np.array([np.cos(theta_ellipse), np.sin(theta_ellipse)])
    ax2.plot(Xavg[0] + Xstd[0, :], Xavg[1] + Xstd[1, :], '-', color='r', linewidth=2)

plt.show()

\end{lstlisting}
In the resulting plot, the left panel shows the original data cloud. The right panel shows the same data with the two principal components overlaid as thick cyan vectors, originating from the data's mean. The red ellipses represent 1, 2, and 3 standard deviations along these principal axes, visually confirming that the components align with the directions of variance in the data.


\section*{The Method of Least Squares}

\section{Deriving the Normal Equations}
The solution to the least squares problem can be derived from two different perspectives: a geometric one based on orthogonal projections, and an analytical one based on calculus. Both lead to the same result, the Normal Equations.

\subsection{Geometric Derivation}
The method of least squares seeks to find the "best" approximate solution to an overdetermined system of linear equations, $\mat{X}\vec{w} = \vec{y}$. Geometrically, this is equivalent to finding the vector in the column space of $\mat{X}$, denoted $\text{Col}(\mat{X})$, that is closest to the vector $\vec{y}$. The solution is the orthogonal projection of $\vec{y}$ onto $\text{Col}(\mat{X})$.

Let the optimal prediction be $\hat{\vec{p}} = \mat{X}\hat{\vec{w}}$. The residual vector, defined as the difference between the true labels and the prediction, is $\vec{r} = \vec{y} - \hat{\vec{p}} = \vec{y} - \mat{X}\hat{\vec{w}}$. For $\hat{\vec{p}}$ to be the orthogonal projection, the residual vector $\vec{r}$ must be orthogonal to every vector in the subspace $\text{Col}(\mat{X})$.

This orthogonality condition is equivalent to stating that $\vec{r}$ must be orthogonal to every column vector of the matrix $\mat{X}$. Let the columns of $\mat{X}$ be $\vec{x}_1, \dots, \vec{x}_p$. The orthogonality can be expressed as a set of $p$ equations, where the dot product of each column with the residual is zero:
\begin{equation}
    \vec{x}_i^\top (\vec{y} - \mat{X}\hat{\vec{w}}) = 0 \quad \text{for } i = 1, \dots, p
\end{equation}
These $p$ equations can be written compactly in matrix form. The matrix whose rows are the transposes of the columns of $\mat{X}$ is precisely the transpose of $\mat{X}$, i.e., $\mat{X}^\top$.
\begin{equation}
    \mat{X}^\top (\vec{y} - \mat{X}\hat{\vec{w}}) = \vec{0}
\end{equation}
Rearranging the terms, we arrive at the fundamental result known as the **Normal Equations**:
\begin{equation}
    \mat{X}^\top \mat{X}\hat{\vec{w}} = \mat{X}^\top \vec{y}
\end{equation}

\subsection{Derivation via Minimization}
An alternative to the geometric derivation is to formulate the least squares problem as a minimization problem and solve it using calculus. The goal is to find the vector $\hat{\vec{w}}$ that minimizes the cost function $J(\vec{w})$, which is the squared norm of the residual vector.
\begin{equation}
    \hat{\vec{w}} = \argmin_{\vec{w} \in \R^p} J(\vec{w}) = \argmin_{\vec{w} \in \R^p} \norm{\vec{y} - \mat{X}\vec{w}}^2
\end{equation}
We can expand the squared norm, which is the dot product of the residual with itself:
\begin{align*}
    J(\vec{w}) &= (\vec{y} - \mat{X}\vec{w})^\top (\vec{y} - \mat{X}\vec{w}) \\
    &= (\vec{y}^\top - \vec{w}^\top \mat{X}^\top)(\vec{y} - \mat{X}\vec{w}) \\
    &= \vec{y}^\top\vec{y} - \vec{y}^\top \mat{X}\vec{w} - \vec{w}^\top \mat{X}^\top \vec{y} + \vec{w}^\top \mat{X}^\top \mat{X}\vec{w}
\end{align*}
Since the term $\vec{w}^\top \mat{X}^\top \vec{y}$ is a scalar, it is equal to its own transpose: $(\vec{w}^\top \mat{X}^\top \vec{y})^\top = \vec{y}^\top \mat{X} \vec{w}$. Thus, the two middle terms are identical. The cost function simplifies to a quadratic function of $\vec{w}$:
\begin{equation}
    J(\vec{w}) = \vec{y}^\top\vec{y} - 2\vec{y}^\top \mat{X}\vec{w} + \vec{w}^\top \mat{X}^\top \mat{X}\vec{w}
\end{equation}
To find the minimum, we compute its gradient with respect to $\vec{w}$ and set it to zero. The gradient is:
\begin{align*}
    \grad_{\vec{w}} J(\vec{w}) &= \vec{0} - 2(\mat{X}^\top \vec{y}) + 2(\mat{X}^\top \mat{X})\vec{w} \\
    &= -2\mat{X}^\top \vec{y} + 2\mat{X}^\top \mat{X}\vec{w}
\end{align*}
Setting the gradient to zero gives:
\[
    -2\mat{X}^\top \vec{y} + 2\mat{X}^\top \mat{X}\hat{\vec{w}} = \vec{0} \implies \mat{X}^\top \mat{X}\hat{\vec{w}} = \mat{X}^\top \vec{y}
\]
This is exactly the same set of normal equations derived from the geometric approach.

\subsection{The Solution and its Properties}
The least squares solution $\hat{\vec{w}}$ is found by solving the normal equations. If the matrix $\mat{X}^\top \mat{X}$ is invertible, a unique solution exists and is given by:
\begin{equation}
    \hat{\vec{w}} = (\mat{X}^\top \mat{X})^{-1} \mat{X}^\top \vec{y}
\end{equation}
This closed-form solution is central to the method of least squares. However, its existence and uniqueness depend on the invertibility of the matrix $\mat{X}^\top \mat{X}$.

\begin{theorem}
    If a matrix $\mat{X} \in \R^{n \times p}$ has linearly independent columns (i.e., $\text{rank}(\mat{X}) = p$), then the matrix $\mat{X}^\top \mat{X}$ is invertible.
\end{theorem}
\begin{proof}
    To prove that $\mat{X}^\top \mat{X}$ is invertible, we can show that its null space contains only the zero vector. Let $\vec{v} \in \R^p$ be a vector in the null space of $\mat{X}^\top \mat{X}$:
    \begin{equation}
        \mat{X}^\top \mat{X} \vec{v} = \vec{0}
    \end{equation}
    Multiplying from the left by $\vec{v}^\top$:
    \begin{equation}
        \vec{v}^\top \mat{X}^\top \mat{X} \vec{v} = \vec{v}^\top \vec{0} = 0 \implies (\mat{X}\vec{v})^\top (\mat{X}\vec{v}) = 0
    \end{equation}
    This is equivalent to the squared Euclidean norm of the vector $\mat{X}\vec{v}$: $\norm{\mat{X}\vec{v}}^2 = 0$, which implies that the vector $\mat{X}\vec{v}$ must be the zero vector:
    \begin{equation}
        \mat{X}\vec{v} = \vec{0}
    \end{equation}
    This means that $\vec{v}$ is in the null space of $\mat{X}$. By our initial assumption, the matrix $\mat{X}$ has linearly independent columns, which means its null space is trivial, containing only the zero vector. Therefore, we must have $\vec{v} = \vec{0}$.
    Since the only vector in the null space of $\mat{X}^\top \mat{X}$ is the zero vector, the matrix $\mat{X}^\top \mat{X}$ is invertible.
\end{proof}

\section{Solving Least Squares with Singular Value Decomposition (SVD)}
The normal equations can be numerically unstable if the matrix $\mat{X}^T\mat{X}$ is singular or nearly singular (ill-conditioned). A more stable approach is to use the SVD of $\mat{X}$. Let the reduced SVD of $\mat{X}$ be $\mat{X} = \mat{U}_r\mat{\Sigma}_r\mat{V}_r^T$.

\subsection{Derivation via SVD Substitution}
\begin{enumerate}
    \item \textbf{Compute $\mat{X}^T\mat{X}$:}
    \begin{align*}
        \mat{X}^T\mat{X} &= (\mat{U}_r\mat{\Sigma}_r\mat{V}_r^T)^T (\mat{U}_r\mat{\Sigma}_r\mat{V}_r^T) = (\mat{V}_r\mat{\Sigma}_r^T\mat{U}_r^T) (\mat{U}_r\mat{\Sigma}_r\mat{V}_r^T) \\
        &= \mat{V}_r\mat{\Sigma}_r(\mat{U}_r^T\mat{U}_r)\mat{\Sigma}_r\mat{V}_r^T = \mat{V}_r\mat{\Sigma}_r^2\mat{V}_r^T
    \end{align*}
    \item \textbf{Compute the inverse $(\mat{X}^T\mat{X})^{-1}$:}
    \begin{align*}
        (\mat{X}^T\mat{X})^{-1} = (\mat{V}_r\mat{\Sigma}_r^2\mat{V}_r^T)^{-1} = (\mat{V}_r^T)^{-1}(\mat{\Sigma}_r^2)^{-1}(\mat{V}_r)^{-1} = \mat{V}_r\mat{\Sigma}_r^{-2}\mat{V}_r^T
    \end{align*}
    \item \textbf{Combine terms to find $\hat{\vec{w}}$:}
    \begin{align*}
        \hat{\vec{w}} &= (\mat{X}^T\mat{X})^{-1}\mat{X}^T\vec{y} = (\mat{V}_r\mat{\Sigma}_r^{-2}\mat{V}_r^T) (\mat{U}_r\mat{\Sigma}_r\mat{V}_r^T)^T \vec{y} \\
        &= (\mat{V}_r\mat{\Sigma}_r^{-2}\mat{V}_r^T) (\mat{V}_r\mat{\Sigma}_r\mat{U}_r^T) \vec{y} = \mat{V}_r\mat{\Sigma}_r^{-2}(\mat{V}_r^T\mat{V}_r)\mat{\Sigma}_r\mat{U}_r^T \vec{y}
    \end{align*}
    Since $\mat{V}_r^T\mat{V}_r = \mat{I}$, this simplifies to:
    \begin{equation}
        \hat{\vec{w}} = \mat{V}_r\mat{\Sigma}_r^{-1}\mat{U}_r^T\vec{y}
    \end{equation}
\end{enumerate}
The term $\mat{V}_r\mat{\Sigma}_r^{-1}\mat{U}_r^T$ is known as the \textbf{pseudoinverse} of $\mat{X}$, denoted $\mat{X}^\dagger$. Thus, the least squares solution can be elegantly expressed as $\hat{\vec{w}} = \mat{X}^\dagger\vec{y}$.

\section{The Problem of Ill-Conditioning and Small Singular Values}
The SVD formulation reveals a critical vulnerability in the least squares solution. The matrix $\mat{\Sigma}_r^{-1}$ is a diagonal matrix whose entries are the reciprocals of the singular values, $1/\sigma_i$. If a singular value $\sigma_i$ is close to zero, its reciprocal $1/\sigma_i$ becomes extremely large. This has severe consequences:
\begin{itemize}
    \item \textbf{Noise Amplification:} Small changes or noise in the input data vector $\vec{y}$ are amplified by the large entries in $\mat{\Sigma}_r^{-1}$, leading to huge and unstable changes in the solution vector $\hat{\vec{w}}$.
    \item \textbf{Ill-Conditioning:} A problem where small changes in the input lead to large changes in the output is said to be \textbf{ill-conditioned}. The presence of small singular values is a direct indicator of ill-conditioning.
    \item \textbf{Overfitting:} The resulting solution vector $\hat{\vec{w}}$ may have an extremely large norm. In machine learning, this often corresponds to \textbf{overfitting}, where the model fits the noise in the training data rather than the underlying signal.
\end{itemize}

\section{Solving Ill-Conditioning with Regularization}
To combat ill-conditioning and overfitting, we introduce \textbf{regularization}. The core idea is to add a \textbf{penalty term} to the cost function, which penalizes large weight vectors and encourages simpler, more stable solutions.
\begin{equation}
J_{reg}(\vec{w}) = \underbrace{\norm{\vec{y} - \mat{X}\vec{w}}_2^2}_{\text{Data Fidelity Term}} + \underbrace{\lambda\Omega(\vec{w})}_{\text{Penalty Term}}
\end{equation}
Where $\lambda \ge 0$ is the regularization parameter, and $\Omega(\vec{w})$ is the penalty function.

\subsection{Ridge Regression (L2 Regularization)}
A common and powerful form of regularization is \textbf{Ridge Regression}, which uses the squared L2 norm as the penalty function: $\Omega(\vec{w}) = \norm{\vec{w}}_2^2$.
The cost function is:
\begin{equation}
J_{ridge}(\vec{w}) = \norm{\vec{y} - \mat{X}\vec{w}}_2^2 + \lambda\norm{\vec{w}}_2^2
\end{equation}
To find the minimum, we take the gradient of $J_{ridge}(\vec{w})$ with respect to $\vec{w}$ and set it to zero. This yields the \textbf{modified normal equations}:
\begin{equation}
(\mat{X}^T\mat{X} + \lambda\mat{I})\hat{\vec{w}}_{ridge} = \mat{X}^T\vec{y}
\end{equation}
The closed-form solution for Ridge Regression is therefore:
\begin{equation}
\hat{\vec{w}}_{ridge} = (\mat{X}^T\mat{X} + \lambda\mat{I})^{-1}\mat{X}^T\vec{y}
\end{equation}
The term $\lambda\mat{I}$ adds a "ridge" to the diagonal of $\mat{X}^T\mat{X}$. This ensures that $(\mat{X}^T\mat{X} + \lambda\mat{I})$ is always invertible and well-conditioned, even if $\mat{X}^T\mat{X}$ had very small or zero eigenvalues. Specifically, adding $\lambda\mat{I}$ increases each eigenvalue of $\mat{X}^T\mat{X}$ by $\lambda$, lifting them away from zero and stabilizing the matrix inversion.

\newpage
\chapter{13-10}

\section{Introduction: Beyond Linearity}

Standard machine learning models, such as basic Linear Regression and Least Squares, are fundamentally constrained by the assumption of linearity. While powerful, this assumption often fails to capture the complexity of real-world data.

\subsection{The Limits of Linear Models}
Standard Least Squares finds the best linear fit to the data. However, if the underlying relationship between features and the target is non-linear, a linear model will provide a poor approximation.

Consider a simple dataset where the data points follow a curved pattern. A linear fit would be a straight line that fails to capture this curvature, leading to high prediction errors. The core challenge is to extend linear models to handle these non-linear relationships without abandoning the well-understood and computationally efficient framework of linear algebra.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.6\textwidth]{image_placeholder_1.png}
    \caption{A linear fit (red dashed line) is a poor approximation for data with an underlying non-linear relationship (blue dots). The ideal model might be a non-linear curve (green solid line).}
\end{figure}

The problem can be stated as: a straight line is a poor approximation for this data. We need a way to model non-linear relationships effectively.

\section{The Solution: Feature Maps}
The core idea behind kernel methods is to project the data into a higher-dimensional \textbf{feature space} where the relationship becomes linear. This allows us to apply linear models in the new space, which corresponds to a non-linear model in the original space.

\subsection{Definition of a Feature Map (\(\phi\))}
A feature map is a function \(\phi\) that maps each data point \(\vec{x} \in \R^p\) to a higher-dimensional space \(\R^D\), where \(D > p\).
\begin{equation}
\phi : \R^p \to \R^D
\end{equation}

\subsection{Example: 1D Input to 2D Feature Space (Polynomial Kernel)}
For a simple 1-dimensional data point \(x\), we can create a 2D feature space using the map:
\begin{equation}
\phi(x) = \begin{pmatrix} x \\ x^2 \end{pmatrix}
\end{equation}
In this new 2D feature space, a linear model of the form \(w_1(x) + w_2(x^2)\) corresponds to a quadratic model in the original 1D space. By learning the weights \(w_1\) and \(w_2\), we can fit a parabola to the data using a linear algorithm in the transformed space.

\subsection{Example: 2D Input to Higher-Dimensional Space}
Consider a 2D input vector \(\vec{x} = (x_1, x_2)^T \in \R^2\). We may encounter data that is not linearly separable in 2D, such as points arranged in concentric circles. For instance, a decision boundary might be a circle.

A common feature map for polynomial kernels of degree 2 transforms \(\vec{x}\) into a 6-dimensional space:
\begin{equation}
\phi(x_1, x_2) = 
\begin{pmatrix}
1 \\
x_1 \\
x_2 \\
x_1^2 \\
x_2^2 \\
\sqrt{2}x_1 x_2
\end{pmatrix} \in \R^6
\end{equation}
Now, a circular decision boundary like \(x_1^2 + x_2^2 = R^2\) becomes linear in the feature space. If we consider only the features \(x_1^2\) and \(x_2^2\), the equation becomes:
\begin{equation}
1 \cdot (x_1^2) + 1 \cdot (x_2^2) = R^2
\end{equation}
This is a linear equation (a hyperplane) in the feature space defined by the axes \(x_1^2\) and \(x_2^2\).

\subsection{Geometric View}
The power of feature maps can be visualized geometrically. A problem that is complex in a low-dimensional space (e.g., requiring a circular boundary) can become simple in a higher-dimensional space, where a separating hyperplane is sufficient. The feature map provides the transformation from the complex, low-dimensional representation to the simple, high-dimensional one.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.8\textwidth]{image_placeholder_2.png}
    \caption{A non-linearly separable problem in low dimensions becomes linearly separable after being mapped to a higher-dimensional feature space.}
\end{figure}

\section{Ridge Regression in Feature Space}
We can now reformulate Ridge Regression to operate in this new, high-dimensional feature space.

\begin{enumerate}
    \item Let \(\mat{\Phi}(\mat{X})\) be the \(n \times D\) matrix where each row \(i\) is the transformed feature vector \(\phi(\vec{x}_i)^T\).
    \item The Ridge Regression problem becomes:
    \begin{equation}
    \underset{\vec{w} \in \R^D}{\text{minimize}} \quad \norm{\mat{\Phi}(\mat{X})\vec{w} - \vec{y}}_2^2 + \lambda \norm{\vec{w}}_2^2
    \end{equation}
\end{enumerate}

\subsection{The Computational Problem}
This approach introduces a significant computational challenge. The dimension \(D\) of the feature space can be very large, or even infinite.
\begin{itemize}
    \item Explicitly computing and storing the matrix \(\mat{\Phi}(\mat{X})\) can be prohibitively expensive or impossible.
    \item The solution to the Ridge Regression problem involves inverting a \(D \times D\) matrix, which is computationally infeasible for large \(D\).
\end{itemize}

\section{The Kernel Trick: A Clever Shortcut}
It turns out that for many algorithms, including Ridge Regression, we don't need to explicitly compute the feature vectors \(\phi(\vec{x})\). We only need access to their inner products. This observation is the foundation of the "kernel trick."

\subsection{Definition (Kernel Function)}
A kernel function \(K\) computes the inner product of two vectors in the feature space, without ever performing the explicit mapping into that space:
\begin{equation}
K(\vec{x}_i, \vec{x}_j) = \langle \phi(\vec{x}_i), \phi(\vec{x}_j) \rangle = \phi(\vec{x}_i)^T \phi(\vec{x}_j)
\end{equation}

\subsection{The "Trick"}
If we can rewrite our algorithm to only use these inner products, we can avoid the computational cost of high-dimensional feature spaces. This allows us to work implicitly in a high-dimensional (or even infinite-dimensional) space while only performing computations in the original low-dimensional space.

\section{Examples of Kernel Functions}
\subsection{Polynomial Kernel (degree \(q\))}
This kernel corresponds to a feature space of all polynomial terms up to degree \(q\).
\begin{equation}
K(\vec{x}, \vec{z}) = (\vec{x}^T \vec{z} + c)^q
\end{equation}
For \(c = 0\), it's the homogeneous polynomial kernel. For \(c > 0\), it includes lower-order terms, making it more flexible.

\subsection{Gaussian Kernel (Radial Basis Function - RBF)}
This kernel corresponds to an \textbf{infinite-dimensional} feature space.
\begin{equation}
K(\vec{x}, \vec{z}) = \exp\left(-\frac{\norm{\vec{x} - \vec{z}}_2^2}{2\sigma^2}\right) = \exp(-\gamma\norm{\vec{x} - \vec{z}}_2^2)
\end{equation}
where \(\gamma = 1/(2\sigma^2)\). This is one of the most powerful and widely used kernels due to its flexibility and ability to model complex relationships.

\section{The Representer Theorem}
How does the kernel trick help us solve for the weight vector \(\vec{w}\)? The Representer Theorem provides the key insight.

\begin{theorem}[Representer Theorem, simplified]
The solution \(\vec{w}^*\) to the Kernel Ridge Regression problem can always be written as a linear combination of the feature vectors of the training data:
\begin{equation}
\vec{w}^* = \sum_{i=1}^{n} \alpha_i \phi(\vec{x}_i) = \mat{\Phi}(\mat{X})^T \vec{\alpha}
\end{equation}
for some coefficient vector \(\vec{\alpha} \in \R^n\).
\end{theorem}

\subsection{Implication}
Instead of searching for the potentially infinite-dimensional vector \(\vec{w} \in \R^D\), we only need to find the \(n\)-dimensional vector of coefficients \(\vec{\alpha}\), where \(n\) is the number of training samples. This reduces the dimensionality of the optimization problem from \(D\) to \(n\).

\subsection{Proof of the Representer Theorem}
Let the feature space be denoted by \(\mathcal{H}\). Any vector \(\vec{w} \in \mathcal{H}\) can be decomposed into two orthogonal components:
\begin{equation}
\vec{w} = \vec{w}_{\parallel} + \vec{w}_{\perp}
\end{equation}
where \(\vec{w}_{\parallel}\) is in the span of the training data \(\{\phi(\vec{x}_i)\}_{i=1}^n\), and \(\vec{w}_{\perp}\) is in its orthogonal complement.

By definition of the orthogonal complement, \(\langle \vec{w}_{\perp}, \phi(\vec{x}_i) \rangle = 0\) for all \(i\). This means the prediction on any training point is unaffected by \(\vec{w}_{\perp}\):
\begin{align*}
\langle \vec{w}, \phi(\vec{x}_i) \rangle &= \langle (\vec{w}_{\parallel} + \vec{w}_{\perp}), \phi(\vec{x}_i) \rangle \\
&= \langle \vec{w}_{\parallel}, \phi(\vec{x}_i) \rangle + \langle \vec{w}_{\perp}, \phi(\vec{x}_i) \rangle \\
&= \langle \vec{w}_{\parallel}, \phi(\vec{x}_i) \rangle
\end{align*}
So the error term \(\norm{\mat{\Phi}(\mat{X})\vec{w} - \vec{y}}_2^2\) only depends on \(\vec{w}_{\parallel}\). By the Pythagorean theorem, the regularization term becomes:
\begin{equation}
\norm{\vec{w}}_2^2 = \norm{\vec{w}_{\parallel}}_2^2 + \norm{\vec{w}_{\perp}}_2^2
\end{equation}
The full cost function is:
\begin{equation}
J(\vec{w}) = \norm{\mat{\Phi}(\mat{X})\vec{w}_{\parallel} - \vec{y}}_2^2 + \lambda (\norm{\vec{w}_{\parallel}}_2^2 + \norm{\vec{w}_{\perp}}_2^2)
\end{equation}
To minimize this function, we must choose the component that makes the function value as small as possible. The term \(\lambda \norm{\vec{w}_{\perp}}_2^2\) is non-negative and does not affect the error term. Any non-zero \(\vec{w}_{\perp}\) will only increase the cost. Therefore, to minimize \(J(\vec{w})\), we must choose \(\vec{w}_{\perp} = \vec{0}\).

This proves that the optimal solution \(\vec{w}^*\) must lie entirely in the span of the training feature vectors.

\subsection{Significance and Generalization}
The Representer Theorem is not limited to Kernel Ridge Regression. It applies to a wide class of regularization problems that involve minimizing a loss function plus a regularizer. Essentially, it states that:
\begin{quote}
"The solution to a wide class of regularization problems that involve minimizing a loss function plus a regularizer can be written as a linear combination of kernel functions evaluated at the training data points."
\end{quote}
The key significance includes:
\begin{itemize}
    \item It theoretically justifies why we can express the solution purely in terms of kernel evaluations, even when working in infinite-dimensional feature spaces.
    \item It guarantees that the kernel-based approach finds the correct solution without needing explicit feature mapping, thus avoiding prohibitive computational costs.
\end{itemize}

\section{The Dual Problem and the Kernel Matrix}
By substituting the form of \(\vec{w}\) from the Representer Theorem into the original optimization problem, we derive a new problem in terms of \(\vec{\alpha}\). This is known as the dual problem. The solution is then found by solving a linear system.

\subsection{Solving for the Coefficients \(\vec{\alpha}\)}
The solution for \(\vec{\alpha}\) is found by solving the linear system:
\begin{equation}
(\mat{K} + \lambda \mat{I}_n)\vec{\alpha} = \vec{y}
\end{equation}
where \(\mat{K}\) is the Kernel Matrix and \(\mat{I}_n\) is the \(n \times n\) identity matrix.

\subsection{The Kernel (Gram) Matrix \(\mat{K}\)}
The matrix \(\mat{K}\) is an \(n \times n\) symmetric matrix, also known as the Gram matrix. Each entry \(K_{ij}\) is the kernel evaluation between two data points \(\vec{x}_i\) and \(\vec{x}_j\):
\begin{equation}
K_{ij} = K(\vec{x}_i, \vec{x}_j) = \langle \phi(\vec{x}_i), \phi(\vec{x}_j) \rangle = \phi(\vec{x}_i)^T \phi(\vec{x}_j)
\end{equation}
Note that the entire matrix can be expressed as \(\mat{K} = \mat{\Phi}(\mat{X})\mat{\Phi}(\mat{X})^T\). The solution for \(\vec{\alpha}\) can then be written in closed form:
\begin{equation}
\vec{\alpha} = (\mat{K} + \lambda \mat{I}_n)^{-1}\vec{y}
\end{equation}

\subsection{Making Predictions}
Once the coefficient vector \(\vec{\alpha}\) has been found, we can predict the value for a new data point \(\vec{x}_*\). The prediction \(\hat{y}_*\) is given by:
\begin{align*}
\hat{y}_* = f(\vec{x}_*) &= \langle \vec{w}^*, \phi(\vec{x}_*) \rangle \\
&= \left\langle \sum_{i=1}^{n} \alpha_i \phi(\vec{x}_i), \phi(\vec{x}_*) \right\rangle \\
&= \sum_{i=1}^{n} \alpha_i \langle \phi(\vec{x}_i), \phi(\vec{x}_*) \rangle \\
&= \sum_{i=1}^{n} \alpha_i K(\vec{x}_i, \vec{x}_*)
\end{align*}

\subsection{Conclusion}
The entire processfrom solving for the coefficients \(\vec{\alpha}\) to making new predictionscan be performed using only kernel function evaluations. We never need to compute the potentially very high-dimensional feature vectors \(\phi(\vec{x})\) explicitly. This is the power of the kernel trick, made possible by the Representer Theorem.

\section{The PageRank Algorithm}

\subsection{The State Vector and the Steady State}
The PageRank algorithm models a "random surfer" navigating the web. The state of this system can be described by a probability distribution vector, which evolves over discrete time steps.

\subsubsection{The State Vector \(\vec{\pi}\)}
Let \(\vec{\pi}_k \in \R^N\) be a vector representing the probability distribution of our random surfer at step $k$, where $N$ is the total number of pages. The $i$-th component of this vector, \((\vec{\pi}_k)_i\), is the probability that the surfer is on page $i$ at step $k$.

The state evolves according to a linear rule, governed by a transition matrix \(\mat{M}\):
\begin{equation}
\vec{\pi}_{k+1} = \mat{M} \vec{\pi}_k
\end{equation}
This iterative process involves repeatedly applying the matrix \(\mat{M}\) to the state vector.

\subsubsection{The Steady State}
As the number of steps $k$ approaches infinity ($k \to \infty$), this process is expected to converge to a stationary probability distribution, denoted by \(\vec{\pi}\). At this steady state, the probabilities no longer change with each step. This implies that the steady-state vector must satisfy the following equation:
\begin{equation}
\vec{\pi} = \mat{M} \vec{\pi}
\end{equation}
This stationary vector \(\vec{\pi}\) is precisely the \textbf{PageRank vector}. Its components give the long-term probability of finding the surfer on any given page, which is interpreted as the importance or "rank" of that page.

\subsection{Connection to Eigenvectors}
The steady-state equation is fundamentally an eigenvector problem. The equation \(\vec{\pi} = \mat{M}\vec{\pi}\) can be rewritten as:
\begin{equation}
\mat{M}\vec{\pi} = 1 \cdot \vec{\pi}
\end{equation}
This shows that the PageRank vector \(\vec{\pi}\) is the eigenvector of the transition matrix \(\mat{M}\) corresponding to an eigenvalue \(\lambda = 1\).

\begin{definition}[The PageRank Vector]
The PageRank vector \(\vec{\pi}\) is the eigenvector of the transition matrix \(\mat{M}\) corresponding to the eigenvalue \(\lambda = 1\).
\end{definition}

This formulation raises several critical questions about the underlying mathematical structure of the problem:
\begin{enumerate}
    \item Is \(\lambda = 1\) always an eigenvalue of \(\mat{M}\)?
    \item Is this eigenvalue the largest one (i.e., the dominant eigenvalue)?
    \item Is the corresponding eigenvector unique?
\end{enumerate}

\subsection{Guarantees: The Perron-Frobenius Theorem}
The answers to these questions are provided by the Perron-Frobenius theorem, a cornerstone result in the theory of non-negative matrices. For our purposes, we consider a simplified version applicable to stochastic matrices.

\begin{theorem}[Perron-Frobenius, simplified]
If \(\mat{M}\) is a column stochastic, positive, and irreducible matrix, then:
\begin{enumerate}
    \item The eigenvalue \(\lambda = 1\) is the largest eigenvalue (the "principal eigenvalue").
    \item The eigenvector corresponding to \(\lambda = 1\) is unique (up to a scalar multiple).
    \item This eigenvector can be chosen to have strictly positive components.
\end{enumerate}
\end{theorem}

The properties of being "positive" and "irreducible" are crucial for these guarantees to hold. In practice, the standard transition matrix derived from the web graph may not satisfy these conditions (e.g., dangling nodes or disconnected components). To address this, the "Google Matrix" is constructed as a slightly modified version of \(\mat{M}\) specifically to ensure these properties are met, thus guaranteeing that a unique and positive PageRank vector exists.

\subsection{Numerical Computation: The Power Method}
Since we know that the PageRank vector is the eigenvector associated with the largest eigenvalue (\(\lambda = 1\)), we can compute it efficiently using the \textbf{Power Method}. This iterative algorithm is simple, scalable, and guaranteed to converge to the PageRank vector under the conditions established by the Perron-Frobenius theorem.

\subsubsection{The Algorithm}
\begin{enumerate}
    \item \textbf{Initialization:} Start with an initial guess for the probability vector, \(\vec{\pi}_0\). A uniform distribution is a common and effective choice:
    \begin{equation}
    \vec{\pi}_0 = [1/N, 1/N, \dots, 1/N]^T
    \end{equation}
    where \(N\) is the number of pages.
    
    \item \textbf{Iteration:} Iterate until convergence using the state evolution rule:
    \begin{equation}
    \vec{\pi}_{k+1} = \mat{M} \vec{\pi}_k
    \end{equation}
    
    \item \textbf{Stopping Criterion:} Stop when the change between successive vectors, \(\vec{\pi}_{k+1}\) and \(\vec{\pi}_k\), is very small (e.g., when the norm of their difference is below a predefined tolerance).
\end{enumerate}

\subsubsection{Why the Power Method Converges}
The convergence of the Power Method is guaranteed because \(\lambda_1 = 1\) is the unique, largest eigenvalue. Let the eigenvectors of \(\mat{M}\) be \(\vec{v}_1, \vec{v}_2, \dots, \vec{v}_N\) with corresponding eigenvalues \(\lambda_1, \lambda_2, \dots, \lambda_N\), ordered such that \(1 = |\lambda_1| > |\lambda_2| \ge \dots \ge |\lambda_N|\).

\begin{enumerate}
    \item \textbf{Decomposition:} We can write our initial guess \(\vec{\pi}_0\) as a linear combination of the eigenvectors:
    \begin{equation}
    \vec{\pi}_0 = c_1 \vec{v}_1 + c_2 \vec{v}_2 + \dots + c_N \vec{v}_N
    \end{equation}
    
    \item \textbf{Iteration:} After one step, we have:
    \begin{equation}
    \vec{\pi}_1 = \mat{M}\vec{\pi}_0 = c_1 \lambda_1 \vec{v}_1 + c_2 \lambda_2 \vec{v}_2 + \dots
    \end{equation}
    After \(k\) steps, the state vector is given by:
    \begin{equation}
    \vec{\pi}_k = \mat{M}^k \vec{\pi}_0 = c_1 \lambda_1^k \vec{v}_1 + c_2 \lambda_2^k \vec{v}_2 + \dots
    \end{equation}
    
    \item \textbf{Convergence:} Since \(\lambda_1 = 1\) and \(|\lambda_i| < 1\) for all \(i > 1\), as \(k \to \infty\), the terms \(\lambda_i^k\) for \(i > 1\) go to zero much faster than \(\lambda_1^k = 1^k = 1\). Consequently, the vector \(\vec{\pi}_k\) becomes dominated by the first term:
    \begin{equation}
    \vec{\pi}_k \approx c_1 \lambda_1^k \vec{v}_1 = c_1(1)^k \vec{v}_1 = c_1 \vec{v}_1
    \end{equation}
\end{enumerate}
The vector thus converges to a scalar multiple of the dominant eigenvector \(\vec{v}_1\), which is our desired PageRank vector \(\vec{\pi}\).

\chapter{Lesson 20-10}

\section{The Matrix Completion Problem}
The Matrix Completion problem is a fundamental task in machine learning and data science. It addresses the challenge of recovering a complete data matrix from a small subset of its observed entries. This problem is prevalent in many real-world applications, most famously in recommender systems.

\subsection{Motivation: The Netflix Challenge}
A canonical example of matrix completion is the problem of predicting user movie ratings, popularized by the Netflix Prize challenge. In this scenario, we have a large matrix where rows correspond to users and columns correspond to movies. Each entry $(i, j)$ in the matrix represents the rating given by user $i$ to movie $j$.

The key characteristics of this problem are:
\begin{itemize}
    \item \textbf{High Dimensionality:} The number of users ($n$) and movies ($d$) can be in the millions and thousands, respectively.
    \item \textbf{Sparsity of Observations:} Most users have rated only a very small fraction of the available content. This results in a data matrix with a vast majority of its entries missing.
\end{itemize}

The primary goal is to predict these missing ratings. Accurately filling in the matrix allows for the creation of better recommendation systems, which in turn leads to increased user engagement. The significance of this problem was highlighted by the \$1 million prize offered by Netflix in 2009 for an algorithm that could significantly improve their existing system.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.8\textwidth]{placeholder_netflix.png}
    \caption{A conceptual representation of the user-movie rating matrix. Most entries are unknown (denoted by '?').}
\end{figure}

\section{The Low-Rank Assumption}

A direct approach to filling the matrix is impossible, as there are infinitely many ways to complete the missing entries. The key insight that makes this problem tractable is the assumption that the true, complete data matrix has a \textbf{low-rank structure}.

\subsection{Latent Factor Models}
The intuition behind the low-rank assumption is that user preferences are not random but are driven by a small number of underlying or "latent" factors. For example, a user's rating for a movie might be explained by a combination of factors such as:
\begin{itemize}
    \item \textbf{Genre preferences:} How much the user enjoys action, comedy, or drama.
    \item \textbf{Era preferences:} Whether the user prefers classic or modern films.
    \item \textbf{Audience type:} The suitability of the film for family, adults, or teens.
\end{itemize}

Mathematically, this means the large $n \times d$ rating matrix $X$ can be approximated by the product of two much thinner matrices:
\begin{equation}
    X \approx U V^T
\end{equation}
where:
\begin{itemize}
    \item $X \in \R^{n \times d}$ is the complete rating matrix.
    \item $U \in \R^{n \times r}$ is a "user features" matrix, where each row represents a user's affinity for the $r$ latent factors.
    \item $V \in \R^{d \times r}$ is a "movie features" matrix, where each row represents how much a movie embodies each of the $r$ latent factors.
\end{itemize}
The crucial assumption here is that the number of latent factors, $r$, which is the rank of the matrix, is much smaller than both the number of users and movies, i.e., $r \ll \min(n, d)$.

\subsection{Degrees of Freedom}
The low-rank assumption drastically reduces the complexity of the problem. A full-rank $n \times n$ matrix has $n^2$ independent entries, or degrees of freedom. However, a matrix of rank $r$ has significantly fewer degrees of freedom. This can be understood by considering its Singular Value Decomposition (SVD). The number of parameters needed to define a rank-$r$ matrix is approximately $(2n - r)r$ for an $n \times n$ matrix, which is considerably smaller than $n^2$ when $r \ll n$. This reduction in intrinsic dimensionality is what allows us to recover the matrix from a limited number of observations.

\section{Problem Formulation and Challenges}

\subsection{Formal Setup}
Let us formalize the matrix completion problem:
\begin{itemize}
    \item \textbf{Given:} A set of observations $\{X_{ij}\}$ for a subset of indices $(i, j) \in \Omega$, where $\Omega \subset \{1, \dots, n\} \times \{1, \dots, d\}$. Let $m = |\Omega|$ be the number of observed entries.
    \item \textbf{Goal:} Recover the complete matrix $X \in \R^{n \times d}$.
    \item \textbf{Assumption:} The true underlying matrix $X$ has a low rank, i.e., $\text{rank}(X) = r$ with $r \ll \min(n, d)$.
\end{itemize}

We can define a projection operator $P_{\Omega}$ that filters a matrix to keep only the observed entries:
\begin{equation}
    [P_{\Omega}(M)]_{ij} = 
    \begin{cases} 
        M_{ij} & \text{if } (i,j) \in \Omega \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

\subsection{The Rank Minimization Problem}
A natural approach is to find the matrix with the lowest possible rank that is consistent with the observed data. This can be formulated as the following optimization problem:
\begin{equation}
    \begin{aligned}
        & \underset{M \in \R^{n \times d}}{\text{minimize}}
        & & \text{rank}(M) \\
        & \text{subject to}
        & & P_{\Omega}(M) = P_{\Omega}(X)
    \end{aligned}
\end{equation}
This is equivalent to finding a matrix $M$ that matches the observed entries $X_{ij}$ for all $(i,j) \in \Omega$ while having the minimum rank.

Unfortunately, this problem is **NP-hard**. The rank function is non-convex and combinatorial in nature, making direct optimization computationally intractable for matrices of any significant size.

\section{Convex Relaxation via the Nuclear Norm}
Since direct rank minimization is infeasible, we turn to a common strategy in optimization: convex relaxation. The goal is to replace the non-convex objective function (rank) with a convex surrogate that promotes the desired property (low rank).

\subsection{Analogy with Sparse Vector Recovery}
An insightful analogy can be drawn with the problem of finding a sparse vector.
\begin{itemize}
    \item \textbf{Goal:} Find a sparse vector (many zero entries).
    \item \textbf{Non-convex formulation:} Minimize the $\ell_0$-norm, $\|x\|_0$, which counts the number of non-zero elements. This is NP-hard.
    \item \textbf{Convex relaxation:} Minimize the $\ell_1$-norm, $\|x\|_1 = \sum_i |x_i|$. This is a convex problem, and it is well-known that minimizing the $\ell_1$-norm promotes sparsity in the solution vector $x$.
\end{itemize}

\subsection{The Nuclear Norm}
We can apply the same logic to the matrix completion problem. The rank of a matrix is the number of its non-zero singular values, which is analogous to the $\ell_0$-norm of the vector of singular values. The convex relaxation is thus the $\ell_1$-norm of this vector. This leads to the definition of the **nuclear norm**.

\textbf{Definition (Nuclear Norm):} The nuclear norm (or trace norm) of a matrix $M$, denoted $\|M\|_*$, is the sum of its singular values:
\begin{equation}
    \|M\|_* = \sum_{i=1}^{\min(n,d)} \sigma_i(M)
\end{equation}
where $\sigma_1(M) \ge \sigma_2(M) \ge \dots \ge 0$ are the singular values of $M$.

\textbf{Properties:}
\begin{itemize}
    \item The nuclear norm is a \textbf{convex function}.
    \item It is the tightest convex relaxation of the rank function.
    \item Just as the $\ell_1$-norm promotes sparsity in vector components, the nuclear norm promotes sparsity in the singular values. A matrix with sparse singular values is, by definition, a low-rank matrix.
\end{itemize}

\subsection{The Convex Formulation for Matrix Completion}
By replacing the rank function with its convex surrogate, the nuclear norm, we arrive at a tractable convex optimization problem:
\begin{equation}
    \begin{aligned}
        & \underset{M \in \R^{n \times d}}{\text{minimize}}
        & & \|M\|_* \\
        & \text{subject to}
        & & P_{\Omega}(M) = P_{\Omega}(X)
    \end{aligned}
\end{equation}

This nuclear norm minimization problem offers significant advantages:
\begin{itemize}
    \item It is a \textbf{convex optimization problem} and can be solved efficiently (e.g., via semidefinite programming).
    \item It comes with \textbf{strong theoretical guarantees}. Under certain mild conditions on the matrix and the sampling of entries, the solution to this convex problem is guaranteed to be the exact solution to the original NP-hard rank minimization problem.
\end{itemize}

This transformation from a non-convex, intractable problem to a convex, solvable one is a powerful demonstration of the role of convex relaxation in modern machine learning.

\section{Gradient-Based Optimization}

Numerical optimization is the backbone of model training. The most fundamental algorithm is Gradient Descent, which iteratively updates parameters to minimize a cost function $J(\theta)$.

\subsection{The Update Rule}
The update is defined by moving in the direction of the negative gradient, scaled by a learning rate $\eta$:
\begin{equation}
\theta_{t+1} = \theta_t - \eta \grad J(\theta_t)
\end{equation}

The choice of $\eta$ is critical for numerical stability. If $\eta$ is too large, the algorithm may overshoot the minimum and diverge; if it is too small, convergence becomes computationally prohibitive.

\subsection{Python Implementation}
In practice, the update is implemented using vectorized operations to ensure computational efficiency:

\begin{lstlisting}[language=Python]
def gradient_descent_step(w, gradient, learning_rate):
    """
    Performs a single update step.
    """
    next_w = w - learning_rate * gradient
    return next_w
\end{lstlisting}

\chapter{Lesson 21-10}

\section{The Need for Derivatives in Machine Learning}

At the core of training most machine learning models is the process of \textbf{optimization}. We seek to find a set of model parameters, denoted by the vector $\boldsymbol{\theta}$, that minimizes a given loss function $L(\boldsymbol{\theta})$. While some simple problems, such as ordinary least squares, have analytical closed-form solutions, most practical and complex models, particularly neural networks, do not. For these models, we must resort to numerical minimization algorithms.

\subsection{Gradient-Based Optimization}
A very popular and effective family of optimization algorithms is \textbf{gradient-based optimization}. The most famous and fundamental example is Gradient Descent. The core idea is to iteratively update the parameters in the direction that most steeply reduces the loss.

The update rule for Gradient Descent is given by:
\begin{equation}
\boldsymbol{\theta}_{\text{new}} = \boldsymbol{\theta}_{\text{old}} - \eta \nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})
\end{equation}
where:
\begin{itemize}
    \item $\nabla_{\boldsymbol{\theta}} L(\boldsymbol{\theta})$ is the \textbf{gradient} of the loss function. It is a vector of partial derivatives with respect to each parameter in $\boldsymbol{\theta}$. The gradient points in the direction of the steepest ascent of the loss function.
    \item To minimize the loss, we move in the \textit{opposite} direction of the gradient.
    \item $\eta$ is the \textbf{learning rate}, a scalar hyperparameter that controls the step size of each update.
\end{itemize}

\textbf{Key Takeaway:} Computing derivatives (gradients) accurately and efficiently is absolutely critical for modern machine learning.

\section{Four Ways to Compute Derivatives}
There are four main approaches to obtain the derivatives required for optimization:

\begin{enumerate}
    \item \textbf{Manual Differentiation:} Using pen and paper to derive the gradient formula by hand.
    \item \textbf{Numerical Differentiation:} Approximating the derivative using finite differences, inspired by the limit definition of a derivative.
    \item \textbf{Symbolic Differentiation:} Manipulating mathematical expressions algorithmically using the rules of calculus to derive an exact formula.
    \item \textbf{Automatic Differentiation (AD):} Breaking down a complex computation into a sequence of elementary operations and applying the chain rule systematically.
\end{enumerate}

\subsection{Comparison of Methods}
Each of these methods comes with its own set of advantages and drawbacks, summarized in the table below.

\begin{table}[h!]
\centering
\begin{tabular}{|p{2cm}|p{5.5cm}|p{5.5cm}|}
\hline
\textbf{Method} & \textbf{Advantages} & \textbf{Drawbacks} \\
\hline
\textbf{Manual} & - Can be highly efficient if simplified well. \newline - Good for understanding. & - Prone to human error. \newline - Time-consuming and tedious. \newline - Must be re-derived if the model changes. \\
\hline
\textbf{Numerical} & - Easy to implement. \newline - Works on any function (black box). & - It's an approximation, not exact. \newline - Suffers from floating-point precision issues. \newline - Computationally very expensive. \\
\hline
\textbf{Symbolic} & - Provides an exact analytical expression. \newline - High precision. & - Can lead to "expression swell" (very large formulas). \newline - Can be computationally inefficient to evaluate. \newline - Requires the whole expression upfront. \\
\hline
\textbf{Automatic} & - Exact derivatives (up to machine precision). \newline - Computationally efficient (often comparable to the original function evaluation). \newline - Implemented in all modern ML frameworks. & - Can have a higher memory footprint. \newline - Implementation can be complex (but we use libraries!). \\
\hline
\end{tabular}
\caption{A comparison of the four main differentiation methods.}
\end{table}

\section{The Problems with Numerical Differentiation}
Although easy to implement, numerical differentiation is rarely used for training large models due to two critical issues: high computational cost and numerical instability.

\subsection{High Computational Cost}
Numerical differentiation approximates a derivative using a finite difference formula, such as the forward difference formula:
\begin{equation}
f'(x) \approx \frac{f(x+h) - f(x)}{h} \quad \text{for a small } h > 0
\end{equation}
To compute the full gradient $\nabla f$ of a function $f: \mathbb{R}^n \to \mathbb{R}$, we must compute a partial derivative for each of the $n$ input variables. Using the formula above, each partial derivative requires two function evaluations (one at the original point and one at the perturbed point). Since the evaluation at the original point can be reused, computing the full gradient requires $n+1$ function evaluations in total. For a deep learning model with millions of parameters ($n$), this is completely infeasible.

Similarly, for computing the Jacobian of a function $f: \mathbb{R}^n \to \mathbb{R}^m$, this approach would require computing $n$ partial derivatives for each of the $m$ outputs, leading to a total cost of $n \times m$ function evaluations, which is prohibitively expensive.

\subsection{Round-off and Truncation Errors}
The accuracy of numerical differentiation is limited by a trade-off between two types of errors:
\begin{itemize}
    \item \textbf{Truncation Error ($\Delta_m$):} This is the mathematical error from approximating the derivative with a finite difference formula. It is proportional to some power of the step size $h$ (e.g., $\mathcal{O}(h)$ or $\mathcal{O}(h^2)$). This error decreases as $h$ becomes smaller.
    \item \textbf{Round-off Error ($\Delta_r$):} This error arises from the limited precision of floating-point arithmetic. When subtracting two very close numbers, such as $f(x+h)$ and $f(x)$ for a tiny $h$, we lose significant precision. This error is inversely proportional to $h$ and therefore grows as $h$ becomes smaller.
\end{itemize}

The total error, $\Delta = \Delta_m + \Delta_r$, exhibits a characteristic U-shape. As shown schematically below for a method with $\mathcal{O}(h^2)$ truncation error, there is an optimal value of $h$ that minimizes the total error. Making $h$ too small pollutes the result with round-off error.

\begin{center}
\includegraphics[draft,width=0.7\textwidth]{image.png}
\end{center}

\subsection{Practical Example in Python}
We can demonstrate these error characteristics by approximating the derivative of $f(x) = \sin(x)$. The exact derivative is $f'(x) = \cos(x)$. The following Python code implements two finite difference methods: a first-order forward difference (D1) and a second-order centered difference (D2).

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt

# Taylor expansions for the error terms
# D1(h) = f'(x) + 1/2 * f''(x)h + 1/6 * f'''(x)h^2 + ...
# D2(h) = f'(x) + 1/6 * f'''(x)h^2 + O(h^4)

def D1(x, h, f): # Forward difference
    return (f(x+h)-f(x))/h

def D2(x, h, f): # Centered difference
    return (f(x+h)-f(x-h))/(2*h)

def f(x): # Example function
    return np.sin(x)

# Setup for the plot
h = np.logspace(-16, 0, num=500, endpoint=True)
x = 1.0

# Compute derivatives and errors
D1f = D1(x, h, f)
D2f = D2(x, h, f)
df = np.cos(x) # Exact derivative
Error1 = np.abs(D1f-df)
Error2 = np.abs(D2f-df)
\end{lstlisting}

Plotting the absolute error $|D_{nf} - f'|$ versus the step size $h$ on a log-log scale reveals the behavior predicted by theory.

\begin{center}
\includegraphics[draft,width=0.7\textwidth]{image.png}
\end{center}

As observed in the plot, both methods achieve a minimum error at an optimal $h$ before round-off errors begin to dominate and degrade the approximation for very small $h$. The second-order method ($\mathcal{O}(h^2)$) achieves a lower minimum error, but both are fundamentally limited by floating-point precision. This instability and high computational cost make numerical differentiation unsuitable for the high-dimensional, high-precision needs of training modern machine learning models.


\section{Automatic Differentiation (AD): An Exact Approach}
Automatic Differentiation (AD) is a family of techniques that provides an elegant solution to the problems of numerical and symbolic differentiation. It is crucial to understand what AD is by first understanding what it is not.

\subsection{Distinguishing AD from Other Methods}

\subsubsection{AD is Not Numerical Differentiation}
AD does not approximate the derivative using finite differences. Instead, it computes the derivative value \textbf{exactly} (up to machine precision) by systematically applying the chain rule to a sequence of elementary operations. It propagates derivative values, not function evaluations at perturbed points. Consequently, there is no step-size $h$ to worry about, and the issues of truncation and round-off error are avoided.

\subsubsection{AD is Not Symbolic Differentiation}
Symbolic differentiation, as seen in tools like MATLAB's Symbolic Toolbox or Mathematica, manipulates mathematical expressions to derive a symbolic formula for the derivative. For example, if given $f(x) = \sin(x)$, it would return the function $f'(x) = \cos(x)$.
AD, in contrast, works with \textbf{concrete numerical values}. It does not build up a giant symbolic expression for the derivative. Instead, it breaks down a complex function into a sequence of elementary operations (e.g., $+$, $-$, $*$, $/$, $\sin$, $\exp$, $\ln$) and applies the chain rule step-by-step to the numerical results of these intermediate operations. This approach avoids the "expression swell" problem common in symbolic methods, where derivative expressions can grow exponentially in size.

\subsection{The Mechanics of Automatic Differentiation}

\subsubsection{Function Decomposition: The Wengert List}
The core idea behind AD is that any function implemented in a computer program, no matter how complex, can be decomposed into a sequence of elementary operations. This sequence is known as a \textbf{Wengert list} or an \textbf{evaluation trace}. This list is constructed using a set of intermediate variables, which can be categorized as follows:
\begin{itemize}
    \item \textbf{Input Variables:} These are the independent variables of the function, which initialize the trace. We can denote them as $v_{i-n} = x_i$ for $i=1, \dots, n$.
    \item \textbf{Intermediate Variables:} These are the results of each elementary operation, which depend only on previously computed variables in the trace. For instance, $v_i = \phi_j(v_k, v_l, \dots)$, where $\phi_j$ is an elementary function and $k, l < i$.
    \item \textbf{Output Variables:} These are the dependent variables of the function, which are simply the last $m$ intermediate variables calculated.
\end{itemize}

\paragraph{Example of a Wengert List}
Consider the function $f(x_1, x_2) = \ln(x_1) + x_1 x_2$. Its Wengert list provides a step-by-step recipe for evaluating the function:
\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Trace} & \textbf{Description} \\
\hline
$v_{-1} = x_1$ & Input 1 \\
$v_0 = x_2$ & Input 2 \\
$v_1 = \ln(v_{-1})$ & Natural log of input 1 \\
$v_2 = v_{-1} \times v_0$ & Product of inputs \\
$v_3 = v_1 + v_2$ & Sum of intermediate results \\
$y = v_3$ & Final output \\
\hline
\end{tabular}
\end{center}

\subsubsection{From List to Graph: The Computational Graph}
The Wengert list directly defines a \textbf{Directed Acyclic Graph (DAG)}, commonly known as a computational graph. In this graph, nodes represent the variables ($v_i$) in the trace, and the directed edges show the dependencies between them.

For the function $f(x_1, x_2) = \ln(x_1) + x_1 x_2$, the computational graph visualizes the flow of computation. The power of this representation is that each edge corresponds to a simple, local partial derivative that is easy to compute.

\subsubsection{Derivatives on the Edges}
The graph can be annotated with the partial derivative of each node with respect to its immediate predecessors. For our example, these local derivatives are:
\begin{align*}
    \frac{\partial v_1}{\partial v_{-1}} &= \frac{1}{v_{-1}} \\
    \frac{\partial v_2}{\partial v_{-1}} &= v_0 \\
    \frac{\partial v_2}{\partial v_0} &= v_{-1} \\
    \frac{\partial v_3}{\partial v_1} &= 1 \\
    \frac{\partial v_3}{\partial v_2} &= 1
\end{align*}
Automatic Differentiation works by combining these simple local derivatives using the chain rule to compute the overall derivative of the output with respect to any input.

\section{Modes of Automatic Differentiation}

\subsection{Forward Mode AD}
Forward mode AD propagates derivative values \textbf{forward} through the computational graph, from inputs to outputs. At each step, we compute the derivative of the current intermediate variable with respect to one of the original input variables. This derivative is often denoted with a dot, e.g., $\dot{v}_i = \frac{\partial v_i}{\partial x_k}$.

To find the partial derivative $\frac{\partial f}{\partial x_k}$, we "seed" the computation by setting the derivative of the input $x_k$ with respect to itself to 1 (i.e., $\dot{x}_k = 1$) and all other input derivatives to 0 (i.e., $\dot{x}_j = 0$ for $j \neq k$).

Let's find $\frac{\partial f}{\partial x_1}$ for our example function at the point $(x_1, x_2) = (2, 5)$. We seed with $\dot{x}_1 = 1$ and $\dot{x}_2 = 0$. The process involves two traces: the \textbf{Primal Trace} for evaluating the function and the \textbf{Tangent Trace} for evaluating the derivative.

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Primal Trace} & \textbf{Value} & \textbf{Tangent Trace ($\dot{v}_i = \frac{\partial v_i}{\partial x_1}$)} \\
\hline
$v_{-1} = x_1$ & $2$ & $\dot{v}_{-1} = \dot{x}_1 = 1$ \\
$v_0 = x_2$ & $5$ & $\dot{v}_0 = \dot{x}_2 = 0$ \\
$v_1 = \ln(v_{-1})$ & $\ln(2)$ & $\dot{v}_1 = \dot{v}_{-1} / v_{-1} = 1/2$ \\
$v_2 = v_{-1} \times v_0$ & $10$ & $\dot{v}_2 = \dot{v}_{-1}v_0 + v_{-1}\dot{v}_0 = 1 \cdot 5 + 2 \cdot 0 = 5$ \\
$v_3 = v_1 + v_2$ & $\ln(2) + 10$ & $\dot{v}_3 = \dot{v}_1 + \dot{v}_2 = 0.5 + 5 = 5.5$ \\
\hline
\end{tabular}
\end{center}
The final result is $\frac{\partial f}{\partial x_1}(2,5) = 5.5$. To find $\frac{\partial f}{\partial x_2}$, we would need another forward pass with a different seed ($\dot{x}_1 = 0, \dot{x}_2 = 1$). This illustrates a key property of forward mode: it requires one pass through the graph for each input variable.

\subsubsection{A Tool for Forward Mode: Dual Numbers}
Forward mode automatic differentiation (AD) can be implemented elegantly using the algebraic structure of \textbf{dual numbers}. A dual number is an expression of the form:
\begin{equation}
z = a + b\epsilon
\end{equation}
where $a, b \in \mathbb{R}$ and $\epsilon$ is an abstract symbol with the defining property:
\begin{equation}
\epsilon^2 = 0, \quad (\epsilon \neq 0)
\end{equation}
The term $a$ is called the \textit{real part} and $b\epsilon$ is the \textit{dual part}.

The utility of dual numbers in differentiation stems from the Taylor series expansion of a function $f(x)$ around a point $x$. If we replace the small step $h$ with our dual symbol $\epsilon$, the expansion becomes $f(x + \epsilon) = f(x) + f'(x)\epsilon + \frac{f''(x)}{2!}\epsilon^2 + \dots$. Due to the property $\epsilon^2 = 0$, all higher-order terms in the series vanish. This leaves a fundamental identity:
\begin{equation}
\boxed{f(x + \epsilon) = f(x) + f'(x)\epsilon}
\end{equation}
This identity is the cornerstone of forward mode AD with dual numbers. It shows that by evaluating a function with a dual number as input, we can simultaneously compute the function's value (the real part) and its derivative (the dual part).

\paragraph{Dual Numbers in Action: A Complete Example}
Let's compute both $f(x)$ and $f'(x)$ for the function $f(x) = \frac{x^2}{\cos(x)}$ at the point $x = \pi$.
We initialize our input as a dual number $x = \pi + 1\epsilon$. Then we compute:
\begin{enumerate}
    \item Numerator: $x^2 = (\pi + \epsilon)^2 = \pi^2 + 2\pi\epsilon + \epsilon^2 = \pi^2 + 2\pi\epsilon$.
    \item Denominator: $\cos(\pi + \epsilon) = \cos(\pi) + (-\sin(\pi))\epsilon = -1 - (0)\epsilon = -1$.
    \item Division: $\frac{\pi^2 + 2\pi\epsilon}{-1} = -\pi^2 - 2\pi\epsilon$.
\end{enumerate}
By comparing the final result to the form $f(\pi) + f'(\pi)\epsilon$, we extract $f(\pi) = -\pi^2$ and $f'(\pi) = -2\pi$.

\paragraph{Dual Number Arithmetic Rules}
The standard arithmetic operations for dual numbers naturally correspond to the rules of differentiation (sum rule, product rule, chain rule).
\begin{itemize}
    \item \textbf{Sum:} $(a + b\epsilon) + (c + d\epsilon) = (a + c) + (b + d)\epsilon$.
    \item \textbf{Product:} $(a + b\epsilon)(c + d\epsilon) = ac + (ad + bc)\epsilon$.
    \item \textbf{Composite Function:} For $h(x) = g(f(x))$, evaluating $h(x+\epsilon) = g(f(x+\epsilon))$ automatically implements the chain rule, yielding $h(x) + h'(x)\epsilon$.
\end{itemize}

\paragraph{Extensions of the Dual Number Method}
The concept can be extended to compute higher-order derivatives (by defining $\epsilon^k = 0$ for $k>2$) or gradients of multivariate functions (by performing one forward pass for each input variable, seeding it with $\epsilon$ while others are seeded with $0$).

\subsection{Reverse Mode AD (Backpropagation)}
Reverse Mode AD, famously known as backpropagation in the context of neural networks, is a powerful technique for computing gradients. Unlike forward mode, which propagates derivatives from inputs to outputs, reverse mode propagates derivatives \textbf{backward} from the output.

The core objective of reverse mode is to compute the derivative of a single output variable with respect to all input and intermediate variables. For a scalar function $y = f(\mathbf{x})$, we compute the \textbf{adjoints} $\bar{v}_i$, which are defined as the partial derivatives of the final output $y$ with respect to each intermediate variable $v_i$:
\begin{equation}
\bar{v}_i = \frac{\partial y}{\partial v_i}
\end{equation}
The process starts by seeding the derivative of the output with respect to itself, which is trivially 1 (i.e., $\bar{y} = \frac{\partial y}{\partial y} = 1$). The chain rule is then applied backward through the computational graph to compute the adjoints for all other variables.

The primary strength of reverse mode lies in its computational efficiency for a specific class of problems common in machine learning.
\begin{center}
\fcolorbox{black}{red!15}{
\begin{minipage}{0.8\textwidth}
\textbf{Core Idea:} Reverse Mode computes the derivative of \textbf{one output} with respect to \textbf{ALL inputs} in a single backward pass.
\end{minipage}
}
\end{center}
This makes it exceptionally efficient for obtaining the full gradient of a scalar function, such as the loss function in a machine learning model. In this scenario, we have a single output (the loss, $m=1$) and potentially millions of inputs (the model parameters, $n \gg 1$). Reverse mode can compute the entire gradient $\nabla L(\theta)$ in one forward pass followed by one backward pass.

\subsection{Forward vs. Reverse Mode: Which to Choose?}
The choice between forward and reverse mode depends entirely on the dimensionality of the function, specifically the number of inputs ($n$) and outputs ($m$).

\begin{table}[h!]
\centering
\begin{tabular}{|l|p{3.5cm}|p{5cm}|}
\hline
\textbf{Aspect} & \textbf{Forward Mode} & \textbf{Reverse Mode} \\
\hline
\textbf{Computes} & One column of Jacobian ($J_f \mathbf{e}_k$) per pass & One row of Jacobian ($\mathbf{e}_i^T J_f$) per pass \\
\hline
\textbf{Cost for Full Jacobian} & $O(n) \times \text{cost}(f)$ & $O(m) \times \text{cost}(f)$ \\
\hline
\textbf{Best For} & Tall Jacobians ($n \ll m$) & Wide Jacobians ($n \gg m$) \\
\hline
\textbf{Example Use Case} & Finding tangent of a curve in 3D space. ($f: \mathbb{R}^1 \to \mathbb{R}^3$) & \textbf{Machine Learning:} Differentiating a scalar loss ($m=1$) w.r.t. millions of parameters ($n$). \\
\hline
\end{tabular}
\caption{Comparison of Forward and Reverse Mode AD.}
\end{table}

\subsubsection{The Machine Learning Rule of Thumb}
In almost all machine learning optimization problems, the goal is to differentiate a \textbf{scalar loss function} ($m=1$) with respect to a \textbf{large number of parameters} ($n \gg 1$). This corresponds to a very wide Jacobian matrix. According to the comparison table, reverse mode is the ideal choice for this scenario.

\begin{center}
\fcolorbox{black}{red!15}{
\begin{minipage}{0.8\textwidth}
\textbf{The Machine Learning Rule of Thumb:} Since we almost always differentiate a \textbf{scalar loss function ($m = 1$)} with respect to a \textbf{large number of parameters ($n \gg 1$)}, Reverse Mode (Backpropagation) is the default choice.
\end{minipage}
}
\end{center}

\subsection{Computational Complexity and Memory}
Let $\text{ops}(f)$ be the number of floating-point operations required to compute a function $f: \mathbb{R}^n \to \mathbb{R}^m$. The computational cost to obtain the full Jacobian matrix for each mode is approximately:
\begin{itemize}
    \item \textbf{Forward Mode:} $\approx n \times c \cdot \text{ops}(f)$
    \item \textbf{Reverse Mode:} $\approx m \times c \cdot \text{ops}(f)$
\end{itemize}
Here, $c$ is a small constant, typically in the range of $[2, 3]$, representing the overhead of AD. This clearly shows that when $n \gg m$, reverse mode is far superior.

\subsubsection{The Drawback of Reverse Mode: Memory}
Despite its computational advantages, reverse mode has a significant drawback: memory consumption.
\begin{itemize}
    \item \textbf{Graph Storage:} Reverse mode must store the entire computational graph (often called a "tape" or trace) of all intermediate variables computed during the forward pass. These values are required for the chain rule application during the backward pass.
    \item \textbf{Memory Growth:} The memory requirement grows in proportion to the number of operations in the function. For very deep and complex models, this can become a major bottleneck.
    \item \textbf{Large Models:} In fields like deep learning, the memory cost can be a significant challenge, necessitating advanced techniques like \textbf{checkpointing} to manage memory by re-computing parts of the forward pass instead of storing everything.
\end{itemize}

\subsection{Matrix-Free Products}
In many optimization algorithms, we do not need the full Jacobian matrix itself, but rather its product with a vector. Automatic differentiation can compute these "matrix-free" products very efficiently.

\subsubsection{Forward Mode: Jacobian-Vector Product ($J_f\mathbf{v}$)}
Forward mode is naturally suited for computing Jacobian-vector products.
\begin{itemize}
    \item We can compute the product of the Jacobian $J_f$ with a vector $\mathbf{v} \in \mathbb{R}^n$ in a \textbf{single forward pass}.
    \item This is achieved by "seeding" the derivative inputs with the vector $\mathbf{v}$. That is, we set the initial tangent values to $\dot{\mathbf{x}} = \mathbf{v}$.
    \item The resulting derivative output vector $\dot{\mathbf{y}}$ is precisely the desired product: $\dot{\mathbf{y}} = J_f\mathbf{v}$.
\end{itemize}

\subsubsection{Reverse Mode: Transposed-Jacobian-Vector Product ($J_f^T\mathbf{v}$)}
Similarly, reverse mode is naturally suited for computing transposed-Jacobian-vector products.
\begin{itemize}
    \item We can compute the product of the transposed Jacobian $J_f^T$ with a vector $\mathbf{v} \in \mathbb{R}^m$ in a \textbf{single reverse pass}.
    \item This is done by "seeding" the derivative outputs with the vector $\mathbf{v}$. We set the initial adjoints to $\bar{\mathbf{y}} = \mathbf{v}$.
    \item The resulting derivative input vector $\bar{\mathbf{x}}$ is the desired product: $\bar{\mathbf{x}} = J_f^T\mathbf{v}$.
\end{itemize}

This capability is fundamental to many advanced second-order optimization methods that rely on Hessian-vector products, which can be efficiently computed by combining forward and reverse modes.

\newpage
\chapter{Lesson 27-10}

\section{The Perceptron: An Early Model of a Neuron}

The foundational concept of a modern neural network is the artificial neuron. The earliest and simplest model is the \textbf{perceptron}, developed by Frank Rosenblatt in the 1950s and 1960s, drawing inspiration from the earlier work of McCulloch and Pitts. A perceptron is a simple computational unit designed to make decisions by weighing evidence.

\subsection{Core Components of a Perceptron}
The core function of a perceptron is to process multiple inputs and produce a single output. Its defining characteristics are:
\begin{itemize}
    \item It takes several \textbf{binary inputs}, denoted as $x_1, x_2, \dots, x_n$, where each $x_j \in \{0, 1\}$.
    \item It produces a \textbf{single binary output}, which is either 0 or 1.
\end{itemize}
A simple schematic of a perceptron with three inputs is shown in Figure \ref{fig:perceptron_basic}.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.6\textwidth]{figure1.png} % Placeholder for a diagram of a perceptron
    \caption{A simple perceptron with three inputs ($x_1, x_2, x_3$) and one output.}
    \label{fig:perceptron_basic}
\end{figure}

\subsection{The Perceptron Rule}
To make a decision, the perceptron assigns a numerical \textbf{weight} ($w_j$) to each input ($x_j$). These weights are real numbers that signify the importance of each input in the decision-making process. The perceptron then computes a weighted sum of its inputs.

The output is determined by comparing this weighted sum to a predefined \textbf{threshold} value. If the sum exceeds the threshold, the perceptron "fires" and outputs 1; otherwise, it outputs 0.

Mathematically, this rule is expressed as:
\begin{equation}
    \text{output} = \begin{cases}
        0 & \text{if } \sum_j w_j x_j \le \text{threshold} \\
        1 & \text{if } \sum_j w_j x_j > \text{threshold}
    \end{cases}
\end{equation}

\subsection{Simplification: The Role of Bias}
The threshold-based rule is notationally cumbersome. We can simplify it by reformulating the inequality. First, we express the weighted sum as a dot product, $\mathbf{w} \cdot \mathbf{x} = \sum_j w_j x_j$.

Next, we move the threshold to the other side of the inequality and introduce a new term called the \textbf{bias}, defined as $b = -\text{threshold}$. The bias can be intuitively understood as a measure of how easy it is for the perceptron to output a 1. A large positive bias makes it easy for the neuron to "fire," while a large negative bias makes it difficult.

With these changes, the perceptron rule becomes:
\begin{equation}
    \text{output} = \begin{cases}
        0 & \text{if } \mathbf{w} \cdot \mathbf{x} + b \le 0 \\
        1 & \text{if } \mathbf{w} \cdot \mathbf{x} + b > 0
    \end{cases}
\end{equation}
This is the modern and more common representation of the perceptron model.

\section{Universal Computation with Perceptrons}

Perceptrons are not only simple but also powerful. A network of perceptrons can, in principle, compute any logical function. This is because the perceptron can be used to model a \textbf{universal gate}, such as the NAND gate.

\subsection{Perceptrons as Logic Gates: The NAND Gate Example}
A universal gate is a logic gate that can be used by itself to construct all other fundamental logic gates (e.g., NOT, AND, OR). It is a well-known result from digital logic that the NAND gate is universal.

Let's demonstrate how a perceptron can be configured to behave exactly like a NAND gate. Consider a perceptron with two inputs, where each input has a weight of $w_1 = w_2 = -2$, and the bias is $b=3$. The behavior for all possible binary inputs is as follows:
\begin{itemize}
    \item \textbf{Inputs (0, 0):} $(-2 \times 0) + (-2 \times 0) + 3 = 3$. Since $3 > 0$, the \textbf{Output is 1}.
    \item \textbf{Inputs (0, 1):} $(-2 \times 0) + (-2 \times 1) + 3 = 1$. Since $1 > 0$, the \textbf{Output is 1}.
    \item \textbf{Inputs (1, 0):} $(-2 \times 1) + (-2 \times 0) + 3 = 1$. Since $1 > 0$, the \textbf{Output is 1}.
    \item \textbf{Inputs (1, 1):} $(-2 \times 1) + (-2 \times 1) + 3 = -1$. Since $-1 \le 0$, the \textbf{Output is 0}.
\end{itemize}
This output pattern is identical to the truth table of a NAND gate. Since a network of NAND gates can compute any logical function, a network of perceptrons is also computationally universal in this regard.

\section{From Perceptrons to Sigmoid Neurons: Enabling Learning}

\subsection{The Limitation of Perceptrons for Learning}
While perceptrons are computationally powerful, they have a critical limitation that makes them unsuitable for modern learning algorithms. The goal of machine learning is to build a network that can \textbf{learn} from data. Learning involves making small, gradual changes to the weights and biases to slowly improve performance.

The perceptron's output is determined by a step function. This means that a small change in a weight or bias can cause the output to abruptly flip from 0 to 1 (or vice versa). This "jumpy" behavior makes it extremely difficult to gradually modify weights to achieve a desired outcome. For gradient-based learning algorithms, which rely on computing derivatives, this discontinuity is a fundamental problem.

To enable effective learning, we need a "smoother" type of neuron.

\subsection{Introducing the Sigmoid Neuron}
To overcome the limitations of the perceptron, we introduce the \textbf{sigmoid neuron}. It shares many similarities with the perceptron but has one crucial difference that makes it suitable for learning via gradient descent.

\begin{itemize}
    \item \textbf{Similarities to Perceptrons:}
    \begin{itemize}
        \item They have inputs ($x_1, x_2, \dots$).
        \item They have weights ($w_1, w_2, \dots$) and a bias ($b$).
        \item They still compute the weighted sum: $z = \mathbf{w} \cdot \mathbf{x} + b$.
    \end{itemize}
    \item \textbf{Key Differences:}
    \begin{itemize}
        \item The inputs are not restricted to be binary; they can be any real value (typically between 0 and 1).
        \item The output is \textbf{not} a binary 0 or 1. Instead, it is a continuous value between 0 and 1, computed by applying the \textbf{sigmoid function}, $\sigma(z)$, to the weighted sum.
    \end{itemize}
\end{itemize}

\subsection{The Sigmoid Function}
The sigmoid function, also known as the logistic function, is defined as:
\begin{equation}
    \sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation}
where $z = \mathbf{w} \cdot \mathbf{x} + b$.

The function's shape is an "S"-curve, which smoothly transitions between 0 and 1.
\begin{itemize}
    \item As $z \to \infty$ (large positive input), $e^{-z} \to 0$, so $\sigma(z) \to 1$.
    \item As $z \to -\infty$ (large negative input), $e^{-z} \to \infty$, so $\sigma(z) \to 0$.
\end{itemize}
Figure \ref{fig:sigmoid} shows the sigmoid function, which can be seen as a smooth approximation of the perceptron's step function.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.5\textwidth]{figure2.png} % Placeholder for sigmoid function graph
    \caption{The Sigmoid function $\sigma(z)$ as a smooth approximation of a step function.}
    \label{fig:sigmoid}
\end{figure}

\subsection{The Benefit of Smoothness}
The crucial property of the sigmoid function is its \textbf{smoothness}. Unlike the perceptron's "step," the sigmoid is a continuous and differentiable function. This implies that a small change in a weight ($\Delta w_j$) or a small change in the bias ($\Delta b$) will produce only a \textbf{small, predictable change in the output} ($\Delta \text{output}$).

This relationship can be approximated using calculus:
\begin{equation}
    \Delta \text{output} \approx \sum_j \frac{\partial \text{output}}{\partial w_j} \Delta w_j + \frac{\partial \text{output}}{\partial b} \Delta b
\end{equation}
This "small change in $\to$ small change out" property is exactly what is needed for learning. It allows gradient-based algorithms, such as \textbf{gradient descent}, to work by making small, iterative adjustments to the weights and biases to minimize error.

\section{Feedforward Network Architecture}
Neural networks are typically organized into \textbf{layers}.

\begin{itemize}
    \item \textbf{Input Layer:} This is the first layer of the network. The neurons in this layer receive the initial data (e.g., the pixel values of an image).
    \item \textbf{Output Layer:} This is the final layer of neurons that produces the network's result (e.g., a classification score).
    \item \textbf{Hidden Layer(s):} These are any layers between the input and output layers. They are called "hidden" because their values are not directly observed as inputs or outputs. They allow the network to learn more complex, abstract features from the data.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.8\textwidth]{figure3.png} % Placeholder for a diagram of a feedforward network
    \caption{A simple feedforward network with one input layer, one hidden layer, and one output layer.}
    \label{fig:network_layers}
\end{figure}

\subsection{Feedforward Networks}
The simplest and most common network architecture is the \textbf{feedforward network}. The name "feedforward" describes how information flows through the network: it moves in only \textbf{one direction}, from the input layer, through any hidden layers, to the output layer.

In these networks, there are no loops or connections that feed information backward. The output of one layer serves as the input to the next. This is different from \textbf{recurrent neural networks (RNNs)}, which do have feedback loops and are specialized for sequential data like text or time series. Our current focus is on feedforward networks.

\subsection{A Matrix-Based Approach for Computation}
To compute the network's output efficiently, we use a vectorized, matrix-based approach. The notation is as follows:
\begin{itemize}
    \item $w_{jk}^l$: The weight for the connection from the $k^{th}$ neuron in layer $l-1$ to the $j^{th}$ neuron in layer $l$.
    \item $b_j^l$: The bias for the $j^{th}$ neuron in layer $l$.
    \item $a_j^l$: The activation (output) of the $j^{th}$ neuron in layer $l$.
\end{itemize}
We can group these values into matrices and vectors for each layer:
\begin{itemize}
    \item $W^l$: The weight matrix for layer $l$, where the entry in the $j^{th}$ row and $k^{th}$ column is $w_{jk}^l$.
    \item $\mathbf{b}^l$: The bias vector for layer $l$.
    \item $\mathbf{a}^l$: The activation vector for layer $l$.
\end{itemize}
Using this notation, the entire computation for a single layer can be expressed in a compact matrix equation. The activation vector for layer $l$ is computed from the activation vector of the previous layer, $l-1$, as follows:
\begin{equation}
    \mathbf{a}^l = \sigma(W^l \mathbf{a}^{l-1} + \mathbf{b}^l)
\end{equation}
Here, the sigmoid function $\sigma$ is applied element-wise to the vector $W^l \mathbf{a}^{l-1} + \mathbf{b}^l$. This equation represents the fundamental feedforward pass for one layer of the network.

\section{Foundations of Backpropagation}
The goal of backpropagation is to compute the partial derivatives of a cost function $J$ with respect to all the weights and biases in the network, i.e., $\frac{\partial J}{\partial w}$ and $\frac{\partial J}{\partial b}$. This is essential for training the network using gradient-based optimization methods.

\subsection{Two Assumptions About the Cost Function}
To use backpropagation, we need to make two main assumptions about the structure of the cost function.

\subsubsection*{Assumption 1: Average Cost}
The cost function $J$ can be written as an average over per-example cost functions $J_x$.
\begin{equation}
J = \frac{1}{n} \sum_x J_x
\end{equation}
where $n$ is the number of training examples. Backpropagation allows us to compute the partial derivatives $\frac{\partial J_x}{\partial w}$ and $\frac{\partial J_x}{\partial b}$ for a \textbf{single example} $x$. We can then obtain the full gradient for the entire dataset by \textbf{averaging} these individual gradients over all training examples. From now on, we will consider the cost $J$ for a single example $x$.

\subsubsection*{Assumption 2: Function of Outputs}
The cost $J$ can be written as a function of the \textbf{output activations} $\mathbf{a}^L$ from the network. For example, the quadratic cost function is:
\begin{equation}
J = \frac{1}{2} \|\mathbf{y} - \mathbf{a}^L\|^2 = \frac{1}{2} \sum_j (y_j - a_j^L)^2
\end{equation}
This clearly depends on the output activations $a_j^L$. The desired output $\mathbf{y}$ is treated as a fixed parameter during the differentiation process.

\subsection{The Hadamard Product}
Backpropagation uses common linear algebra operations, but also one less common operation: the Hadamard product.

\textbf{Definition:} Suppose $\mathbf{s}$ and $\mathbf{t}$ are two vectors of the same dimension. The Hadamard product, denoted $\mathbf{s} \odot \mathbf{t}$, is the \textbf{element-wise product} of the two vectors.
\begin{equation}
(\mathbf{s} \odot \mathbf{t})_j = s_j t_j
\end{equation}
For example:
\begin{equation*}
\begin{bmatrix} 1 \\ 2 \end{bmatrix} \odot \begin{bmatrix} 3 \\ 4 \end{bmatrix} = \begin{bmatrix} 1 \times 3 \\ 2 \times 4 \end{bmatrix} = \begin{bmatrix} 3 \\ 8 \end{bmatrix}
\end{equation*}
This operation is crucial for the matrix-based equations in backpropagation.

\section{The Four Fundamental Equations of Backpropagation}
Backpropagation is built on four key equations. To get there, we must first define the concept of \textbf{error}.

\subsection{Definition: Error ($\delta_j^l$)}
We define the \textbf{error} $\delta_j^l$ of neuron $j$ in layer $l$ as the partial derivative of the cost $J$ with respect to the neuron's \textbf{weighted input} $z_j^l$.
\begin{equation}
\delta_j^l = \frac{\partial J}{\partial z_j^l}
\end{equation}
We use $\boldsymbol{\delta}^l$ for the vector of errors in layer $l$. Backpropagation provides a way to compute $\boldsymbol{\delta}^l$ for every layer, and then relates this error to the gradients we want to find ($\frac{\partial J}{\partial w}$ and $\frac{\partial J}{\partial b}$).

\subsection{Equation (BP1): Error in the Output Layer}
The error $\boldsymbol{\delta}^L$ in the output layer $L$ is given by:
\begin{equation}
\boldsymbol{\delta}^L = \nabla_a J \odot \sigma'(z^L) \quad \text{(BP1)}
\end{equation}
Where:
\begin{itemize}
    \item $\nabla_a J$ is the vector of partial derivatives $\frac{\partial J}{\partial a_j^L}$. This measures how fast the cost changes with respect to the output activations.
    \item $\sigma'(\mathbf{z}^L)$ is the vector of derivatives $\sigma'(z_j^L)$ of the activation function, evaluated at the weighted inputs $\mathbf{z}^L$.
    \item $\odot$ is the Hadamard product (element-wise multiplication).
\end{itemize}
In component form, this is: $\delta_j^L = \frac{\partial J}{\partial a_j^L} \sigma'(z_j^L)$.

\subsubsection*{Proof of (BP1)}
\begin{enumerate}
    \item \textbf{Start with the definition of error:} $\delta_j^L = \frac{\partial J}{\partial z_j^L}$.
    \item \textbf{Apply the chain rule,} summing over all neurons $k$ in the output layer $L$:
    \begin{equation*}
    \delta_j^L = \sum_k \frac{\partial J}{\partial a_k^L} \frac{\partial a_k^L}{\partial z_j^L}
    \end{equation*}
    \item \textbf{The activation is $a_k^L = \sigma(z_k^L)$.} The derivative $\frac{\partial a_k^L}{\partial z_j^L}$ is non-zero \textbf{only when} $k=j$, because the activation of one neuron only depends on its own weighted input.
    \begin{equation*}
    \frac{\partial a_k^L}{\partial z_j^L} = 
    \begin{cases} 
    \sigma'(z_j^L) & \text{if } k = j \\
    0 & \text{if } k \neq j
    \end{cases}
    \end{equation*}
    \item \textbf{The sum simplifies to just the $k=j$ term:}
    \begin{equation*}
    \delta_j^L = \frac{\partial J}{\partial a_j^L} \sigma'(z_j^L)
    \end{equation*}
\end{enumerate}

\subsection{Equation (BP2): Error in terms of Next Layer}
The error $\boldsymbol{\delta}^l$ in any layer $l$ is computed in terms of the error $\boldsymbol{\delta}^{l+1}$ from the \textbf{next} layer:
\begin{equation}
\boldsymbol{\delta}^l = \left( (\mathbf{W}^{l+1})^T \boldsymbol{\delta}^{l+1} \right) \odot \sigma'(\mathbf{z}^l) \quad \text{(BP2)}
\end{equation}
This equation is the heart of "backpropagation". We start with $\boldsymbol{\delta}^L$ (using BP1) and use (BP2) to compute $\boldsymbol{\delta}^{L-1}$, $\boldsymbol{\delta}^{L-2}$, and so on, \textbf{moving backward} through the network.

\subsubsection*{Proof of (BP2)}
\begin{enumerate}
    \item \textbf{Start with the definition of $\delta_j^l$:} $\delta_j^l = \frac{\partial J}{\partial z_j^l}$
    \item \textbf{Apply the chain rule:} The weighted input $z_j^l$ influences the cost $J$ through its effect on all neurons in the next layer, $l+1$. We sum over all neurons $k$ in layer $l+1$:
    \begin{align*}
        \delta_j^l &= \sum_k \frac{\partial J}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_j^l} = \sum_k \delta_k^{l+1} \frac{\partial z_k^{l+1}}{\partial z_j^l}
    \end{align*}
    \item \textbf{Recall the definition of $z_k^{l+1}$:} $z_k^{l+1} = \sum_j w_{kj}^{l+1} a_j^l + b_k^{l+1} = \sum_j w_{kj}^{l+1} \sigma(z_j^l) + b_k^{l+1}$.
    \item \textbf{Take the partial derivative:} Differentiating $z_k^{l+1}$ with respect to $z_j^l$ yields $w_{kj}^{l+1} \sigma'(z_j^l)$, since only the term in the sum corresponding to index $j$ is non-zero.
    \item \textbf{Substitute this back:}
    \begin{align*}
        \delta_j^l &= \sum_k \delta_k^{l+1} \left( w_{kj}^{l+1} \sigma'(z_j^l) \right) = \sigma'(z_j^l) \sum_k w_{kj}^{l+1} \delta_k^{l+1}
    \end{align*}
    This completes the proof.
\end{enumerate}

\subsection{Equation (BP3): Bias Gradients}
The third equation provides the gradient of the cost function with respect to the biases.
\begin{equation}
\frac{\partial J}{\partial b_j^l} = \delta_j^l \quad \text{(BP3)}
\end{equation}

\subsubsection*{Proof of (BP3)}
\begin{enumerate}
    \item \textbf{By chain rule:} $\frac{\partial J}{\partial b_j^l} = \frac{\partial J}{\partial z_j^l} \frac{\partial z_j^l}{\partial b_j^l}$.
    \item \textbf{By definition:} The term $\frac{\partial J}{\partial z_j^l}$ is simply $\delta_j^l$.
    \item \textbf{From the definition of $z_j^l = \sum_k w_{jk}^{l} a_k^{l-1} + b_j^l$}, we can see that $\frac{\partial z_j^l}{\partial b_j^l} = 1$.
    \item \textbf{Substituting back:} $\frac{\partial J}{\partial b_j^l} = \delta_j^l \cdot 1 = \delta_j^l$.
\end{enumerate}

\subsection{Equation (BP4): Weight Gradients}
The fourth and final equation gives the gradient of the cost function with respect to the weights.
\begin{equation}
\frac{\partial J}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l \quad \text{(BP4)}
\end{equation}

\subsubsection*{Proof of (BP4)}
\begin{enumerate}
    \item \textbf{By chain rule:} $\frac{\partial J}{\partial w_{jk}^l} = \frac{\partial J}{\partial z_j^l} \frac{\partial z_j^l}{\partial w_{jk}^l}$.
    \item \textbf{By definition:} Again, $\frac{\partial J}{\partial z_j^l} = \delta_j^l$.
    \item \textbf{From the definition of $z_j^l = \sum_{k'} w_{jk'}^{l} a_{k'}^{l-1} + b_j^l$}, we find that $\frac{\partial z_j^l}{\partial w_{jk}^l} = a_k^{l-1}$.
    \item \textbf{Substituting back:} $\frac{\partial J}{\partial w_{jk}^l} = \delta_j^l \cdot a_k^{l-1} = a_k^{l-1} \delta_j^l$.
\end{enumerate}

\section{The Backpropagation Algorithm}
The four equations (BP1-BP4) provide a complete and efficient algorithm to compute the gradient of the cost function $J_x$ for a single training example $x$. The algorithm consists of a forward pass followed by a backward pass.

\begin{enumerate}
    \item \textbf{Input:} Set the activation for the input layer, $a^1$, to be the input data $x$.
    \item \textbf{Feedforward:} For each layer $l = 2, 3, \dots, L$:
    \begin{itemize}
        \item Compute the weighted input: $z^l = W^l a^{l-1} + b^l$.
        \item Compute the activation: $a^l = \sigma(z^l)$.
    \end{itemize}
    During this pass, all intermediate values of $z^l$ and $a^l$ are stored for use in the backward pass.

    \item \textbf{Output Error $\delta^L$:} Compute the error for the output layer $L$ using (BP1):
    \begin{equation}
        \delta^L = \nabla_a J \odot \sigma'(z^L)
    \end{equation}

    \item \textbf{Backpropagate Error:} For each layer $l = L-1, L-2, \dots, 2$:
    \begin{equation}
        \delta^l = \left((W^{l+1})^T \delta^{l+1}\right) \odot \sigma'(z^l)
    \end{equation}
    This step iteratively computes the error for each hidden layer, moving from the output towards the input.

    \item \textbf{Output Gradient:} The gradient of the cost $J_x$ is given by the partial derivatives with respect to all weights and biases, which can now be computed using the stored activations and the backpropagated errors.
    \begin{align}
        \frac{\partial J_x}{\partial w_{jk}^l} &= a_k^{l-1} \delta_j^l \\
        \frac{\partial J_x}{\partial b_j^l} &= \delta_j^l
    \end{align}
\end{enumerate}

\section{Advanced Activation Functions}
While foundational activation functions like ReLU and its variants are highly effective, research continues to produce more complex, non-monotonic functions that offer performance benefits in specific deep learning domains, albeit often at a higher computational cost.

\subsection{The Mish Function}
Mish is a smooth, non-monotonic activation function that has demonstrated state-of-the-art performance in computer vision tasks. It is defined as a self-gated function, where the input is multiplied by a transformed version of itself.

The mathematical formulation is:
\begin{equation}
f(x) = x \cdot \tanh(\text{softplus}(x))
\end{equation}
where the softplus function is defined as:
\begin{equation}
\text{softplus}(x) = \ln(1 + e^x)
\end{equation}

Visually, the Mish function resembles Swish. For positive inputs ($x \to \infty$), it behaves like the identity function ($f(x) \approx x$). For large negative inputs ($x \to -\infty$), it approaches a constant value slightly below zero. A key feature is a small negative dip around $x \approx -1.2$, making it non-monotonic.

\subsubsection{Analysis of Mish}

\paragraph{Properties}
\begin{itemize}
    \item \textbf{Shape:} Similar to the Swish function, Mish is smooth and non-monotonic.
    \item \textbf{Bounds:} It is unbounded above but bounded below, with a lower limit of approximately -0.31.
    \item \textbf{Continuity:} The function is continuously differentiable ($C^\infty$), which aids in gradient-based optimization.
\end{itemize}

\paragraph{Advantages}
\begin{itemize}
    \item \textbf{Performance:} It is empirically reported to provide better performance than both Swish and ReLU in many complex deep learning tasks, particularly in object detection.
    \item \textbf{Optimization:} Its smoothness helps with optimization stability and model generalization. The slight negative output for negative inputs can help mitigate the issues of dying neurons.
\end{itemize}

\paragraph{Drawbacks}
\begin{itemize}
    \item \textbf{Computational Cost:} Mish is significantly more computationally expensive than functions like ReLU or even Swish. Its implementation involves the hyperbolic tangent, natural logarithm, and exponential functions, which are more costly to compute than simple comparisons or multiplications.
\end{itemize}

\paragraph{Use Cases}
\begin{itemize}
    \item \textbf{State-of-the-Art Vision Models:} Mish is a state-of-the-art activation function used in demanding computer vision applications. A notable example is its use in the YOLOv4 object detection architecture. Its effectiveness is primarily demonstrated through empirical results rather than extensive theoretical proofs.
\end{itemize}

\section{A Practical Guide to Choosing an Activation Function}
The choice of an activation function is highly dependent on the problem, the network architecture, and computational constraints. There is no single function that is universally superior. The following provides a set of general rules of thumb for making this choice.

\subsection{General Rules of Thumb}
\begin{itemize}
    \item \textbf{Start with ReLU:} It is fast, efficient, and works well for a majority of problems, especially for Convolutional Neural Networks (CNNs). It should be the default starting point for any new project. However, one must be mindful of the "Dying ReLU" problem, where neurons can become permanently inactive if their input is always negative during training.

    \item \textbf{If ReLU Fails:} If you encounter issues with dying neurons or suboptimal performance with ReLU, consider one of its variants.
    \begin{itemize}
        \item \textbf{Leaky ReLU, ELU, or PReLU:} These functions are designed to fix the "dying neuron" problem by allowing a small, non-zero gradient when the unit is not active. They are good alternatives to try before moving to more complex functions.
    \end{itemize}

    \item \textbf{For Deep Feedforward Networks (FNNs):}
    \begin{itemize}
        \item \textbf{Consider SELU:} The Scaled Exponential Linear Unit (SELU) is designed to induce self-normalization, meaning the output of each layer will preserve a mean of 0 and a standard deviation of 1. This property is particularly useful in deep FNNs but requires a specific weight initialization scheme (Lecun initialization).
    \end{itemize}

    \item \textbf{For Recurrent Neural Networks (RNNs):}
    \begin{itemize}
        \item \textbf{Tanh or Sigmoid:} Despite the development of newer functions, Tanh remains a standard and effective choice for RNNs due to its bounded nature (output between -1 and 1), which helps control the flow of gradients through time. Sigmoid is also used, particularly in gating mechanisms like those in LSTMs and GRUs.
    \end{itemize}

    \item \textbf{For State-of-the-Art Performance:}
    \begin{itemize}
        \item \textbf{Experiment with Swish or Mish:} If computational cost is not a primary bottleneck and the goal is to maximize performance on a complex task (e.g., object detection), experimenting with more advanced functions like Swish and Mish is recommended. These functions have shown superior empirical performance in certain domains.
    \end{itemize}
\end{itemize}

\newpage
\chapter{Lesson 03-11}

\section{The Problem of Learning Slowdown}

A significant challenge in training neural networks is the phenomenon of "learning slowdown," particularly when using certain cost functions like the Quadratic Cost (or Mean Squared Error). This issue arises when the network's updates to its weights and biases become very small, even when the network's predictions are far from the desired output.

\subsection{The Quadratic Cost and Its Gradient}
The Quadratic Cost, also known as Mean Squared Error (MSE), is a common starting point for regression problems. For a training set of size $n$, it is defined as:
\begin{equation}
    J = \frac{1}{2n} \sum_{x} \| \mathbf{y}(x) - \mathbf{a}^L(x) \|^2
\end{equation}
where $\mathbf{y}(x)$ is the target output and $\mathbf{a}^L(x)$ is the activation (output) of the final layer $L$ for input $x$.

To update the network's parameters via gradient descent, we compute the partial derivatives of the cost function with respect to the weights ($w$) and biases ($b$) of the output layer. For a neuron $j$ in the output layer $L$, these gradients are:
\begin{align}
    \frac{\partial J}{\partial w_{jk}^L} &= (a_j^L - y_j) \sigma'(z_j^L) a_k^{L-1} \\
    \frac{\partial J}{\partial b_j^L} &= (a_j^L - y_j) \sigma'(z_j^L)
\end{align}
Here, $a_j^L$ is the activation of the $j$-th neuron, $y_j$ is its target value, $z_j^L$ is the weighted input to the neuron, and $a_k^{L-1}$ is the activation of the $k$-th neuron from the previous layer.

The critical term in these equations is $\sigma'(z_j^L)$, the derivative of the activation function. If we use the sigmoid activation function, $\sigma(z)$, its derivative $\sigma'(z)$ has a bell shape that approaches zero as its input $z$ becomes very large (positive or negative). This corresponds to the neuron's activation $a_j^L = \sigma(z_j^L)$ approaching 1 or 0.

This leads to a paradox:
\begin{itemize}
    \item When a neuron is confidently wrong (e.g., its output $a_j^L \approx 0$ when the target $y_j=1$), the error term $(a_j^L - y_j)$ is large.
    \item However, an output close to 0 or 1 means the neuron is "saturated," and the derivative term $\sigma'(z_j^L)$ will be close to zero.
    \item The product of these terms results in a very small gradient, causing the learning process to become extremely slow, precisely when large corrections are needed. This is the essence of the learning slowdown problem, also related to the vanishing gradient problem.
\end{itemize}

\section{The Cross-Entropy Cost Function}
To overcome the learning slowdown caused by the $\sigma'$ term in the gradient, we can select a different cost function. The Cross-Entropy cost function is designed specifically for this purpose, particularly in classification tasks.

\subsection{Definition}
For a single neuron in the output layer performing binary classification (where target $y$ is either 0 or 1), the Cross-Entropy cost is defined as:
\begin{equation}
    J = -\frac{1}{n} \sum_{x} \left[ y \ln(a) + (1-y) \ln(1-a) \right]
\end{equation}
where $a = \sigma(wx+b)$ is the neuron's output activation. This function meets the two primary conditions for a cost function:
\begin{enumerate}
    \item It is non-negative ($J > 0$).
    \item It approaches zero as the network's output $a$ approaches the target value $y$. For example, if $y=1$ and $a \to 1$, the first term $y \ln(a)$ approaches 0, and the second term is zero. Conversely, if $y=0$ and $a \to 0$, the second term $(1-y) \ln(1-a)$ approaches 0, and the first term is zero.
\end{enumerate}

\subsection{Gradient of the Cross-Entropy Cost}
The key advantage of the cross-entropy function becomes apparent when we calculate its gradient with respect to the weights and biases. We use the chain rule as before:
\begin{equation}
    \frac{\partial J}{\partial w_{jk}^L} = \frac{\partial J}{\partial a_j^L} \frac{\partial a_j^L}{\partial z_j^L} \frac{\partial z_j^L}{\partial w_{jk}^L}
\end{equation}
Let's compute each term individually for a single training example (for clarity, we drop the summation and the $1/n$ term):
\begin{itemize}
    \item \textbf{First term ($\frac{\partial J}{\partial a_j^L}$):}
    \begin{equation}
        \frac{\partial J}{\partial a_j^L} = -\left( \frac{y_j}{a_j^L} - \frac{1-y_j}{1-a_j^L} \right) = \frac{a_j^L - y_j}{a_j^L(1-a_j^L)}
    \end{equation}

    \item \textbf{Second term ($\frac{\partial a_j^L}{\partial z_j^L}$):} This is simply the derivative of the sigmoid function, $\sigma'(z_j^L)$. A well-known property of the sigmoid function is that its derivative can be expressed in terms of the function itself:
    \begin{equation}
        \frac{\partial a_j^L}{\partial z_j^L} = \sigma'(z_j^L) = \sigma(z_j^L)(1-\sigma(z_j^L)) = a_j^L(1-a_j^L)
    \end{equation}
\end{itemize}
When we combine these terms, a remarkable cancellation occurs:
\begin{equation}
    \frac{\partial J}{\partial w_{jk}^L} = \underbrace{\frac{a_j^L - y_j}{a_j^L(1-a_j^L)}}_{\frac{\partial J}{\partial a_j^L}} \cdot \underbrace{a_j^L(1-a_j^L)}_{\frac{\partial a_j^L}{\partial z_j^L}} \cdot \underbrace{a_k^{L-1}}_{\frac{\partial z_j^L}{\partial w_{jk}^L}}
\end{equation}
The $a_j^L(1-a_j^L)$ terms cancel out completely. The problematic $\sigma'$ term vanishes from the final gradient expression. The resulting gradients are elegantly simple:
\begin{align}
    \frac{\partial J}{\partial w_{jk}^L} &= (a_j^L - y_j) a_k^{L-1} \\
    \frac{\partial J}{\partial b_j^L} &= (a_j^L - y_j)
\end{align}
The gradient is now directly proportional to the error of the output neuron, $(a_j^L - y_j)$. This means that when the network makes a large error, the gradient will be large, leading to a significant update and faster learning. This property makes the cross-entropy cost function a much more effective choice for classification problems than the quadratic cost.

\section{The Problem of Overfitting}
Another fundamental problem in training neural networks is \textbf{overfitting}.

\subsection{Definition and Symptoms}
A model is said to be overfitting when it learns the training data \textit{too well}, to the point that it begins to model the noise and random idiosyncrasies of the training set rather than the underlying generalizable patterns.

The primary symptom of overfitting is a divergence between the model's performance on the training data and its performance on unseen data (i.e., a validation or test set).
\begin{itemize}
    \item \textbf{Training cost/error} continues to decrease over time as the model gets better at fitting the training samples.
    \item \textbf{Validation cost/error} initially decreases along with the training cost, but then reaches a minimum point and begins to increase. This indicates that the model is losing its ability to generalize.
\end{itemize}
This often happens when a model has too much capacity (too many parameters) relative to the amount of available training data.

\subsection{Regularization: A General Strategy}
To combat overfitting, we use a set of techniques collectively known as \textbf{regularization}. The general idea is to introduce a "complexity penalty" to the cost function that discourages the model from learning overly complex patterns. While getting more training data is the most effective defense, it is not always feasible. Regularization provides a mathematical way to constrain the model's complexity.

\subsection{L2 Regularization (Weight Decay)}
L2 regularization is one of the most common regularization techniques. The core idea is to penalize large weights in the network. A network with smaller weights is considered "simpler" and is generally less prone to overfitting.

We achieve this by adding a regularization term to the original cost function ($C_0$):
\begin{equation}
    C = C_0 + \frac{\lambda}{2n} \sum_{w} w^2
\end{equation}
Here:
\begin{itemize}
    \item $C_0$ is the original cost function (e.g., cross-entropy).
    \item The second term is the L2 norm of the weight vector, summed over all weights in the network.
    \item $\lambda > 0$ is the \textbf{regularization hyperparameter}, which controls the strength of the penalty. A larger $\lambda$ results in a stronger penalty and encourages smaller weights.
\end{itemize}
This technique is also known as \textbf{weight decay}. The reason for this name becomes clear when examining the gradient descent update rule. The new gradient with respect to a weight $w$ is:
\begin{equation}
    \frac{\partial C}{\partial w} = \frac{\partial C_0}{\partial w} + \frac{\lambda}{n}w
\end{equation}
The SGD update rule for the weights then becomes:
\begin{align}
    w &\to w - \eta \left( \frac{\partial C_0}{\partial w} + \frac{\lambda}{n}w \right) \\
      &= \left(1 - \frac{\eta\lambda}{n}\right)w - \eta \frac{\partial C_0}{\partial w}
\end{align}
The term $\left(1 - \frac{\eta\lambda}{n}\right)$ is a factor less than 1. This means that at every update step, the weight $w$ is first scaled down (or "decays") before the standard gradient update from the original cost is applied. This systematically prevents the weights from growing too large, thus regularizing the model. Biases are typically not regularized as they have a smaller effect on model complexity compared to weights.

\subsection{L1 Regularization and Sparsity}

L1 regularization penalizes the \textit{absolute value} of the weights. The L1-regularized cost function is defined as:
\begin{equation}
C = C_0 + \frac{\lambda}{n} \sum_{w} |w|
\end{equation}
The gradient of the L1 term is the sign of the weight, leading to the following SGD update rule:
\begin{align}
w & \rightarrow w - \eta \left( \frac{\partial C_0}{\partial w} + \frac{\lambda}{n} \text{sgn}(w) \right) \\
  & = w - \frac{\eta\lambda}{n}\text{sgn}(w) - \eta \frac{\partial C_0}{\partial w}
\end{align}
where $\text{sgn}(w)$ is -1 if $w < 0$ and +1 if $w > 0$.

\subsubsection{Difference from L2 and Sparsity}
The key difference between L1 and L2 regularization lies in how they shrink the weights:
\begin{itemize}
    \item \textbf{L2 (Weight Decay):} Shrinks weights by a \textit{proportional} amount. Large weights are shrunk more than small weights.
    \item \textbf{L1:} Shrinks weights by a \textit{constant} amount, proportional to $\frac{\eta\lambda}{n}$, pushing them towards zero.
\end{itemize}
This property of L1 regularization tends to set many weights to be \textbf{exactly zero}. This creates a \textbf{sparse model}, which is highly useful for feature selection, as it effectively eliminates the influence of less important features.

\subsection{Visualizing L1 vs. L2 Regularization}
The tendency of L1 to produce sparse solutions can be understood geometrically. We can view regularization as a constrained optimization problem: minimizing the original cost $C_0$ subject to the constraint that the weights lie within a certain region.
\begin{itemize}
    \item For \textbf{L2 regularization}, this constraint region is a circle (or hypersphere), defined by $\sum w_i^2 \le k$.
    \item For \textbf{L1 regularization}, this region is a diamond (or hyper-diamond), defined by $\sum |w_i| \le k$.
\end{itemize}
The optimal solution is found where the level curves (contours) of the original cost function $C_0$ first touch the constraint region. Because the L1 diamond has sharp corners at the axes, the intersection is very likely to occur at one of these corners. A solution at a corner means one of the weight components is exactly zero. In contrast, the smooth L2 circle is unlikely to be touched exactly on an axis, resulting in small but non-zero weights.

\subsection{Dropout}
Dropout is a radically different regularization technique that, instead of modifying the cost function, modifies the network itself during training.

\subsubsection{Training with Dropout}
For each mini-batch during training, the following steps are performed:
\begin{enumerate}
    \item Go through each hidden layer of the network.
    \item For each neuron, \textbf{randomly "drop" it} (set its output to 0) with a certain probability $p$ (e.g., $p = 0.5$).
    \item Perform forward- and back-propagation on this new, "thinned" network.
    \item Update the weights and biases based on the gradients from the thinned network.
    \item Repeat this process for the next mini-batch, with a new random set of dropped-out neurons.
\end{enumerate}
The intuition is that this process is like training a massive \textbf{ensemble} of different, smaller networks. It forces the network to learn redundant representations and prevents it from becoming overly reliant on any single neuron, thus improving generalization.

\subsubsection{Testing and Evaluation}
During the testing or evaluation phase, the full, complete network is used (no neurons are dropped). However, to compensate for the fact that more neurons are active now compared to training, the weights outgoing from the hidden layers are multiplied by the dropout probability $p$.

\section{Improving Training Stability and Speed}
\subsection{The Problem with Weight Initialization}
The way weights are initialized can have a significant impact on the training process. A classical approach is to initialize weights $w$ and biases $b$ from a standard Gaussian (Normal) distribution with mean 0 and standard deviation 1, i.e., $\mathcal{N}(0, 1)$.

Consider a neuron with $n_{in}$ inputs. The weighted sum $z$ is calculated as:
\begin{equation}
z = \sum_{j=1}^{n_{in}} w_j x_j + b
\end{equation}
If the inputs $x_j$ are normalized (e.g., approximately 50\% are 0 and 50\% are 1), and weights are drawn from $\mathcal{N}(0, 1)$, the weighted sum $z$ will also follow a Gaussian distribution. However, its standard deviation will be large:
\begin{equation}
\text{std}(z) \approx \sqrt{n_{in} \cdot \text{std}(w)^2} = \sqrt{n_{in} \cdot 1^2} = \sqrt{n_{in}}
\end{equation}
If $n_{in}$ is large (e.g., 1000 inputs), then $\text{std}(z) \approx \sqrt{1000} \approx 31.6$. This means $z$ is very likely to be a large positive or negative number. For activation functions like the sigmoid, large values of $|z|$ push the neuron's output into the \textbf{saturated regions} (near 0 or 1). In these regions, the gradient $\sigma'(z)$ is close to zero, leading to the vanishing gradient problem and \textbf{slow learning} from the very start of training.

\subsection{A Better Weight Initialization Scheme}
To solve this problem, we can change the standard deviation of the initial weight distribution.
\paragraph{The Solution}
Initialize weights from a Gaussian distribution with mean 0, but with a standard deviation of $1/\sqrt{n_{in}}$:
\begin{equation}
w \sim \mathcal{N}(0, 1/\sqrt{n_{in}})
\end{equation}
where $n_{in}$ is the number of input connections to the neuron.

\paragraph{Why This Works}
With this new initialization, we can re-calculate the standard deviation of $z$:
\begin{equation}
\text{std}(z) = \sqrt{n_{in} \cdot \text{std}(w)^2} = \sqrt{n_{in} \cdot \left(\frac{1}{\sqrt{n_{in}}}\right)^2} = \sqrt{n_{in} \cdot \frac{1}{n_{in}}} = \sqrt{1} = 1
\end{equation}
This ensures that the weighted sum $z$ is now a well-behaved Gaussian $\mathcal{N}(0, 1)$. This keeps the neuron in the "active" region of the sigmoid function, where the gradient is not close to zero, allowing learning to proceed at a much healthier pace. Biases can be initialized from $\mathcal{N}(0, 1)$ or simply set to 0.

\section{Hyperparameter Tuning}
Hyperparameters (HPs) are the settings that are not learned by the model but must be set before training. Tuning them is an iterative process of experimentation often referred to as a "black art."

Key hyperparameters include:
\begin{itemize}
    \item Learning rate $\eta$
    \item Regularization parameter $\lambda$
    \item Mini-batch size
    \item Number of epochs
    \item Network architecture (number of layers, neurons per layer)
\end{itemize}

\subsection{A Broad Strategy for Tuning}
A common iterative strategy for hyperparameter tuning is as follows:
\begin{enumerate}
    \item \textbf{Simplify first.} Turn off all regularization ($\lambda=0$, no dropout) to establish a baseline.
    \item \textbf{Find $\eta$.} Find the "order of magnitude" for the learning rate $\eta$ that causes the training cost to start decreasing. A good starting point is often $\eta=0.01$. If the cost explodes or oscillates, decrease $\eta$ (e.g., to 0.001). If the cost decreases too slowly, increase $\eta$ (e.g., to 0.1 or 1.0).
    \item \textbf{Tune $\lambda$.} Once a reasonable $\eta$ is found, turn on regularization. Start with a value like $\lambda=1.0$ and try values on a log-scale (e.g., 0.1, 1.0, 10.0). Use the \textbf{validation accuracy} to pick the best $\lambda$.
    \item \textbf{Re-tune $\eta$.} Your best $\lambda$ might require a new, fine-tuned $\eta$.
    \item \textbf{Tune Mini-batch size.} This often interacts with $\eta$. A smaller batch size might need a smaller $\eta$. Often, you pick a size that fits your GPU memory and tune $\eta$ accordingly.
\end{enumerate}

\subsection{Tuning the Learning Rate ($\eta$)}
The learning rate $\eta$ is arguably the most important hyperparameter.
\begin{itemize}
    \item \textbf{Too High:} The cost will oscillate wildly or "explode," resulting in NaN (Not a Number) values.
    \item \textbf{Too Low:} The cost will decrease, but \textit{extremely} slowly, wasting computational time.
    \item \textbf{Just Right:} The cost decreases steadily.
\end{itemize}

\paragraph{The "Golden Rule"} A powerful technique is to monitor the \textbf{validation accuracy}. Train with a starting $\eta$ (e.g., 0.1). When the validation accuracy stops improving ("plateaus"), divide $\eta$ by a factor (e.g., 2 or 10) and continue training. Repeating this "learning rate schedule" several times can effectively navigate the cost landscape.

\section{Key Function Properties: Smoothness and Strong Convexity}
In the analysis of optimization algorithms, particularly gradient-based methods, two properties of the objective function are of paramount importance: L-smoothness and $\mu$-strong convexity. These properties provide tangential quadratic bounds on the function, which are essential for deriving convergence guarantees.

\subsection{L-Smoothness (Quadratic Upper Bound)}
A differentiable function $f$ is said to be L-smooth if its gradient is L-Lipschitz continuous. This means there exists a constant $L > 0$ such that for all $\mathbf{x}, \mathbf{y}$ in the domain:
\begin{equation}
    ||\nabla f(\mathbf{x}) - \nabla f(\mathbf{y})|| \leq L||\mathbf{x} - \mathbf{y}||
\end{equation}
The practical meaning of L-smoothness is that the function's curvature is bounded above; it cannot curve or bend "too sharply." For convex, differentiable functions, this condition is equivalent to stating that the function is globally bounded above by a quadratic function tangent at any point $\mathbf{x}$:
\begin{equation}
    f(\mathbf{y}) \leq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) + \frac{L}{2} ||\mathbf{y} - \mathbf{x}||^2
\end{equation}
This inequality signifies that the function $f$ always lies \textit{below} a tangential quadratic "bowl" that opens upwards.

\subsection{$\mu$-Strong Convexity (Quadratic Lower Bound)}
A function $f$ is $\mu$-strongly convex if there exists a constant $\mu > 0$ such that for all $\mathbf{x}, \mathbf{y}$:
\begin{equation}
    f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x}) + \frac{\mu}{2} ||\mathbf{y} - \mathbf{x}||^2
\end{equation}
This property provides a quadratic lower bound on the function. Geometrically, it means the function always lies \textit{above} a tangential quadratic bowl. This is a stronger condition than standard convexity and guarantees that the function is "at least" quadratic, ensuring a unique and sharp minimum.

\subsection{Geometric Interpretation}
When a function is both L-smooth and $\mu$-strongly convex, it is "sandwiched" between two tangential quadratic functions at any given point. This tightly constrains the behavior of the function, which is a key insight for analyzing optimization algorithms.

\begin{figure}[h!]
    \centering
    % Placeholder for actual graph
    \includegraphics[draft,width=0.8\textwidth]{placeholder.png} 
    \caption{A function $f(x)$ (black) that is both L-smooth and $\mu$-strongly convex is bounded by an upper quadratic $q_L(x)$ (red) and a lower quadratic $q_\mu(x)$ (blue) at a point of tangency $x_0$. The function is constrained to lie within the shaded region.}
\end{figure}

\section{The Gradient Descent Algorithm}

\subsection{Objective}
The primary objective of unconstrained optimization is to find the vector $\mathbf{x}^*$ that minimizes a differentiable function $f: \mathbb{R}^d \to \mathbb{R}$. We seek to find:
\begin{equation}
    \mathbf{x}^* = \arg \min_{\mathbf{x} \in \mathbb{R}^d} f(\mathbf{x})
\end{equation}
In the context of machine learning, $f(\mathbf{x})$ is typically the cost function, and $\mathbf{x}$ represents the model's parameters (e.g., weights and biases).

\subsection{The Iterative Update Rule}
Gradient Descent (GD) is an iterative method designed to solve this minimization problem. It generates a sequence of solutions $\mathbf{x}_0, \mathbf{x}_1, \mathbf{x}_2, \dots$ that ideally converge to $\mathbf{x}^*$. The general form of the update rule is:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{x}_t + \mathbf{v}_t
\end{equation}
where $\mathbf{x}_t$ is the solution at iteration $t$ and $\mathbf{v}_t$ is an update vector that determines the direction and magnitude of the step.

\subsection{Choosing the Direction}
To ensure that each step moves closer to the minimum, we require that the function value decreases at each iteration, i.e., $f(\mathbf{x}_{t+1}) < f(\mathbf{x}_t)$. We can analyze this using a first-order Taylor expansion of $f$ around $\mathbf{x}_t$:
\begin{equation}
    f(\mathbf{x}_{t+1}) = f(\mathbf{x}_t + \mathbf{v}_t) \approx f(\mathbf{x}_t) + \nabla f(\mathbf{x}_t)^\top \mathbf{v}_t
\end{equation}
For $f(\mathbf{x}_{t+1})$ to be less than $f(\mathbf{x}_t)$, we must ensure that the second term is negative:
\begin{equation}
    \nabla f(\mathbf{x}_t)^\top \mathbf{v}_t < 0
\end{equation}
The direction $\mathbf{v}_t$ that maximizes this decrease for a given step length is the direction of the \textbf{negative gradient}, $-\nabla f(\mathbf{x}_t)$.

\subsection{The Gradient Descent Algorithm}
By choosing the direction of the negative gradient, we arrive at the standard Gradient Descent algorithm. The update vector is defined as $\mathbf{v}_t = -\gamma \nabla f(\mathbf{x}_t)$, where $\gamma > 0$ is a scalar known as the \textbf{step size} or, more commonly in machine learning, the \textbf{learning rate}.

The complete algorithm is as follows:
\begin{enumerate}
    \item Choose an initial guess $\mathbf{x}_0$.
    \item For $t = 0, 1, 2, \dots$
    \begin{equation}
        \mathbf{x}_{t+1} := \mathbf{x}_t - \gamma \nabla f(\mathbf{x}_t)
    \end{equation}
\end{enumerate}

\section{Basic Convergence Analysis (Proof)}
Our goal is to bound the error of the algorithm, which we define as the suboptimality of the function value at a given iterate, $f(\mathbf{x}_t) - f(\mathbf{x}^*)$.

\subsection{Step 1: Using Convexity}
From the first-order characterization of convexity, we have the following inequality:
\begin{equation}
    f(\mathbf{y}) \geq f(\mathbf{x}) + \nabla f(\mathbf{x})^\top (\mathbf{y} - \mathbf{x})
\end{equation}
By setting $\mathbf{y} = \mathbf{x}_t$ and $\mathbf{x} = \mathbf{x}^*$ and rearranging, we get a bound on our error term:
\begin{equation}
    f(\mathbf{x}_t) - f(\mathbf{x}^*) \leq \nabla f(\mathbf{x}_t)^\top (\mathbf{x}_t - \mathbf{x}^*)
\end{equation}
Note that $\nabla f(\mathbf{x}^*) = \mathbf{0}$. We can rewrite the inequality as $f(\mathbf{x}^*) \geq f(\mathbf{x}_t) + \nabla f(\mathbf{x}_t)^\top (\mathbf{x}^* - \mathbf{x}_t)$, which leads to the same result. Let $\mathbf{g}_t = \nabla f(\mathbf{x}_t)$. Our task is now to bound the term $\mathbf{g}_t^\top (\mathbf{x}_t - \mathbf{x}^*)$.

\subsection{Step 2: Using the GD Update Rule}
We can express the gradient $\mathbf{g}_t$ using the algorithm's update rule:
\begin{equation}
    \mathbf{x}_{t+1} = \mathbf{x}_t - \gamma \mathbf{g}_t \quad \implies \quad \mathbf{g}_t = \frac{\mathbf{x}_t - \mathbf{x}_{t+1}}{\gamma}
\end{equation}

\subsection{Step 3: Substitution and Algebraic Identity}
By substituting the expression for $\mathbf{g}_t$ back into our bound, we get:
\begin{equation}
    \mathbf{g}_t^\top (\mathbf{x}_t - \mathbf{x}^*) = \frac{1}{\gamma} (\mathbf{x}_t - \mathbf{x}_{t+1})^\top (\mathbf{x}_t - \mathbf{x}^*)
\end{equation}
We now use the following algebraic identity for any two vectors $\mathbf{v}$ and $\mathbf{w}$:
\begin{equation}
    2\mathbf{v}^\top \mathbf{w} = ||\mathbf{v}||^2 + ||\mathbf{w}||^2 - ||\mathbf{v} - \mathbf{w}||^2
\end{equation}
Let $\mathbf{v} = \mathbf{x}_t - \mathbf{x}_{t+1}$ and $\mathbf{w} = \mathbf{x}_t - \mathbf{x}^*$. Then $\mathbf{v} - \mathbf{w} = \mathbf{x}^* - \mathbf{x}_{t+1}$. Substituting these into the identity gives:
\begin{equation}
    2(\mathbf{x}_t - \mathbf{x}_{t+1})^\top (\mathbf{x}_t - \mathbf{x}^*) = ||\mathbf{x}_t - \mathbf{x}_{t+1}||^2 + ||\mathbf{x}_t - \mathbf{x}^*||^2 - ||\mathbf{x}_{t+1} - \mathbf{x}^*||^2
\end{equation}
This allows us to reformulate our bound on the error.

\newpage
\chapter{Lesson 04-11}

\section{Recap of Basic Convergence Analysis for Convex Functions}

The foundation for analyzing the convergence of Gradient Descent (GD) rests on a key inequality derived from basic principles. For a general convex function, we arrived at the following bound:
\begin{equation}
    \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{\gamma}{2} \sum_{t=0}^{T-1} \norm{\vec{g}_t}^2 + \frac{1}{2\gamma} \norm{\vec{x}_0 - \vec{x}^*}^2
\end{equation}
where $\vec{g}_t = \grad f(\vec{x}_t)$, $\vec{x}^*$ is an optimal solution, and $\gamma$ is the step size. This result, derived from convexity and the GD update rule, serves as the starting point for analyzing convergence under more specific assumptions about the function $f$. By imposing additional structure on $f$, such as Lipschitz continuity or smoothness, we can derive tighter bounds and faster convergence rates.

\section{Convergence for Lipschitz Convex Functions}

A common assumption in optimization is that the function's gradients are bounded. This property is known as B-Lipschitz continuity of the function, which is a weaker condition than L-smoothness.

\subsection{Theorem for Lipschitz Convex Functions}
Let $f$ be convex and differentiable. Assume:
\begin{itemize}
    \item \textbf{Bounded Initial Distance:} The initial point $\vec{x}_0$ is a finite distance from the optimal set, i.e., $\norm{\vec{x}_0 - \vec{x}^*} \leq R$.
    \item \textbf{Bounded Gradients (B-Lipschitz):} The norm of the gradient is bounded for all $\vec{x}$, i.e., $\norm{\grad f(\vec{x})} \leq B$.
\end{itemize}
By choosing a constant step size $\gamma := \frac{R}{B\sqrt{T}}$, after $T$ iterations of Gradient Descent, we have a bound on the \textit{average error}:
\begin{equation}
    \frac{1}{T} \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{RB}{\sqrt{T}}
\end{equation}

\subsubsection{Implications of the Theorem}
This theorem provides several important insights:
\begin{enumerate}
    \item \textbf{Convergence Rate:} The error of the best iterate, $f(\vec{x}_{\text{best}}) - f(\vec{x}^*)$, is $O(1/\sqrt{T})$. This is because the minimum value in a sequence is always less than or equal to the average.
    \item \textbf{Iteration Complexity:} To guarantee that $f(\vec{x}_{\text{best}}) - f(\vec{x}^*) \leq \epsilon$, we need to set the number of iterations $T \geq \frac{R^2 B^2}{\epsilon^2}$.
    \item \textbf{Overall Rate:} This implies a convergence rate of $O(1/\epsilon^2)$ in terms of the desired accuracy $\epsilon$.
\end{enumerate}

\subsection{Proof for Lipschitz Convex Functions}
The proof begins with the "basic analysis" bound derived for general convex functions.

\subsubsection{Step 1: Start with the Basic Analysis Bound}
We start from the inequality established previously:
\begin{equation*}
    \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{\gamma}{2} \sum_{t=0}^{T-1} \norm{\vec{g}_t}^2 + \frac{1}{2\gamma} \norm{\vec{x}_0 - \vec{x}^*}^2
\end{equation*}

\subsubsection{Step 2: Apply Assumptions}
We use the bounds $\norm{\vec{g}_t} \leq B$ and $\norm{\vec{x}_0 - \vec{x}^*} \leq R$ to simplify the right-hand side.
\begin{align*}
    \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) &\leq \frac{\gamma}{2} \sum_{t=0}^{T-1} B^2 + \frac{1}{2\gamma} R^2 \\
    &= \frac{\gamma B^2 T}{2} + \frac{R^2}{2\gamma}
\end{align*}
The summation $\sum_{t=0}^{T-1} B^2$ becomes $T \cdot B^2$ because $B^2$ is a constant summed $T$ times.

\subsubsection{Step 3: Optimize the Step Size $\gamma$}
The right-hand side is a function of the step size $\gamma$, let's call it $q(\gamma)$. We want to choose $\gamma$ to minimize this upper bound.
\begin{equation*}
    q(\gamma) = \frac{\gamma B^2 T}{2} + \frac{R^2}{2\gamma}
\end{equation*}
To find the minimum, we take the derivative with respect to $\gamma$ and set it to zero:
\begin{equation*}
    q'(\gamma) = \frac{B^2 T}{2} - \frac{R^2}{2\gamma^2} = 0 \implies \gamma^2 = \frac{R^2}{B^2 T} \implies \gamma^* = \frac{R}{B\sqrt{T}}
\end{equation*}
This gives the optimal step size $\gamma^*$ that provides the tightest upper bound.

\subsubsection{Step 4: Substitute Optimal $\gamma^*$}
Plugging $\gamma = \gamma^*$ back into the bound $q(\gamma)$:
\begin{align*}
    \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) &\leq q(\gamma^*) = \frac{1}{2} \left(\frac{R}{B\sqrt{T}}\right) B^2 T + \frac{1}{2} \left(\frac{B\sqrt{T}}{R}\right) R^2 \\
    &= \frac{RB\sqrt{T}}{2} + \frac{RB\sqrt{T}}{2} \\
    &= RB\sqrt{T}
\end{align*}

\subsubsection{Step 5: Obtain Average Error}
Finally, we divide both sides by $T$ to get the bound on the average error:
\begin{equation*}
    \frac{1}{T} \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{RB}{\sqrt{T}}
\end{equation*}
This completes the proof and establishes the $O(1/\sqrt{T})$ convergence rate for the average error.

\section{Convergence for Smooth Convex Functions}
The L-smoothness assumption is stronger than B-Lipschitz continuity and, as a result, yields a better convergence rate.

\subsection{Lemma: Sufficient Decrease}
Let $f$ be L-smooth. By choosing the step size $\gamma = 1/L$, every single step of Gradient Descent is guaranteed to decrease the function value.
\begin{equation}
    f(\vec{x}_{t+1}) \leq f(\vec{x}_t) - \frac{1}{2L}\norm{\grad f(\vec{x}_t)}^2
\end{equation}

\textbf{Proof:}
We start from the L-smoothness upper bound:
\begin{equation*}
    f(\vec{x}_{t+1}) \leq f(\vec{x}_t) + \grad f(\vec{x}_t)^\top (\vec{x}_{t+1} - \vec{x}_t) + \frac{L}{2} \norm{\vec{x}_{t+1} - \vec{x}_t}^2
\end{equation*}
Substitute the gradient descent update rule $\vec{x}_{t+1} - \vec{x}_t = -\frac{1}{L}\grad f(\vec{x}_t)$ (using $\gamma = 1/L$):
\begin{align*}
    f(\vec{x}_{t+1}) &\leq f(\vec{x}_t) + \grad f(\vec{x}_t)^\top \left(-\frac{1}{L}\grad f(\vec{x}_t)\right) + \frac{L}{2} \norm{-\frac{1}{L}\grad f(\vec{x}_t)}^2 \\
    &\leq f(\vec{x}_t) - \frac{1}{L}\norm{\grad f(\vec{x}_t)}^2 + \frac{L}{2L^2}\norm{\grad f(\vec{x}_t)}^2 \\
    &\leq f(\vec{x}_t) - \frac{1}{L}\norm{\grad f(\vec{x}_t)}^2 + \frac{1}{2L}\norm{\grad f(\vec{x}_t)}^2 \\
    &= f(\vec{x}_t) - \frac{1}{2L}\norm{\grad f(\vec{x}_t)}^2
\end{align*}
This confirms that each step provides a sufficient decrease in the function value, proportional to the squared norm of the gradient.

\subsection{Theorem for Smooth Convex Functions}
Let $f$ be convex and L-smooth. By choosing $\gamma = 1/L$, we obtain a much better result for the convergence rate.
\begin{equation}
    f(\vec{x}_T) - f(\vec{x}^*) \leq \frac{L}{2T}\norm{\vec{x}_0 - \vec{x}^*}^2
\end{equation}
This guarantees that the error decreases as $O(1/T)$. Consequently, to guarantee $f(\vec{x}_T) - f(\vec{x}^*) \leq \epsilon$, we need $T \geq \frac{LR^2}{2\epsilon}$ iterations, where $R = \norm{\vec{x}_0 - \vec{x}^*}$. This gives a convergence rate of $O(1/\epsilon)$.

\subsection{Proof for Smooth Convex Functions}

\begin{enumerate}
    \item \textbf{Start:} Begin with the basic analysis result, but set $\gamma = 1/L$:
    \begin{equation*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{1}{2L} \sum_{t=0}^{T-1} \norm{\vec{g}_t}^2 + \frac{L}{2} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}

    \item \textbf{Bound Gradients:} Use the Sufficient Decrease Lemma, rearranged as $\frac{1}{2L}\norm{\vec{g}_t}^2 \leq f(\vec{x}_t) - f(\vec{x}_{t+1})$. Sum this inequality from $t=0$ to $T-1$:
    \begin{equation*}
        \frac{1}{2L} \sum_{t=0}^{T-1} \norm{\vec{g}_t}^2 \leq \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}_{t+1}))
    \end{equation*}

    \item \textbf{Telescoping Sum:} The right-hand side of the sum is a telescoping series, which collapses:
    \begin{equation*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}_{t+1})) = f(\vec{x}_0) - f(\vec{x}_T)
    \end{equation*}
    So, we have the bound: $\frac{1}{2L} \sum_{t=0}^{T-1} \norm{\vec{g}_t}^2 \leq f(\vec{x}_0) - f(\vec{x}_T)$.

    \item \textbf{Substitute:} Plug this bound for the gradient sum back into the equation from Step 1:
    \begin{equation*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) \leq (f(\vec{x}_0) - f(\vec{x}_T)) + \frac{L}{2} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}

    \item \textbf{Rearrange:} Move the function values to the left side:
    \begin{equation*}
        \sum_{t=0}^{T-1} (f(\vec{x}_t) - f(\vec{x}^*)) - (f(\vec{x}_0) - f(\vec{x}_T)) \leq \frac{L}{2} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}
    This simplifies to (by separating the sum and incorporating $f(x_0)$ and $f(x_T)$):
    \begin{equation*}
        \sum_{t=1}^{T} (f(\vec{x}_t) - f(\vec{x}^*)) \leq \frac{L}{2} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}
    
    \item \textbf{Monotonicity:} From the Sufficient Decrease Lemma, we know $f(\vec{x}_{t+1}) \leq f(\vec{x}_t)$. Thus, $f(\vec{x}_T)$ is the smallest value in the sequence $\{f(\vec{x}_1), \dots, f(\vec{x}_T)\}$. This implies:
    \begin{equation*}
        T(f(\vec{x}_T) - f(\vec{x}^*)) \leq \sum_{t=1}^{T} (f(\vec{x}_t) - f(\vec{x}^*))
    \end{equation*}

    \item \textbf{Combine:} Combining the results from steps 5 and 6:
    \begin{equation*}
        T(f(\vec{x}_T) - f(\vec{x}^*)) \leq \frac{L}{2} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}
    Dividing by $T$ gives the final result:
    \begin{equation*}
        f(\vec{x}_T) - f(\vec{x}^*) \leq \frac{L}{2T} \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation*}
\end{enumerate}

\section{Convergence for Smooth \& Strongly Convex Functions}
This is the "best" class of functions for standard Gradient Descent, offering the fastest convergence rate.

\subsection{Theorem for Smooth and Strongly Convex Functions}
Let $f$ be L-smooth and $\mu$-strongly convex ($\mu > 0$). By choosing $\gamma = 1/L$, we obtain two powerful results:
\begin{enumerate}
    \item[\textbf{(i)}] \textbf{(Distance):} The squared distance to the optimum decreases geometrically at each step:
    \begin{equation}
        \norm{\vec{x}_{t+1} - \vec{x}^*}^2 \leq \left(1 - \frac{\mu}{L}\right) \norm{\vec{x}_t - \vec{x}^*}^2
    \end{equation}
    \item[\textbf{(ii)}] \textbf{(Function Value):} The function error decreases exponentially over iterations:
    \begin{equation}
        f(\vec{x}_T) - f(\vec{x}^*) \leq \frac{L}{2} \left(1 - \frac{\mu}{L}\right)^T \norm{\vec{x}_0 - \vec{x}^*}^2
    \end{equation}
\end{enumerate}

\subsubsection{Linear Convergence}
Let $\kappa = L/\mu$ be the \textbf{condition number} of the problem. A large condition number indicates that the function is ill-conditioned (e.g., has very elongated level sets), which can slow down convergence.

The convergence rate shown above is known as \textbf{linear convergence} (or geometric convergence). This means the error is multiplied by a constant factor less than 1 at each iteration.

The number of steps required to reach a tolerance $\epsilon$ is $T = O(\kappa \log(1/\epsilon))$, which is significantly faster than the previous rates.

\subsection{Proof I: Initial Setup}
We start from two key equations:
\begin{enumerate}
    \item The equation derived from the basic analysis (labeled BA1):
    \begin{equation*}
        \vec{g}_t^\top(\vec{x}_t - \vec{x}^*) = \frac{\gamma}{2}\norm{\vec{g}_t}^2 + \frac{1}{2\gamma}(\norm{\vec{x}_t - \vec{x}^*}^2 - \norm{\vec{x}_{t+1} - \vec{x}^*}^2) \quad \text{(BA1)}
    \end{equation*}
    \item The definition of $\mu$-strong convexity (labeled SC1):
    \begin{equation*}
        f(\vec{y}) \geq f(\vec{x}) + \grad f(\vec{x})^\top(\vec{y} - \vec{x}) + \frac{\mu}{2}\norm{\vec{y} - \vec{x}}^2 \quad \text{(SC1)}
    \end{equation*}
\end{enumerate}
We set $\vec{x} = \vec{x}_t$, $\vec{y} = \vec{x}^*$, and $\vec{g}_t = \grad f(\vec{x}_t)$ in SC1 and rearrange to get a lower bound on the gradient term:
\begin{equation}
    \vec{g}_t^\top(\vec{x}_t - \vec{x}^*) \geq f(\vec{x}_t) - f(\vec{x}^*) + \frac{\mu}{2}\norm{\vec{x}_t - \vec{x}^*}^2 \quad \text{(SC2)}
\end{equation}
By combining BA1 and SC2, we can derive the linear convergence rate.

\subsection{Proof IV: Bounding the Noise Term}
In a previous step, we derived a recurrence relation for the distance to the optimum, which included a "Noise" term. Let us now show that this term is non-positive.
\begin{align*}
    \text{Noise} = 2\gamma(f(\vec{x}^*) - f(\vec{x}_t)) + \gamma^2 \norm{\vec{g}_t}^2
\end{align*}
From the sufficient decrease property for L-smooth functions (Step 4 of the proof), we have the inequality $f(\vec{x}^*) - f(\vec{x}_t) \le -\frac{1}{2L}\norm{\vec{g}_t}^2$. By substituting this inequality and choosing the step size $\gamma = 1/L$, we can bound the noise term:
\begin{align*}
    \text{Noise} & \le 2\left(\frac{1}{L}\right) \left(-\frac{1}{2L}\norm{\vec{g}_t}^2\right) + \left(\frac{1}{L}\right)^2 \norm{\vec{g}_t}^2 \\
    & \le -\frac{1}{L^2}\norm{\vec{g}_t}^2 + \frac{1}{L^2}\norm{\vec{g}_t}^2 = 0
\end{align*}
Thus, the noise term is less than or equal to zero. This is a crucial result, as it means this component of the recurrence relation actively contributes to reducing the error at each iteration, ensuring progress towards the minimum.

\subsection{Proof V: Recursive Application for Convergence Rates}
With the noise term shown to be non-positive, we can finalize the proof for both distance and function value convergence.

\subsubsection{Proof of (i): Distance to Optimum}
The inequality from Step 1 of the proof, which relates successive iterates, was:
\begin{equation*}
    \norm{\vec{x}_{t+1} - \vec{x}^*}^2 \le (1 - \mu\gamma)\norm{\vec{x}_t - \vec{x}^*}^2 + \text{Noise}
\end{equation*}
Since $\text{Noise} \le 0$, this simplifies to a geometric contraction:
\begin{equation*}
    \norm{\vec{x}_{t+1} - \vec{x}^*}^2 \le (1 - \mu\gamma)\norm{\vec{x}_t - \vec{x}^*}^2
\end{equation*}
Substituting our chosen step size $\gamma = 1/L$:
\begin{equation*}
    \norm{\vec{x}_{t+1} - \vec{x}^*}^2 \le \left(1 - \frac{\mu}{L}\right)\norm{\vec{x}_t - \vec{x}^*}^2
\end{equation*}
By applying this relation recursively for $T$ steps, we arrive at the geometric convergence rate for the distance to the optimum:
\begin{equation}
    \norm{\vec{x}_T - \vec{x}^*}^2 \le \left(1 - \frac{\mu}{L}\right)^T \norm{\vec{x}_0 - \vec{x}^*}^2
\end{equation}

\subsubsection{Proof of (ii): Function Value Error}
To prove the convergence rate for the function value, we start with the L-smoothness upper bound, centered at the optimum $\vec{x}^*$:
\begin{equation*}
    f(\vec{x}_T) \le f(\vec{x}^*) + \grad f(\vec{x}^*)^T (\vec{x}_T - \vec{x}^*) + \frac{L}{2} \norm{\vec{x}_T - \vec{x}^*}^2
\end{equation*}

\subsection{Proof VI: Final Combination}
We simplify the inequality from the previous step by noting that the gradient at the minimum is zero, i.e., $\grad f(\vec{x}^*) = \mathbf{0}$. This eliminates the linear term:
\begin{equation*}
    f(\vec{x}_T) - f(\vec{x}^*) \le \frac{L}{2} \norm{\vec{x}_T - \vec{x}^*}^2
\end{equation*}
Now, we combine this with the result from part (i). We substitute the bound on $\norm{\vec{x}_T - \vec{x}^*}^2$ into the inequality above:
\begin{equation}
    f(\vec{x}_T) - f(\vec{x}^*) \le \frac{L}{2} \left(1 - \frac{\mu}{L}\right)^T \norm{\vec{x}_0 - \vec{x}^*}^2
\end{equation}
This completes the proof, establishing an exponential decrease in the function error.

\section{Convergence Summary and the Line Search Challenge}

\subsection{Gradient Descent Convergence Rates}
The theoretical guarantees for gradient descent depend heavily on the properties of the objective function. Stronger assumptions (e.g., adding smoothness, then strong convexity) lead to exponentially faster convergence guarantees. The table below summarizes the error rates and the number of iterations $T$ required to achieve an error of $\epsilon$.

\begin{center}
\begin{tabular}{|l|c|c|}
    \hline
    \textbf{Function Class} & \textbf{Error Rate} & \textbf{Iterations T for error $\epsilon$} \\
    \hline
    Lipschitz Convex & $O(1/\sqrt{T})$ & $O(1/\epsilon^2)$ \\
    Smooth Convex & $O(1/T)$ & $O(1/\epsilon)$ \\
    Smooth \& Strongly Conv. & $O((1 - \mu/L)^T)$ & $O(\kappa \log(1/\epsilon))$ \\
    \hline
\end{tabular}
\end{center}
Here, $\kappa = L/\mu$ is the condition number of the function.

\subsection{The Core Problem: Choosing the Step Size}
The standard gradient descent update rule is:
\begin{equation*}
    \vec{x}_{k+1} = \vec{x}_k - \gamma_k \grad f(\vec{x}_k)
\end{equation*}
A central challenge in gradient descent is choosing the step size (or learning rate) $\gamma_k$ at each iteration.
\begin{itemize}
    \item \textbf{Plain GD (Fixed $\gamma$):} Using a fixed step size is simple but problematic. If $\gamma$ is too small, convergence is very slow. If it is too large, the algorithm can overshoot the minimum, leading to oscillation or even divergence.
    \item \textbf{Line Search:} A more robust strategy is to choose $\gamma_k$ intelligently at each step. This transforms the problem into a sequence of 1D minimization problems.
\end{itemize}

\subsection{Line Search Strategies}
Given the current iterate $\vec{x}_k$ and the descent direction $\vec{d}_k = -\grad f(\vec{x}_k)$, we want to find a step size $\gamma_k > 0$ that minimizes the function along that line. We define a new, 1-dimensional function:
\begin{equation*}
    \phi(\gamma) = f(\vec{x}_k + \gamma \vec{d}_k)
\end{equation*}
The goal is to find a good $\gamma_k$ that minimizes $\phi(\gamma)$. There are two main strategies for this.

\subsubsection{Method 1: Exact Line Search}
The idea is to find the $\gamma_k$ that perfectly minimizes the function along the search direction.
\begin{equation*}
    \gamma_k = \argmin_{\gamma>0} \, \phi(\gamma) = \argmin_{\gamma>0} \, f(\vec{x}_k + \gamma \vec{d}_k)
\end{equation*}
Theoretically, this is solved by finding the root of the derivative, $\phi'(\gamma) = 0$. Using the chain rule, this gives:
\begin{equation*}
    \phi'(\gamma) = \grad f(\vec{x}_k + \gamma \vec{d}_k)^T \vec{d}_k = 0
\end{equation*}
This condition means that the new gradient, $\grad f(\vec{x}_{k+1})$, must be orthogonal to the previous search direction, $\vec{d}_k$.

\begin{itemize}
    \item \textbf{Advantages:} Makes the most possible progress in the chosen direction and can converge in very few iterations, especially for quadratic functions, where it produces a characteristic "zigzag" path.
    \item \textbf{Drawbacks:} The computational cost is extremely high, as solving the `arg min` subproblem is often as hard as the original problem. It is almost never used in practice for complex models like those in deep learning.
\end{itemize}

\subsubsection{Method 2: Backtracking Line Search (Inexact)}
The idea is not to find the perfect $\gamma_k$, but to find a "good enough" value that guarantees sufficient decrease quickly. This is an iterative procedure.

\textbf{Algorithm:}
\begin{enumerate}
    \item \textbf{Choose parameters:}
    \begin{itemize}
        \item $\bar{\gamma} > 0$ (initial guess, e.g., $\bar{\gamma} = 1.0$)
        \item $c \in (0, 1)$ (controls "sufficient decrease", e.g., $c = 10^{-4}$)
        \item $\tau \in (0, 1)$ (shrink factor, e.g., $\tau = 0.5$)
    \end{itemize}
    \item Set $\gamma = \bar{\gamma}$.
    \item \textbf{While} $f(\vec{x}_k + \gamma \vec{d}_k) > f(\vec{x}_k) + c \gamma \grad f(\vec{x}_k)^T \vec{d}_k$:
    \begin{itemize}
        \item $\gamma \leftarrow \tau\gamma$ (Shrink the step size)
    \end{itemize}
    \item \textbf{End While}
    \item Set $\gamma_k = \gamma$.
\end{enumerate}
\textbf{Note:} The condition $f(\vec{x}_k + \gamma \vec{d}_k) \le f(\vec{x}_k) + c \gamma \grad f(\vec{x}_k)^T \vec{d}_k$ is called the \textbf{Armijo Condition}. Since $\vec{d}_k = -\vec{g}_k$ and $\grad f(\vec{x}_k)^T \vec{d}_k = -\grad f(\vec{x}_k)^T \vec{g}_k = -\norm{\vec{g}_k}^2$, it ensures the new point is sufficiently lower than the old one.

\begin{itemize}
    \item \textbf{Advantages:} Practical, efficient, robust, and widely used. It is the default line search in many serious optimization packages.
    \item \textbf{Drawbacks:} Requires tuning parameters ($c, \tau, \bar{\gamma}$), though default values often work well. It can require several function evaluations within one iteration, which can be costly.
\end{itemize}

\textbf{Key Takeaway:} For most optimization problems, an \textbf{inexact line search} like backtracking provides the best balance of low computational cost and robust convergence.

\section{Constrained Minimization}
So far, we have focused on unconstrained minimization problems of the form:
\begin{equation*}
    \underset{\vec{x} \in \mathbb{R}^n}{\text{minimize}} \quad f(\vec{x})
\end{equation*}
However, many real-world problems have constraints. The problem then becomes:
\begin{equation*}
    \underset{\vec{x} \in C}{\text{minimize}} \quad f(\vec{x})
\end{equation*}
where $C$ is a closed, convex set representing our constraints.

\textbf{Examples of Constraint Sets C:}
\begin{itemize}
    \item \textbf{Non-negativity:} $C = \{\vec{x} \mid x_i \ge 0 \text{ for all } i\}$
    \item \textbf{Box Constraints:} $C = \{\vec{x} \mid l_i \le x_i \le u_i\}$
    \item \textbf{Norm Balls:} We want to find a solution $\vec{x}$ with a "small" norm.
    \begin{itemize}
        \item $L_2$ Ball: $C = \{\vec{x} \mid \norm{\vec{x}}_2 \le R\}$ (The "Unitary Ball" if $R=1$)
        \item $L_1$ Ball: $C = \{\vec{x} \mid \norm{\vec{x}}_1 \le R\}$
    \end{itemize}
\end{itemize}

\subsection{Why Standard Gradient Descent Fails}
The problem with standard gradient descent is that even if the current iterate $\vec{x}_k$ is in the set $C$ (i.e., $\vec{x}_k$ is \textit{feasible}), the next step $\vec{x}_{k+1}$ may land \textbf{outside} of $C$. We need a way to move in the direction of the negative gradient while staying \textit{inside} the set $C$.

\subsection{The Projection Operator: $\mathcal{P}_C$}
To handle constraints, we introduce the projection operator.

\textbf{Definition:} The \textit{projection} of a point $\vec{y}$ onto a convex set $C$, denoted $\mathcal{P}_C(\vec{y})$, is the point in $C$ that is closest to $\vec{y}$.
\begin{equation*}
    \mathcal{P}_C(\vec{y}) = \argmin_{\vec{x} \in C} \norm{\vec{x} - \vec{y}}_2^2
\end{equation*}
\begin{itemize}
    \item If $\vec{y} \in C$: The closest point is $\vec{y}$ itself. $\mathcal{P}_C(\vec{y}) = \vec{y}$.
    \item If $\vec{y} \notin C$: $\mathcal{P}_C(\vec{y})$ is a point on the boundary of $C$.
\end{itemize}

\section{The Projected Gradient Method (PGM) Algorithm}

For constrained optimization problems, where we seek to minimize a function $f(\vec{x})$ subject to $\vec{x} \in C$ for some feasible set $C$, a simple and effective approach is the Projected Gradient Method (PGM). The core idea is to first take a standard gradient descent step and then project the resulting point back onto the feasible set. This can be summarized as: \textbf{Descend, then Project}.

At each iteration $k$, the algorithm performs two steps:
\begin{enumerate}
    \item \textbf{Gradient Step (like standard GD):} Take a step in the negative gradient direction.
    \begin{equation}
        \vec{y}_{k+1} = \vec{x}_k - \gamma_k \grad f(\vec{x}_k)
    \end{equation}
    This point $\vec{y}_{k+1}$ is our "desired" point, but it may lie outside the feasible set $C$, making it an infeasible solution.

    \item \textbf{Projection Step:} Project the result $\vec{y}_{k+1}$ back onto the feasible set $C$. The projection operator $\mathcal{P}_C(\vec{y})$ finds the point in $C$ that is closest to $\vec{y}$ in Euclidean distance.
    \begin{equation}
        \vec{x}_{k+1} = \mathcal{P}_C(\vec{y}_{k+1})
    \end{equation}
\end{enumerate}

Combining these two steps, we can write a single-line update rule for PGM:
\begin{equation}
    \vec{x}_{k+1} = \mathcal{P}_C \left( \vec{x}_k - \gamma_k \grad f(\vec{x}_k) \right)
\end{equation}

A key condition for the efficiency of this method is that the projection $\mathcal{P}_C(\vec{y})$ must be \textbf{easy to compute}. If the projection itself is a hard optimization problem, the benefits of PGM are lost.

\subsection{Case 1: Projection onto the $L_2$ Ball}
A common and computationally cheap projection is onto an $L_2$ ball. Let the feasible set be $C = \{\vec{x} \mid \norm{\vec{x}}_2 \leq R\}$. This is known as the "unitary ball" if $R=1$. The intermediate point after the gradient step is $\vec{y} = \vec{x}_k - \gamma_k \grad f(\vec{x}_k)$. We then need to compute $\vec{x}_{k+1} = \mathcal{P}_C(\vec{y})$.

The $L_2$ projection is a simple "shrinking" operation:
\begin{equation}
\mathcal{P}_C(\vec{y}) =
\begin{cases}
    \vec{y} & \text{if } \norm{\vec{y}}_2 \leq R \quad (\text{already inside}) \\
    R \cdot \frac{\vec{y}}{\norm{\vec{y}}_2} & \text{if } \norm{\vec{y}}_2 > R \quad (\text{shrink to boundary})
\end{cases}
\end{equation}
If the point is already within the ball, it remains unchanged. If it is outside, it is scaled down along its direction until it lies on the boundary of the ball.

This can be written more compactly as:
\begin{equation}
    \mathcal{P}_C(\vec{y}) = \vec{y} \min \left(1, \frac{R}{\norm{\vec{y}}_2}\right)
\end{equation}

Key properties of the $L_2$ projection:
\begin{itemize}
    \item It is \textbf{very cheap} to compute, requiring only a norm calculation and a vector scaling.
    \item It scales the entire vector but \textbf{does not change its direction}.
    \item It \textbf{does not create sparsity}. If all components of $\vec{y}$ are non-zero, they will remain non-zero after projection.
\end{itemize}

\subsection{Case 2: Projection onto the $L_1$ Ball}
Now, let's consider the feasible set defined by an $L_1$ ball: $C = \{\vec{x} \mid \norm{\vec{x}}_1 \leq R\}$. This case is much trickier. The projection is defined by the following optimization problem:
\begin{equation}
    \mathcal{P}_C(\vec{y}) = \argmin_{\vec{x} \in C} \norm{\vec{x} - \vec{y}}_2^2 \quad \text{s.t.} \quad \sum_i |x_i| \leq R
\end{equation}
Unlike the $L_2$ case, there is no simple, closed-form formula for this projection. However, efficient algorithms exist that can compute it in $O(n \log n)$ time.

The crucial property of the $L_1$ projection is that it is \textbf{sparsity-inducing}. It preferentially sets small components of $\vec{y}$ to exactly zero. This is a direct consequence of the geometry of the $L_1$ ball, which has "corners" at the axes.

\section{The Link: Constrained vs. Regularized Problems}
There is a deep connection in optimization, often formalized via Lagrangian duality, between a constrained problem and a regularized one.

\begin{itemize}
    \item \textbf{Form 1: Constrained Problem}
    \begin{equation}
        \underset{\vec{x} \in C}{\text{minimize}} \quad f(\vec{x}) \quad \text{where} \quad C = \{\vec{x} \mid \Omega(\vec{x}) \leq R\}
    \end{equation}
    This form is solved by \textbf{Projected Gradient Descent}:
    \begin{equation}
        \vec{x}_{k+1} = \mathcal{P}_C(\vec{x}_k - \gamma_k \vec{g}_k)
    \end{equation}
    where $\vec{g}_k = \grad f(\vec{x}_k)$.

    \item \textbf{Form 2: Regularized Problem}
    \begin{equation}
        \underset{\vec{x}}{\text{minimize}} \quad f(\vec{x}) + \lambda \cdot \Omega(\vec{x})
    \end{equation}
    This form is solved by \textbf{Proximal Gradient Descent}:
    \begin{equation}
        \vec{x}_{k+1} = \text{prox}_{\gamma_k \lambda \Omega}(\vec{x}_k - \gamma_k \vec{g}_k)
    \end{equation}
\end{itemize}
For convex problems, for every $R > 0$, there exists a $\lambda \geq 0$ (and vice-versa) such that these two forms have the \textbf{same solution}.

\subsection{PGM on $L_2$ Ball vs. $L_2$ Regularization (Ridge)}
\begin{itemize}
    \item \textbf{PGM on $L_2$ Ball:}
    \[
    \text{minimize} \quad f(\vec{x}) \quad \text{s.t.} \quad \norm{\vec{x}}_2 \leq R
    \]
    The mechanism is a \textbf{"Hard" constraint}. If a vector is too long after the gradient step, it gets clipped to the boundary.

    \item \textbf{$L_2$ Regularization (Ridge Regression):}
    \[
    \text{minimize} \quad f(\vec{x}) + \lambda \norm{\vec{x}}_2^2
    \]
    The proximal operator for this problem is equivalent to \textbf{weight decay}. The update rule is:
    \[
    \vec{x}_{k+1} = (1 - \gamma_k \lambda') (\vec{x}_k - \gamma_k \vec{g}_k)
    \]
    The mechanism is a \textbf{"Soft" penalty}. All weights are shrunk (decayed) by a factor at each step.
\end{itemize}
\textbf{Connection:} Both methods \textbf{shrink} weights to control model complexity.

\subsection{PGM on $L_1$ Ball vs. $L_1$ Regularization (LASSO)}
This is the most important connection.
\begin{itemize}
    \item \textbf{PGM on $L_1$ Ball:}
    \[
    \text{minimize} \quad f(\vec{x}) \quad \text{s.t.} \quad \norm{\vec{x}}_1 \leq R
    \]
    The mechanism is that the $L_1$ projection \textbf{creates sparsity} by setting small components to 0.

    \item \textbf{$L_1$ Regularization (LASSO):}
    \[
    \text{minimize} \quad f(\vec{x}) + \lambda \norm{\vec{x}}_1
    \]
    The proximal operator for this problem is the \textbf{Soft-Thresholding Operator} $S_\tau$:
    \[
    [S_\tau(\vec{y})]_i = \text{sign}(y_i) \cdot \max(0, |y_i| - \tau)
    \]
    where $\tau = \gamma_k \lambda$. The mechanism is that this operator also \textbf{creates sparsity} by setting components with absolute value less than $\tau$ to 0.
\end{itemize}
\textbf{Connection:} Both methods \textbf{induce sparsity} by setting irrelevant features to exactly 0.

\section{Introduction to Stochastic Gradient Descent (SGD)}

\subsection{The Problem Setting: Sum-Structured Objectives}
Many objective functions in machine learning are structured as a sum or an average over a dataset. If we have $n$ data points, the objective function $f(\vec{x})$ can often be written as:
\begin{equation}
    f(\vec{x}) = \frac{1}{n} \sum_{i=1}^n f_i(\vec{x})
\end{equation}
Here, $f_i$ is typically the loss function for the $i$-th data point, and $n$ is the total number of data points in the training set.

\subsection{Stochastic Gradient Descent (SGD) Iteration}
Standard Gradient Descent would require computing the gradient of the full objective function $f(\vec{x})$, which involves summing the gradients from all $n$ data points. This can be computationally expensive if $n$ is large.

Stochastic Gradient Descent (SGD) approximates this by using the gradient of only a single, randomly chosen data point at each iteration. An iteration of SGD is defined as:
\begin{enumerate}
    \item Sample an index $i \in \{1, \dots, n\}$ uniformly at random.
    \item Update $\vec{x}$ using only the gradient of $f_i$:
    \begin{equation}
        \vec{x}_{t+1} := \vec{x}_t - \gamma_t \grad f_i(\vec{x}_t)
    \end{equation}
\end{enumerate}

\subsection{The Advantage of SGD: Efficiency}
\begin{itemize}
    \item \textbf{Full Gradient (GD):} To compute $\grad f(\vec{x}_t) = \frac{1}{n} \sum_{i=1}^n \grad f_i(\vec{x}_t)$, we must compute $n$ individual gradients.
    \item \textbf{Stochastic Gradient (SGD):} An iteration requires computing only a \textbf{single} gradient, $\grad f_i(\vec{x}_t)$.
\end{itemize}
This makes each SGD iteration $n$ times cheaper than a full GD iteration. The core question is whether these much cheaper iterations still provide meaningful progress toward the minimum. The answer is yes, but with trade-offs.

\subsection{Unbiasedness of the Stochastic Gradient}
The update vector $\vec{g}_t := \grad f_i(\vec{x}_t)$ is called the \textbf{stochastic gradient}. This stochastic gradient is an \textbf{unbiased estimator} of the true gradient $\grad f(\vec{x})$.

In expectation, over the random choice of index $i$, the stochastic gradient $\vec{g}_t$ equals the full gradient:
\begin{equation}
    \E_i[\vec{g}_t | \vec{x}_t = \vec{x}] = \E_i[\grad f_i(\vec{x})] = \sum_{i=1}^n \frac{1}{n} \grad f_i(\vec{x}) = \grad f(\vec{x})
\end{equation}
This property is crucial for the convergence analysis of SGD. Although a single stochastic gradient step might not point directly downhill, on average, the steps move in the correct direction.

\subsection{How to Sample: With or Without Replacement?}
A crucial implementation detail is how the index $i$ is sampled at each step.
\begin{itemize}
    \item \textbf{Sampling With Replacement:}
    \begin{itemize}
        \item \textbf{What it is:} In each step, pick an index $i \in \{1, \dots, n\}$ uniformly at random from the entire set of indices.
        \item \textbf{Analysis:} This is the standard assumption used in theoretical proofs because it ensures that the sampled gradients $(\vec{g}_t, \vec{g}_{t+1}, \dots)$ are all independent and identically distributed (i.i.d.), which simplifies the analysis.
        \item \textbf{Drawback:} In one "epoch" (a pass through $n$ samples), it is very likely that you will miss some data points and sample others multiple times.
    \end{itemize}
    \item \textbf{Sampling Without Replacement ("Random Reshuffling"):}
    \begin{itemize}
        \item \textbf{What it is:}
        \begin{enumerate}
            \item Create a random permutation (shuffle) of the $n$ data indices.
            \item Pass through the data in that shuffled order.
            \item When all $n$ are used, re-shuffle and start a new epoch.
        \end{enumerate}
        \item \textbf{This is what is done in practice!}
        \item \textbf{Advantage:} Guarantees every data point is used exactly once per epoch. Empirically, it converges faster.
        \item \textbf{Drawback:} The samples within an epoch are \textbf{not} independent. This makes the theoretical analysis much, much harder.
    \end{itemize}
\end{itemize}

\section{The Goal in Machine Learning: Optimization vs. Generalization}

A fundamental distinction must be made between the goals of pure numerical optimization and optimization within the context of machine learning.

\subsection{Optimization vs. Generalization}

In **pure optimization**, the objective is to find the exact minimum of a given function $f(\vec{x})$. For instance, in an engineering problem like aeronautical design, one might seek to find the precise parameters defining an airfoil shape that results in the absolute minimum aerodynamic drag. In this context, finding the true minimizer of the objective function is the ultimate goal.

In **machine learning**, the function being minimized, $f(\vec{x})$, represents the **training loss**. While we employ optimization algorithms to reduce this loss, our primary objective is not to minimize it perfectly. The true measure of a model's success is its performance on new, unseen data. This is quantified by the **test loss**, also known as the "generalization error."

Minimizing the training loss to its absolute minimum often leads to a phenomenon known as **overfitting**. In this scenario, the model essentially "memorizes" the training data, capturing not only the underlying patterns but also the noise specific to that dataset. Consequently, while it performs exceptionally well on the data it was trained on, it fails to generalize to new data, resulting in poor performance on the test set.

Therefore, the core challenge in machine learning is not just optimization but balancing the reduction of training loss with the ability to generalize well.

\subsection{Early Stopping: A Practical Regularization Technique}
To combat overfitting, we almost never train an optimization algorithm like Stochastic Gradient Descent (SGD) until it fully converges on the training loss. Instead, a widely used technique is **early stopping**.

The process is as follows:
\begin{enumerate}
    \item We monitor the model's performance not only on the training set but also on a separate **validation set**a portion of data not used for training the model's parameters.
    \item While the training loss will typically continue to decrease with more training epochs, the validation loss will decrease initially and then begin to increase as the model starts to overfit.
    \item We **stop the training process** at the point where the validation loss reaches its minimum and begins to worsen. This is done even if the training loss is still on a downward trend.
\end{enumerate}
This "early stopping" acts as a powerful form of regularization, preventing the model from specializing too much to the training data and thereby improving its ability to generalize. The point where validation loss begins to rise while training loss falls marks the onset of overfitting.

\section{Stochastic Gradient Descent (SGD) Dynamics}

\subsection{An Illustrative Example: 1D Linear Regression}
To understand the behavior of SGD, consider a simple 1D linear regression problem. The full-batch loss function $f(x)$ is the average of the losses from $N$ individual data samples:
\begin{equation}
    f(x) = \frac{1}{2N} \sum_{i=1}^{N} (a_i x - b_i)^2
\end{equation}
Each component of the sum, $f_i(x) = \frac{1}{2}(a_i x - b_i)^2$, is a single parabola. The minimum of each individual parabola $f_i(x)$ is located at $x_i^* = b_i / a_i$.

The global loss function $f(x)$ is itself a parabola, being the average of all the individual parabolas. Its global minimum, $x^*$, can be found analytically by setting its gradient to zero: $\grad f(x^*) = 0$. This yields:
\begin{equation}
    x^* = \frac{\sum_{i=1}^{N} a_i b_i}{\sum_{i=1}^{N} a_i^2}
\end{equation}

\subsection{The Dynamics of SGD: Two Zones}
The convergence behavior of SGD can be characterized by two distinct regions of operation.

\begin{enumerate}
    \item \textbf{Far-out Zone:} When the current parameter estimate $x_t$ is far from the global minimum $x^*$, the gradients of most individual loss functions, $\grad f_i(x_t)$, point in roughly the same direction as the true gradient $\grad f(x_t)$. In this zone, the stochastic gradient $g_t$ (computed from a single sample) is a good approximation of the true gradient. As a result, SGD makes fast and consistent progress toward the minimum, much like full-batch Gradient Descent.

    \item \textbf{Region of Confusion:} When $x_t$ is near the global minimum $x^*$, it typically lies *between* the minima of the individual parabolas ($x_i^*$). In this region, the individual gradients can point in opposing directions; some may be positive while others are negative, effectively "fighting" each other. The stochastic gradient $g_t$ now exhibits high variance. An update step based on a single sample might even point away from the true minimum $x^*$.
\end{enumerate}

Due to this high variance near the minimum, SGD with a constant stepsize cannot converge to the exact minimum $x^*$. Instead, it will "bounce around" this region of confusion.

\begin{center}
    % Placeholder for the image. Assumes image.png is in the same directory.
    \includegraphics[draft,width=0.7\textwidth]{image.png} 
    \textit{Visualization of individual losses (dashed parabolas) and the global loss (solid black). The green "Far-out Zone" shows consistent gradient directions, while the red "Region of Confusion" near the global minimum shows conflicting gradients.}
\end{center}

\section{Practical Variants and Strategies for SGD}

\subsection{Mini-Batch SGD}
In practice, neither full-batch GD (using all samples) nor standard SGD (using a single sample) is the most common approach. The preferred method is **Mini-Batch SGD**, which offers a compromise. Instead of one sample, we compute the gradient by averaging over a small, randomly chosen subset of $m$ samples, known as a "mini-batch".

The gradient estimate at iteration $t$ is:
\begin{equation}
    \tilde{\vec{g}}_t := \frac{1}{m} \sum_{j=1}^{m} \grad f_{i_j}(\vec{x}_t)
\end{equation}
where $\{i_j\}$ are $m$ distinct, randomly chosen indices. The update step is then:
\begin{equation}
    \vec{x}_{t+1} := \vec{x}_t - \gamma_t \tilde{\vec{g}}_t
\end{equation}
The batch size $m$ is a hyperparameter:
\begin{itemize}
    \item If $m=1$, this reduces to standard SGD.
    \item If $m=n$ (the total number of samples), this becomes full-batch GD.
\end{itemize}

\subsection{Benefits of Mini-Batching}
Mini-batching provides two significant advantages:
\begin{enumerate}
    \item \textbf{Variance Reduction:} Averaging reduces noise. The variance of the mini-batch gradient estimate is $m$ times smaller than that of the single-sample gradient. This relationship is given by:
    \begin{equation}
        \E[\norm{\tilde{\vec{g}}_t - \grad f(\vec{x}_t)}^2] = \frac{1}{m} \E[\norm{\vec{g}_t^1 - \grad f(\vec{x}_t)}^2]
    \end{equation}
    A lower variance makes the gradient estimate $\tilde{\vec{g}}_t$ more accurate, leading to more stable and reliable convergence.
    
    \item \textbf{Parallel Computation:} The gradients for all $m$ samples in a mini-batch are computed at the same point $\vec{x}_t$. These computations are independent and can be perfectly parallelized. This is the primary reason mini-batching is ubiquitous in deep learning, as it allows for efficient use of massively parallel hardware like GPUs (Graphical Processing Units), which can compute all $m$ gradients simultaneously.
\end{enumerate}

\subsection{Drawbacks and the Role of Noise}
While reducing variance is beneficial, eliminating it entirely by using a very large mini-batch is not always desirable.
\begin{itemize}
    \item The "noise" inherent in small batches (small $m$) acts as a form of regularization. It can help the algorithm escape sharp, non-generalizing local minima and push it toward "flatter" minima, which are empirically found to generalize better.
    \item A \textbf{too-large mini-batch} ($m \rightarrow n$) reduces this beneficial noise. The algorithm's behavior approaches that of full-batch GD, which can get stuck in poor local minima and is more prone to overfitting.
    \item Therefore, finding the "best" mini-batch size $m$ is a critical aspect of \textbf{hyperparameter tuning}.
\end{itemize}

\subsection{Stepsize (Learning Rate) Selection}
The choice of stepsize $\gamma_t$ (often called the learning rate) is arguably the most critical hyperparameter for gradient-based methods.
\begin{itemize}
    \item If the stepsize is \textbf{too large}, the algorithm becomes unstable, "overshoots" the minimum, and may diverge, causing the loss to explode.
    \item If the stepsize is \textbf{too small}, the algorithm will be stable, but convergence will be extremely slow.
\end{itemize}

Common strategies for selecting the stepsize include:
\begin{enumerate}
    \item \textbf{Constant Stepsize:} $\gamma_t = \gamma$. This is simple but will cause the algorithm to "bounce around" the minimum without ever fully converging. It requires $\gamma$ to be small enough to prevent divergence.
    \item \textbf{Decreasing Stepsize:} Ensure that $\gamma_t \rightarrow 0$ as $t \rightarrow \infty$. Common choices include $\gamma_t \propto \frac{1}{t}$ or $\gamma_t \propto \frac{1}{\sqrt{t}}$. This guarantees convergence to the minimum but can be slow to tune and may decay too quickly.
    \item \textbf{Annealing / Schedulers (Practice):} This is the most common and effective strategy in modern deep learning. One starts with a relatively large, constant $\gamma$ to make fast progress in the "far-out zone." Then, the learning rate is periodically "decayed" (decreased), for example, by dividing it by 10 every 20 epochs. This allows for rapid initial convergence followed by finer adjustments as the algorithm approaches the minimum.
\end{enumerate}

\newpage
\chapter{Lesson 10-11}

\section*{First-Order Optimization Methods}

\section{Challenges of Mini-Batch Stochastic Gradient Descent (SGD)}

Stochastic Gradient Descent (SGD), even when implemented with mini-batches, does not guarantee good convergence and presents several significant challenges in practice. These challenges necessitate the development of more sophisticated optimization algorithms.

\subsection{Choosing a Proper Learning Rate ($\gamma$)}
The learning rate is one of the most critical hyperparameters. Its value dictates the size of the steps taken towards the minimum of the loss function. The choice of $\gamma$ involves a crucial trade-off:
\begin{itemize}
    \item \textbf{Too small:} A very small learning rate leads to painfully slow convergence. The optimizer takes tiny steps, requiring a large number of iterations to reach the minimum.
    \item \textbf{Too large:} A large learning rate can cause the optimizer to overshoot the minimum, leading to oscillations around it. In the worst-case scenario, the updates can cause the loss to increase, leading to divergence of the algorithm.
\end{itemize}

\subsection{Learning Rate Schedules}
A common technique to balance the speed-stability trade-off is to use a \textit{learning rate schedule}, where the learning rate is decreased over time (e.g., annealing). However, this approach has its own drawbacks:
\begin{itemize}
    \item The schedule (e.g., how fast to decrease $\gamma$) must be pre-defined and tuned in advance.
    \item These schedules are "blind" as they are unable to adapt to the specific characteristics of the dataset or the dynamics of the training process.
\end{itemize}

\subsection{Same Learning Rate for All Parameters}
Applying the same learning rate to all parameters can be problematic, especially when dealing with sparse data. In such datasets, some features appear very infrequently, while others are common.
\begin{itemize}
    \item For infrequent features, we want to perform larger updates to their corresponding parameters, as we have fewer opportunities to learn them.
    \item For frequent features, smaller updates are preferable to avoid instability.
\end{itemize}
A single learning rate fails to account for these differing update frequencies.

\subsection{Escaping Suboptimal Points}
The error functions associated with deep neural networks are typically high-dimensional and non-convex. This landscape is populated with numerous suboptimal local minima. While local minima are a concern, a more significant difficulty arises from \textbf{saddle points}.
\begin{itemize}
    \item A saddle point is a location where the gradient is zero, but it is not a minimum. In a simple 2D case, it's a point where the surface curves up in one dimension and down in another.
    \item At or near a saddle point, the gradient is close to zero in all dimensions. This makes it extremely difficult for standard SGD to "escape," as the update steps become infinitesimally small, effectively halting the learning process.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.7\textwidth]{https://i.imgur.com/83p1y3c.png}
    \caption{Visualization of Gradient Descent getting stuck at a saddle point. The path (blue line) slows down significantly as it approaches the flat region of the saddle point where the gradient is near zero.}
    \label{fig:saddle_point}
\end{figure}

\section{Momentum}

To address the challenges of vanilla SGD, particularly the slow convergence in ravines and at saddle points, the concept of momentum was introduced.

\subsection{Idea}
The core idea behind momentum is to accelerate SGD in the relevant direction and dampen oscillations. It simulates the physical concept of momentum, where a ball rolling down a hill accumulates velocity. This is especially useful for navigating surfaces that are much steeper in one dimension than another.

It achieves this by adding a fraction ($\mu$) of the update vector from the previous time step to the current update vector.

\subsection{Update Rules}
The update rules for SGD with momentum are as follows:
\begin{align}
    \mathbf{v}_t &= \mu \mathbf{v}_{t-1} + \gamma \nabla_{\mathbf{w}} J(\mathbf{w}) \label{eq:momentum_v} \\
    \mathbf{w} &= \mathbf{w} - \mathbf{v}_t \label{eq:momentum_w}
\end{align}
Where:
\begin{itemize}
    \item $\mathbf{w}$: parameter vector.
    \item $\gamma$: learning rate.
    \item $\mu$: momentum term (e.g., a value like 0.9 is common). This determines the contribution of the previous update vector.
    \item $\mathbf{v}_t$: update vector at time step $t$, often referred to as "velocity."
\end{itemize}

\subsection{Advantages \& Drawbacks}
\begin{itemize}
    \item[+] Gains faster convergence in many scenarios.
    \item[+] Reduces oscillations by averaging gradients over time.
    \item[--] The "ball" is blind. It can accumulate too much momentum and roll past the minimum or be led off-track if the slope of the loss surface changes abruptly.
\end{itemize}

\section{Nesterov Accelerated Gradient (NAG)}
Nesterov Accelerated Gradient (NAG) is a refinement of the momentum method that introduces a "lookahead" step, creating a "smarter ball" that can anticipate changes in the gradient.

\subsection{Idea}
NAG gives the momentum term a notion of where it is going. Instead of calculating the gradient at the current position $\mathbf{w}$, it first makes a "lookahead" step in the direction of the previous momentum and then calculates the gradient at this approximate future position. This allows the algorithm to slow down before the hill slopes up again, preventing overshooting.

The gradient is calculated at the approximate future position $\left(\mathbf{w} - \mu \mathbf{v}_{t-1}\right)$.

\subsection{Update Rules}
The update rules are slightly modified from standard momentum:
\begin{align}
    \mathbf{v}_t &= \mu \mathbf{v}_{t-1} + \gamma \nabla_{\mathbf{w}} J(\mathbf{w} - \mu \mathbf{v}_{t-1}) \\
    \mathbf{w} &= \mathbf{w} - \mathbf{v}_t
\end{align}
We calculate the gradient not with respect to the current parameters $\mathbf{w}$, but with respect to the "looked-ahead" position.

\subsection{NAG Visualization}
The difference between standard momentum and NAG can be visualized through vector addition.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.9\textwidth]{https://i.imgur.com/kS9T0aH.png}
    \caption{Comparison of Momentum update (left) and Nesterov Momentum update (right). In NAG, the gradient step is computed after the momentum step, providing a more accurate correction.}
    \label{fig:nag_viz}
\end{figure}

\begin{itemize}
    \item \textbf{Momentum Update:} First, it computes the gradient at the current position (red vector) and then adds it to the scaled previous momentum step (green vector) to get the final update (blue vector).
    \item \textbf{Nesterov Momentum Update:} First, it takes the momentum step (green vector). Then, it computes the gradient from this "lookahead" position (red vector) and adds it to form the final update (blue vector). This acts as a correction factor.
\end{itemize}

\section{Adagrad}
Adagrad is an adaptive learning rate algorithm that addresses the issue of using a single learning rate for all parameters.

\subsection{Idea}
Adagrad adapts the learning rate to the parameters on a per-parameter basis. The key idea is to perform:
\begin{itemize}
    \item \textbf{Larger updates} for infrequent parameters.
    \item \textbf{Smaller updates} for frequent parameters.
\end{itemize}
This makes Adagrad very well-suited for sparse data.

\subsection{Update Rules}
Let $\mathbf{g}_{t,i} = \nabla_{\mathbf{w}} J(w_{t,i})$ be the gradient of the objective function with respect to parameter $i$ at step $t$. The per-parameter update is:
\begin{equation}
    w_{t+1, i} = w_{t, i} - \frac{\gamma}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}
\end{equation}
Where:
\begin{itemize}
    \item $\mathbf{G}_t$: A diagonal matrix where each diagonal element $G_{t,ii}$ is the sum of the squares of the gradients with respect to parameter $w_i$ up to time step $t$.
    \item $\epsilon$: A small smoothing term (e.g., $10^{-8}$) to prevent division by zero.
\end{itemize}
In vectorized form, the update is:
\begin{equation}
    \mathbf{w}_{t+1} = \mathbf{w}_t - \frac{\gamma}{\sqrt{\mathbf{G}_t + \epsilon}} \odot \mathbf{g}_t
\end{equation}
where $\odot$ denotes the element-wise (Hadamard) product.

\subsection{Advantages \& Drawbacks}
\textbf{Advantages:}
\begin{itemize}
    \item The learning rate is adapted for each individual parameter.
    \item It is well-suited for sparse data.
    \item It largely eliminates the need to manually tune the learning rate. Most implementations use a default $\gamma = 0.01$.
\end{itemize}
\textbf{Main Drawback:}
\begin{itemize}
    \item \textbf{Accumulation of squared gradients in the denominator.} Since every added term is positive, the sum in $\mathbf{G}_t$ keeps growing during training. This causes the effective learning rate to shrink and eventually become infinitesimally small, at which point the algorithm effectively stops learning.
\end{itemize}

\section{Visualizing Optimizer Performance}

To understand the practical implications of Adagrad's drawback, we can visualize its behavior on a cost surface.

\begin{figure}[h!]
    \centering
    \fbox{\parbox[c][10cm][c]{12cm}{\centering \Huge Visualization of Optimization Paths \\ \large (Adagrad, Momentum, etc. on a 3D cost surface)}}
    \caption{Comparison of optimization paths. Adagrad (green) takes very small steps and gets stuck prematurely due to its rapidly decaying learning rate. In contrast, methods like Momentum can navigate the surface more effectively.}
\end{figure}

In demonstrations, even with a larger initial learning rate (e.g., $10^{-2}$ instead of $10^{-3}$), Adagrad's path is short. It quickly converges to a suboptimal point and ceases to make further progress. This behavior contrasts sharply with other optimizers like SGD with Momentum, which can maintain momentum to traverse flat regions and converge more effectively, especially on complex surfaces like those with saddle points or ravines.

\section{Adadelta: Resolving the Decaying Learning Rate}
Adadelta is an extension of Adagrad that seeks to resolve its aggressive, monotonically decreasing learning rate.

\subsection{Core Idea}
Instead of accumulating \textit{all} past squared gradients, Adadelta restricts the window of accumulated past gradients to a fixed size by using a \textbf{decaying average} of past squared gradients. This prevents the denominator from growing indefinitely and allows the learning to continue.

\subsection{Update Rules}
The algorithm maintains a running average of squared gradients, using a decay term $\mu$.
\begin{equation}
    E[g^2]_t = \mu E[g^2]_{t-1} + (1-\mu)g_t^2
\end{equation}
The parameter update is then scaled using the Root Mean Square (RMS) of past updates and past gradients, effectively eliminating the need for a global learning rate:
\begin{align}
    \Delta w_t &= - \frac{\text{RMS}[\Delta w]_{t-1}}{\text{RMS}[g]_t} g_t \\
    w_{t+1} &= w_t + \Delta w_t
\end{align}
where the RMS is defined as:
\begin{equation}
    \text{RMS}[x]_t = \sqrt{E[x^2]_t + \epsilon}
\end{equation}
The term $\epsilon$ is a small constant for numerical stability. A key feature here is that the update rule units match, which helps stabilize the process.

\subsection{Advantages of Adadelta}
\begin{itemize}
    \item It solves Adagrad's decaying learning rate problem.
    \item \textbf{No learning rate needs to be set.} The learning rate has been eliminated from the update rule, removing one of the most critical hyperparameters to tune.
\end{itemize}

\section{RMSprop: An Independent Solution}
RMSprop is another adaptive learning rate method developed independently by Geoff Hinton around the same time as Adadelta. It shares a similar motivation: to resolve Adagrad's diminishing learning rates.

\subsection{Core Idea}
RMSprop also divides the learning rate by an exponentially decaying average of squared gradients. Its update rule is nearly identical to the first part of Adadelta's derivation but retains an explicit learning rate $\gamma$.

\subsection{Update Rules}
The running average of squared gradients is computed as follows:
\begin{equation}
    E[g^2]_t = \mu E[g^2]_{t-1} + (1-\mu)g_t^2
\end{equation}
The final parameter update keeps the learning rate $\gamma$:
\begin{equation}
    w_{t+1} = w_t - \frac{\gamma}{\sqrt{E[g^2]_t + \epsilon}} g_t
\end{equation}
Typical default values are $\mu = 0.9$ (decay rate) and $\gamma = 0.001$ (learning rate).

\subsection{Advantages \& Drawbacks}
\begin{itemize}
    \item[+] Fixes Adagrad's vanishing learning rate.
    \item[+] Empirically works very well in practice.
    \item[-] Still requires a learning rate $\gamma$ to be chosen, unlike Adadelta.
\end{itemize}

\section{Adam: Adaptive Moment Estimation}
Adam is currently one of the most popular and effective optimization algorithms. It combines the ideas of both Momentum and RMSprop.

\subsection{Core Idea}
Adam computes adaptive learning rates for each parameter. It keeps an exponentially decaying average of past gradients (like Momentum, a first moment estimate) and an exponentially decaying average of past squared gradients (like RMSprop, a second moment estimate).

\subsection{Update Rules}
1. \textbf{Update biased moment estimates:}
\begin{align}
    \mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1)\mathbf{g}_t \quad \text{(1st moment, mean)} \\
    \mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1-\beta_2)\mathbf{g}_t^2 \quad \text{(2nd moment, variance)}
\end{align}
2. \textbf{Compute bias-corrected estimates:} To counteract the fact that these estimates are initialized at zero and are biased towards zero, especially during the initial steps of training, Adam computes corrected estimates:
\begin{align}
    \hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
    \hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1 - \beta_2^t}
\end{align}
3. \textbf{Final update:}
\begin{equation}
    w_{t+1} = w_t - \frac{\gamma}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} \hat{\mathbf{m}}_t
\end{equation}

\subsection{Advantages \& Drawbacks}
\begin{itemize}
    \item[+] \textbf{Advantages:}
    \begin{itemize}
        \item Combines the benefits of Adagrad/RMSprop (handles sparse gradients) and Momentum (accelerates convergence).
        \item Bias-correction helps counteract the initial bias towards zero.
        \item Works very well in practice and is often the default choice for deep learning applications.
        \item Default values (e.g., $\gamma = 0.001, \beta_1 = 0.9, \beta_2 = 0.999$) are often effective.
    \end{itemize}
    \item[-] \textbf{Drawbacks:}
    \begin{itemize}
        \item More complex, as it stores two moving averages per parameter ($\mathbf{m}_t, \mathbf{v}_t$), increasing memory requirements.
        \item Some studies note it can occasionally fail to converge in settings where SGD with Momentum does, though this point is still debated.
    \end{itemize}
\end{itemize}

\section{Which Optimizer to Use? A Summary}

\begin{itemize}
    \item \textbf{If your input data is sparse:} Use one of the adaptive learning-rate methods (Adagrad, Adadelta, RMSprop, Adam). These methods are designed to handle sparse gradients effectively, and you will likely not need to tune the learning rate extensively.
    
    \item \textbf{Overall Recommendation:} \textbf{Adam} is often the best overall choice. It combines the strengths of other adaptive methods and generally performs well across a wide range of problems.
    
    \item \textbf{Warning:} Many recent papers still use \textbf{SGD (with momentum)} and a simple learning rate annealing schedule. SGD \textit{can} find a good minimum, but it often takes longer, is more reliant on good initialization and learning rate scheduling, and may get stuck in saddle points more easily than adaptive methods.
    
    \item \textbf{Final Verdict:} If you care about \textbf{fast convergence} and are training a deep or complex network, choose an \textbf{adaptive learning rate method} like Adam.
\end{itemize}

\section*{Second-Order Optimization Methods}

\section{Newton's Method: A Review}

We begin by revisiting Newton's method, a powerful second-order optimization algorithm. To build intuition, we first consider its application in the one-dimensional problem of root finding before extending it to minimization in n-dimensions.

\subsection{The Problem: Root Finding}
Given a differentiable function $f(x)$, our objective is to find a value $x^*$ such that:
\begin{equation}
f(x^*) = 0
\end{equation}
This value $x^*$ is referred to as a "root" or "zero" of the function. This is a fundamental problem that appears in many contexts, such as finding $\sqrt{2}$, which is equivalent to finding the root of $f(x) = x^2 - 2$.

\subsection{The Core Idea: Tangent Lines and Derivation}
The strategy of Newton's method is to iteratively improve an initial guess, $x_0$, by using local linear approximations of the function. The core idea relies on the fact that the tangent line at a point is a good local approximation of the function. The root of the tangent line should therefore be close to the root of the function itself.

The derivation proceeds as follows:
\begin{enumerate}
    \item \textbf{Start with an initial guess, $x_0$.}
    \item \textbf{Approximate the function $f(x)$ at $x_k$ with its tangent line.} The equation for the tangent line at a point $(x_k, f(x_k))$ is given by the first-order Taylor expansion:
    \begin{equation}
    y = f(x_k) + f'(x_k)(x - x_k)
    \end{equation}
    \item \textbf{Find the root of this tangent line.} This root will become our new, improved guess, $x_{k+1}$. We find it by setting $y=0$ and $x = x_{k+1}$:
    \begin{equation}
    0 = f(x_k) + f'(x_k)(x_{k+1} - x_k)
    \end{equation}
    \item \textbf{Solve for $x_{k+1}$}, assuming $f'(x_k) \neq 0$:
    \begin{align*}
    f'(x_k)(x_{k+1} - x_k) &= -f(x_k) \\
    x_{k+1} - x_k &= -\frac{f(x_k)}{f'(x_k)}
    \end{align*}
    This gives us the celebrated Newton's method recurrence relation:
    \begin{equation}
    \boxed{x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}}
    \end{equation}
    \item \textbf{Repeat} the process from $x_1$ to get $x_2$, and so on, until convergence.
\end{enumerate}

\subsection{Visualizing the Method}
The iterative process can be visualized as "riding the tangent" down to the root. Starting at $x_0$, we find the tangent, calculate its intersection with the x-axis to get $x_1$, and repeat. As shown in Figure \ref{fig:newton_root}, the method can converge very quickly to the true root.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.6\textwidth]{visualizing_method_root_finding.png}
    \caption{Geometric interpretation of Newton's method for root finding. Each new guess ($x_1, x_2$) is the root of the tangent line from the previous point.}
    \label{fig:newton_root}
\end{figure}

\section{From Root Finding to Optimization}
The connection between root finding and optimization is direct and fundamental.

\subsection{First-Order Optimality Condition}
A necessary condition for a point $x^*$ to be a local minimum of a smooth function $f(x)$ is that its gradient (or derivative in 1D) is zero:
\begin{equation}
f'(x^*) = 0
\end{equation}
\textbf{Connection:} Finding a minimum of $f(x)$ is the \textbf{same problem} as finding a root of its derivative, $f'(x)$.

This allows us to apply Newton's method to a new function, $g(x) = f'(x)$, to solve the minimization problem.

\subsection{Derivation for Minimization}
To derive Newton's method for optimization, we follow these steps:
\begin{enumerate}
    \item \textbf{Goal:} Find $x$ such that $g(x) = 0$, where we define $g(x) = f'(x)$.
    \item \textbf{Apply Newton's formula to $g(x)$:} The standard root-finding update rule for $g(x)$ is:
    \begin{equation}
    x_{k+1} = x_k - \frac{g(x_k)}{g'(x_k)}
    \end{equation}
    \item \textbf{Substitute back:} Since $g(x) = f'(x)$ and $g'(x) = f''(x)$, we substitute these back into the formula to obtain the recurrence relation for optimization:
    \begin{equation}
    \boxed{x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}}
    \end{equation}
\end{enumerate}

\subsection{Intuition: Quadratic Approximation}
The geometric intuition for minimization is slightly different from root finding. By using the second-order Taylor expansion around $x_k$, we approximate the function $f(x)$ with a quadratic function (a parabola):
\begin{equation}
q(x) \approx f(x_k) + f'(x_k)(x - x_k) + \frac{1}{2}f''(x_k)(x - x_k)^2
\end{equation}
This is equivalent to fitting a parabola to the function $f(x)$ at the point $x_k$. Newton's method then simply jumps to the exact minimum of this quadratic model.

Finding the minimum of this parabola $q(x)$ means finding where its derivative is zero:
\begin{equation}
q'(x) = f'(x_k) + f''(x_k)(x - x_k) = 0
\end{equation}
Setting $q'(x_{k+1}) = 0$ and solving for $x_{k+1}$ gives the exact same formula as before.

\section{Newton's Method for n-Dimensional Optimization}
The intuition from the 1D case holds perfectly for the n-dimensional problem. We simply generalize our derivative concepts.

\subsection{Generalizing to n-Dimensions}
\begin{itemize}
    \item The \textbf{1D Derivative $f'(x)$} becomes the \textbf{Gradient $\nabla f(\mathbf{x})$} (a vector).
    \begin{equation}
    \nabla f(\mathbf{x}) = \begin{pmatrix} \partial f / \partial x_1 \\ \vdots \\ \partial f / \partial x_n \end{pmatrix}
    \end{equation}
    \item The \textbf{2nd Derivative $f''(x)$} becomes the \textbf{Hessian $D^2 f(\mathbf{x})$} (an $n \times n$ matrix).
    \begin{equation}
    D^2 f(\mathbf{x}) = \begin{pmatrix}
    \frac{\partial^2 f}{\partial x_1^2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial^2 f}{\partial x_n \partial x_1} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
    \end{pmatrix}
    \end{equation}
\end{itemize}

The optimization recurrence relation for the n-D case is then a direct analogy of the 1D case, replacing division with matrix inversion:
\begin{equation}
\boxed{\mathbf{x}_{k+1} = \mathbf{x}_k - [D^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)}
\end{equation}

\subsection{In Practice: Solving a Linear System}
A critical point in the practical implementation is that we \textbf{do not compute the inverse} of the Hessian matrix, $[D^2 f(\mathbf{x}_k)]^{-1}$. Matrix inversion is numerically unstable and computationally expensive. Instead, we solve the equivalent linear system for the step direction $\Delta \mathbf{x}_k$:
\begin{equation}
[D^2 f(\mathbf{x}_k)] \Delta \mathbf{x}_k = - \nabla f(\mathbf{x}_k)
\end{equation}
Then, the update is performed as:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \Delta \mathbf{x}_k
\end{equation}
This approach is numerically more stable and efficient.

\section{Challenges and Caveats for n-Dimensions}
While powerful, Newton's method introduces significant computational challenges in higher dimensions.

\begin{itemize}
    \item \textbf{Cost of Derivatives:}
    \begin{itemize}
        \item The gradient $\nabla f(\mathbf{x})$ has $n$ components to compute.
        \item The Hessian $D^2 f(\mathbf{x})$ is an $n \times n$ matrix with $O(n^2)$ components to compute. This can be very expensive for high-dimensional problems.
    \end{itemize}
    
    \item \textbf{Cost of the Step (The Bottleneck):}
    \begin{itemize}
        \item We must solve an $n \times n$ linear system at each iteration.
        \item Using standard methods like LU decomposition, this costs $O(n^3)$ operations, which is infeasible for large $n$.
    \end{itemize}
    
    \item \textbf{Hessian Properties:}
    \begin{itemize}
        \item The Hessian matrix must be \textbf{invertible}. If it is singular, the method fails.
        \item For minimization, the Hessian $D^2 f(\mathbf{x}_k)$ must be \textbf{positive definite}. If it is not, the step may point towards a maximum or a saddle point, rather than a minimum.
    \end{itemize}
\end{itemize}

\section{Summary: 1D vs n-D Minimization}
The analogy between the 1D and n-D versions of Newton's method provides a clear framework for understanding the generalization and its associated costs. The key concepts are summarized below.

\begin{table}[h!]
\centering
\begin{tabular}{l|l|l}
\hline
\textbf{Concept} & \textbf{1D Version} & \textbf{n-D Generalization} \\
\hline \hline
Problem & $\min_x f(x)$ & $\min_{\mathbf{x}} f(\mathbf{x})$ \\
Condition & $f'(x) = 0$ & $\nabla f(\mathbf{x}) = \mathbf{0}$ \\
Model & Parabola & Paraboloid \\
2nd Derivative & $f''(x)$ (scalar) & $\mathbf{D}^2 f(\mathbf{x})$ (matrix) \\
Update Step & $x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}$ & $\mathbf{x}_{k+1} = \mathbf{x}_k - [\mathbf{D}^2 f(\mathbf{x}_k)]^{-1} \nabla f(\mathbf{x}_k)$ \\
\textbf{Cost of Step} & $\boldsymbol{O(1)}$ (division) & $\boldsymbol{O(n^3)}$ (solve system) \\
\hline
\end{tabular}
\caption{Comparison of Newton's method for minimization in 1D and n-D.}
\end{table}

The most critical distinction lies in the last row: the cost of the update step transitions from a constant-time operation (a single division) in 1D to a cubic-time operation in n-D.

\section{Next Steps: Quasi-Newton Methods}
The prohibitive $O(n^3)$ computational cost and the requirement to compute the full Hessian matrix motivate the development of more practical, scalable algorithms. This leads to the family of \textbf{Quasi-Newton Methods}.

The core idea is to avoid the explicit computation and inversion of the true Hessian matrix. Instead, these methods build an \textit{approximation} of the Hessian (or its inverse) iteratively, using only gradient information from previous steps.

One of the most famous and widely used Quasi-Newton algorithms is \textbf{BFGS} (BroydenFletcherGoldfarbShanno). These methods are designed to:
\begin{enumerate}
    \item Reduce the per-iteration cost from $O(n^3)$ to something more manageable, often $O(n^2)$.
    \item Avoid the explicit computation of second derivatives.
    \item Maintain a positive definite approximation to the Hessian, ensuring that each step is a descent direction for minimization.
\end{enumerate}

It is important to note that approximating the Hessian via finite differences of the gradient is generally not a viable approach. This numerical differentiation is prone to floating-point errors and instability, similar to the issues encountered with numerical automatic differentiation. Quasi-Newton methods employ more sophisticated and numerically stable update rules to construct their Hessian approximations.

\newpage
\chapter{Lesson 17-11}

\section{Introduction to Quasi-Newton Methods}
In numerical optimization, there exists a fundamental trade-off between the computational cost per iteration and the rate of convergence. Quasi-Newton methods aim to strike a balance between two well-known extremes: Steepest Descent and the Newton-Raphson method.

\subsection{Motivation: The Optimization Trade-off}
\begin{itemize}
    \item \textbf{Steepest Descent:} This first-order method is computationally inexpensive, with a cost of $O(n)$ per iteration. However, its convergence is slow (linear), often exhibiting a characteristic "zig-zagging" behavior in narrow valleys of the objective function's landscape.
    \item \textbf{Newton-Raphson Method:} This second-order method boasts a very fast (quadratic) rate of convergence near the optimum. This speed comes at a high computational cost of $O(n^3)$ per iteration, primarily due to the need to compute, store, and invert the full Hessian matrix.
\end{itemize}

\textbf{The Goal of Quasi-Newton Methods:} The primary aim is to develop optimization algorithms that converge faster than Steepest Descent but have a significantly lower computational cost per iteration than the full Newton-Raphson method.

\subsection{The Newton-Raphson Method Revisited}
The Newton-Raphson method can be understood as iteratively minimizing a quadratic model of the objective function $f(\mathbf{x})$. At a given iterate $\mathbf{x}_k$, we approximate $f(\mathbf{x})$ using its second-order Taylor expansion:
\begin{equation}
q(\mathbf{x}) = f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T (\mathbf{x} - \mathbf{x}_k) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_k)^T D^2 f(\mathbf{x}_k) (\mathbf{x} - \mathbf{x}_k)
\end{equation}
where $\nabla f(\mathbf{x}_k)$ is the gradient and $D^2 f(\mathbf{x}_k)$ is the Hessian matrix evaluated at $\mathbf{x}_k$.

If the Hessian $D^2 f(\mathbf{x}_k)$ is positive definite, the quadratic model $q(\mathbf{x})$ has a unique minimizer, which defines the next iterate:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - (D^2 f(\mathbf{x}_k))^{-1} \nabla f(\mathbf{x}_k)
\end{equation}
This is the Newton-Raphson update rule.

\section{The Quasi-Newton Approach}
The core idea behind Quasi-Newton methods is to replace the true (and computationally expensive) Hessian matrix $D^2 f(\mathbf{x}_k)$ with a simpler, positive definite approximation, which we denote as $B_k$.

\subsection{The Quasi-Newton Quadratic Model}
By substituting $B_k$ for the true Hessian, we construct a new quadratic model:
\begin{equation}
p(\mathbf{x}) = f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^T (\mathbf{x} - \mathbf{x}_k) + \frac{1}{2} (\mathbf{x} - \mathbf{x}_k)^T B_k (\mathbf{x} - \mathbf{x}_k)
\end{equation}
Assuming $B_k$ is positive definite, the minimizer of this model, $\mathbf{x}^*$, is given by:
\begin{equation}
\mathbf{x}^* = \mathbf{x}_k - B_k^{-1} \nabla f(\mathbf{x}_k)
\end{equation}

\subsection{From Full Update to Search Direction}
Since $B_k$ is only an approximation of the Hessian, the step $\mathbf{x}^* - \mathbf{x}_k$ is not treated as a full update to the next iterate. Instead, it is used to define a \textbf{search direction}, $\mathbf{d}_k$:
\begin{equation}
\mathbf{d}_k = -B_k^{-1} \nabla f(\mathbf{x}_k)
\end{equation}
The next iterate is then found by performing a line search along this direction to find a suitable step length $\alpha_k$:
\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{d}_k
\end{equation}
This framework leaves a key question open: \textbf{How do we compute the next approximation $B_{k+1}$ cheaply and effectively?}

\subsection{A Wish List for the Approximation $B_k$}
To guide the design of our Hessian approximation, we establish a "wish list" of desirable properties.
\begin{description}
    \item[P1: Nonsingular] $B_k$ must be nonsingular so that its inverse exists and the search direction $\mathbf{d}_k$ is well-defined.
    \item[P2: Descent Direction] $B_k$ must produce a descent direction, meaning that moving along $\mathbf{d}_k$ locally decreases the function value. This is equivalent to requiring $\nabla f(\mathbf{x}_k)^T \mathbf{d}_k < 0$.
    \item[P3: Symmetry] $B_k$ should be symmetric, as the true Hessian matrix is symmetric.
    \item[P4: Cheap to Compute] $B_{k+1}$ should be computable cheaply, ideally by "recycling" information already available from the current and previous steps.
    \item[P5: Closeness] The update from $B_k$ to $B_{k+1}$ should be "small" or "close," reflecting the idea that we are updating our information rather than discarding it. This is often achieved via a low-rank update.
    \item[P6: Low Computational Cost] The total work per iteration should be at most $O(n^2)$, providing a substantial speed-up over the $O(n^3)$ cost of Newton-Raphson.
\end{description}

The first three properties can be satisfied by a single, powerful requirement: \textbf{$B_k$ must be symmetric positive definite (SPD)}.
\begin{itemize}
    \item If $B_k$ is SPD, it is automatically nonsingular (satisfying P1) and symmetric (satisfying P3).
    \item Furthermore, P2 is also satisfied. Since $B_k$ is SPD, its inverse $B_k^{-1}$ is also SPD. Therefore, for any non-zero gradient $\nabla f(\mathbf{x}_k) \neq \mathbf{0}$:
    \begin{equation}
    \nabla f(\mathbf{x}_k)^T \mathbf{d}_k = \nabla f(\mathbf{x}_k)^T (-B_k^{-1} \nabla f(\mathbf{x}_k)) = -\nabla f(\mathbf{x}_k)^T B_k^{-1} \nabla f(\mathbf{x}_k) < 0
    \end{equation}
    This confirms that $\mathbf{d}_k$ is a valid descent direction.
\end{itemize}

\subsection{The Secant Condition}
To satisfy property P4 (cheap computation), we must find a way to update $B_k$ without re-computing a full Hessian. The key insight comes from relating the change in gradient to the change in position. We define two key vectors:
\begin{align}
\bm{\delta}_k &= \mathbf{x}_{k+1} - \mathbf{x}_k \quad (\text{change in position}) \\
\bm{\gamma}_k &= \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k) \quad (\text{change in gradient})
\end{align}
From a first-order Taylor expansion of the gradient $\nabla f$, we have:
\begin{equation}
\nabla f(\mathbf{x}_{k+1}) \approx \nabla f(\mathbf{x}_k) + D^2f(\mathbf{x}_k)(\mathbf{x}_{k+1} - \mathbf{x}_k)
\end{equation}
Rearranging this gives an approximation involving our key vectors:
\begin{equation}
\bm{\gamma}_k \approx D^2f(\mathbf{x}_k) \bm{\delta}_k
\end{equation}
We enforce this relationship on our new Hessian approximation $B_{k+1}$, turning the approximation into an equality. This yields the \textbf{Secant Condition}, the most important equation in quasi-Newton methods:
\begin{equation}
B_{k+1} \bm{\delta}_k = \bm{\gamma}_k
\end{equation}
This condition forces our new approximation $B_{k+1}$ to correctly predict the change in the gradient over the step we just took.

\subsubsection{Geometric Interpretation in 1D}
To build intuition, we can translate the Secant Condition to a 1D setting. Here, the vectors become scalars, the gradient becomes the first derivative $f'(x)$, and the Hessian matrix becomes a scalar approximation $b_{k+1}$. The Secant Condition simplifies to:
\begin{equation}
b_{k+1} \cdot (x_{k+1} - x_k) = f'(x_{k+1}) - f'(x_k)
\end{equation}
Solving for our scalar Hessian approximation $b_{k+1}$, we get:
\begin{equation}
b_{k+1} = \frac{f'(x_{k+1}) - f'(x_k)}{x_{k+1} - x_k}
\end{equation}
This has a clear geometric meaning. The expression is the definition of a slope, $\frac{\Delta y}{\Delta x}$. However, the "y" values are not taken from the function $f(x)$ itself, but from its \textbf{first derivative}, $f'(x)$. Therefore, the Secant Condition forces our approximation of the second derivative, $b_{k+1}$, to be the slope of the secant line connecting the points $(x_k, f'(x_k))$ and $(x_{k+1}, f'(x_{k+1}))$ on the graph of the first derivative. This is a finite difference approximation of the second derivative, $f''(x)$.

\section{Deriving and Analyzing the Symmetric Rank-1 (SR1) Formula}

Continuing our exploration of quasi-Newton methods, we derive the update formula for the Symmetric Rank-1 (SR1) method. By substituting the expression for the update vector $\mathbf{u}$ back into the general rank-1 update form, we arrive at the final expression.

\subsection{The SR1 Update Formula}
The update equation for the Hessian approximation $\mathbf{B}_k$ is given by:
\begin{equation}
\mathbf{B}_{k+1} = \mathbf{B}_k + \frac{(\bm{\gamma}_k - \mathbf{B}_k \bm{\delta}_k)(\bm{\gamma}_k - \mathbf{B}_k \bm{\delta}_k)^T}{(\bm{\gamma}_k - \mathbf{B}_k \bm{\delta}_k)^T \bm{\delta}_k}
\end{equation}
This is known as the Symmetric Rank-1 (SR1) method. It is a simple and intuitive update that satisfies the secant condition.

\subsection{Drawbacks of the SR1 Method}
Despite its simplicity, the SR1 method has serious practical drawbacks that can hinder its performance and reliability.

\begin{itemize}
    \item \textbf{Positive Definiteness is not guaranteed.} The updated Hessian approximation, $\mathbf{B}_{k+1}$, may not be positive definite even if the previous approximation, $\mathbf{B}_k$, is. This is a critical failure because if $\mathbf{B}_{k+1}$ is not positive definite, the search direction $\mathbf{d}_k$ may not be a descent direction, causing Property 2 (P2) of a desirable optimization algorithm to fail.

    \item \textbf{Numerical Instability.} The denominator in the update formula, $(\bm{\gamma}_k - \mathbf{B}_k \bm{\delta}_k)^T \bm{\delta}_k$, can be zero or numerically very small (i.e., $\approx 0$). If this occurs, the update becomes undefined or numerically unstable, leading to unreliable steps.
\end{itemize}

\subsection{The Complexity Problem (P6)}
We aim for an algorithm with a computational cost of $O(n^2)$ per iteration (Property P6). Let's analyze the complexity of the SR1 method as derived so far.
\begin{itemize}
    \item \textbf{Computing $\mathbf{B}_{k+1}$:} This step involves vector operations (additions, matrix-vector products) and an outer product. These operations have a complexity of $O(n^2)$, which aligns with our goal. This is good.

    \item \textbf{Computing $\mathbf{d}_k$:} To find the next search direction, we must solve the linear system $\mathbf{B}_k \mathbf{d}_k = -\nabla f(\mathbf{x}_k)$. Solving a general linear system of this form requires $O(n^3)$ operations. This is bad.
\end{itemize}
The $O(n^3)$ cost of solving the linear system dominates the $O(n^2)$ cost of the update. Therefore, the total iteration cost remains $O(n^3)$, which is the same as the standard Newton-Raphson method, offering no computational advantage in this form.

\section{Improving SR1 with the Sherman-Morrison-Woodbury Formula}
To address the $O(n^3)$ complexity bottleneck, we can reformulate the problem. Instead of updating the Hessian approximation $\mathbf{B}_k$ and then solving a linear system, we can directly update its inverse, denoted as $\mathbf{H}_k = \mathbf{B}_k^{-1}$.

\subsection{The Sherman-Morrison-Woodbury Theorem}
The Sherman-Morrison-Woodbury (SMW) formula provides a way to compute the inverse of a matrix that has been updated by a low-rank matrix.
\begin{theorem}[Sherman-Morrison-Woodbury]
If $\mathbf{B}$ is an $n \times n$ matrix and $\mathbf{U}, \mathbf{V}$ are $n \times p$ matrices, then:
\begin{equation}
(\mathbf{B} + \mathbf{U}\mathbf{V}^T)^{-1} = \mathbf{B}^{-1} - \mathbf{B}^{-1}\mathbf{U}(\mathbf{I}_p + \mathbf{V}^T \mathbf{B}^{-1} \mathbf{U})^{-1}\mathbf{V}^T \mathbf{B}^{-1}
\end{equation}
\end{theorem}
This theorem is the key to efficiently updating the inverse Hessian.

\subsection{SR1 Inverse Update Formula}
We can apply the SMW formula to the SR1 update. Let's assume we know $\mathbf{H}_k = \mathbf{B}_k^{-1}$. The SR1 update is a rank-1 update, so we can set $\mathbf{U} = \mathbf{u} = (\bm{\gamma}_k - \mathbf{B}_k \bm{\delta}_k)$ and $\mathbf{V} = \mathbf{u}^T / ((\dots)^T \bm{\delta}_k)$. By applying the SMW formula and performing algebraic simplification, we arrive at the inverse update formula:
\begin{equation}
\mathbf{H}_{k+1} = \mathbf{H}_k + \frac{(\bm{\delta}_k - \mathbf{H}_k \bm{\gamma}_k)(\bm{\delta}_k - \mathbf{H}_k \bm{\gamma}_k)^T}{(\bm{\delta}_k - \mathbf{H}_k \bm{\gamma}_k)^T \bm{\gamma}_k}
\end{equation}
This is also a symmetric rank-1 update, but it applies directly to the inverse Hessian $\mathbf{H}_k$.

\subsection{SR1 with Inverse Hessian: P6 Solved!}
By updating the inverse Hessian $\mathbf{H}_k$ instead of $\mathbf{B}_k$, we can construct a significantly more efficient algorithm.

\textbf{Algorithm (SR1 update $\mathbf{H}_k$ instead of $\mathbf{B}_k$)}
\begin{itemize}
    \item \textbf{S0:} Start with $\mathbf{H}_0 = \mathbf{B}_0^{-1}$ (e.g., $\mathbf{H}_0 = \mathbf{I}$).
    \item \textbf{S2 (Compute $\mathbf{d}_k$):}
        \begin{equation}
        \mathbf{d}_k = -\mathbf{H}_k \nabla f(\mathbf{x}_k)
        \end{equation}
        This is now a matrix-vector multiplication, which costs $O(n^2)$.
    \item \textbf{S5 (Compute $\mathbf{H}_{k+1}$):}
        \begin{equation}
        \mathbf{H}_{k+1} = \mathbf{H}_k + \frac{(\dots)(\dots)^T}{(\dots)^T(\dots)}
        \end{equation}
        This involves vector additions and an outer product, which costs $O(n^2)$.
\end{itemize}

\subsubsection{Complexity and Convergence}
\begin{itemize}
    \item \textbf{Complexity:} The total work per iteration is now $O(n^2)$, successfully satisfying our goal (P6). We have avoided the costly $O(n^3)$ linear system solution.
    \item \textbf{Convergence:} The method converges \textbf{superlinearly} when close to the minimizer. This is slightly slower than the quadratic convergence of Newton's method but is often a worthwhile trade-off for the greatly reduced per-iteration cost.
\end{itemize}

\subsection{SR1 Summary}
The SR1 method, particularly when updating the inverse Hessian, is a simple, rank-1 update that satisfies the secant condition and achieves an efficient $O(n^2)$ iteration cost.

However, the fundamental problems remain even with the inverse update:
\begin{itemize}
    \item The denominator $(\bm{\delta}_k - \mathbf{H}_k \bm{\gamma}_k)^T \bm{\gamma}_k$ can still be zero.
    \item The updated inverse Hessian $\mathbf{H}_{k+1}$ (and thus $\mathbf{B}_{k+1}$) is not guaranteed to be positive definite.
\end{itemize}
The lack of guaranteed positive definiteness is a serious practical problem that motivates the search for a more robust method.

\section{Motivation for the BFGS Method}
The SR1 method is good, but its primary weakness is the lack of guaranteed positive definiteness. This leads to a crucial question:
\begin{itemize}
    \item \textbf{Question:} Can we find an update that satisfies all 6 desirable properties (P1-P6), especially maintaining positive definiteness (P2) while achieving $O(n^2)$ complexity (P6)?
    \item \textbf{Answer:} Yes, but it requires a slightly more complex update.
\end{itemize}

This brings us to the \textbf{Broyden-Fletcher-Goldfarb-Shanno (BFGS)} algorithm.
\begin{itemize}
    \item It is the most widely used and effective quasi-Newton algorithm.
    \item It is a symmetric \textbf{rank-2} update of the form:
    \begin{equation}
    \mathbf{B}_{k+1} = \mathbf{B}_k + \mathbf{u}\mathbf{u}^T + \mathbf{v}\mathbf{v}^T
    \end{equation}
    \item It overcomes the weaknesses of SR1 (especially regarding positive definiteness) while retaining the desirable $O(n^2)$ speed.
\end{itemize}

\section{Efficiently Updating Cholesky Factors in Quasi-Newton Methods}
In the context of Quasi-Newton methods, particularly BFGS, we maintain an approximation of the Hessian (or its inverse). When using Cholesky factorization to ensure the Hessian approximation $\mathbf{B}_k$ remains positive definite, we express it as $\mathbf{B}_k = \mathbf{L}_k \mathbf{L}_k^T$, where $\mathbf{L}_k$ is a lower triangular matrix.

The rank-two update of BFGS leads to an updated matrix $\mathbf{B}_{k+1}$. A key challenge is to efficiently find the new Cholesky factor $\mathbf{L}_{k+1}$ without performing a full Cholesky decomposition of $\mathbf{B}_{k+1}$, which would be an $O(n^3)$ operation.

The update to the Cholesky factor can be formulated as a rank-one update to a matrix $\mathbf{J}^T$ (where $\mathbf{J}^T$ can be related to $\mathbf{L}^T$), resulting in a new matrix $\mathbf{J}_+^T$ that is "almost" upper-triangular. Specifically, it is the sum of an upper-triangular matrix and a rank-one matrix. The question becomes: how do we efficiently restore the triangular structure to find the new Cholesky factor?

\subsection{The QR Factorization Trick}
A powerful and efficient technique to restore triangularity is the QR factorization.

\begin{proof}[Proposition: QR Factorization]
Any square matrix $\mathbf{X}$ has a unique factorization $\mathbf{X} = \mathbf{Q}\mathbf{R}$, where:
\begin{itemize}
    \item $\mathbf{Q}$ is an orthogonal matrix ($\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$).
    \item $\mathbf{R}$ is an upper-triangular matrix with a non-negative diagonal.
\end{itemize}
\end{proof}

The strategy is as follows:
\begin{enumerate}
    \item We start with our non-triangular matrix $\mathbf{J}_+^T$. This matrix is the result of a rank-one update to an upper-triangular matrix, so it has a specific structure that can be exploited.
    \item We compute its QR factorization: $\mathbf{J}_+^T = \mathbf{Q}_+ \mathbf{R}_+$.
    \item This computation can be done very efficiently, in $O(n^2)$ time, because $\mathbf{J}_+^T$ is almost triangular. This is typically achieved using a sequence of Givens rotations, which are designed to introduce zeros into a matrix in a targeted manner.
    \item We then analyze the new Hessian approximation $\mathbf{B}_+$. Assuming the Hessian approximation is given by $\mathbf{B}_+ = \mathbf{J}_+^T \mathbf{J}_+$, we can substitute the QR factorization:
    \begin{align*}
        \mathbf{B}_+ &= (\mathbf{Q}_+ \mathbf{R}_+)^T (\mathbf{Q}_+ \mathbf{R}_+) \\
        &= \mathbf{R}_+^T \mathbf{Q}_+^T \mathbf{Q}_+ \mathbf{R}_+ \\
        &= \mathbf{R}_+^T \mathbf{I} \mathbf{R}_+ \quad (\text{since } \mathbf{Q}_+ \text{ is orthogonal}) \\
        &= \mathbf{R}_+^T \mathbf{R}_+
    \end{align*}
    \item \textbf{Result:} We have found a new decomposition for $\mathbf{B}_+ = \mathbf{R}_+^T \mathbf{R}_+$. Since $\mathbf{R}_+$ is upper-triangular, $\mathbf{R}_+^T$ is lower triangular. We can therefore set our new Cholesky factor $\mathbf{L}_+ = \mathbf{R}_+^T$. This entire update has been achieved in $O(n^2)$ time.
\end{enumerate}

\section{The BFGS Algorithm with Cholesky Updates}
Incorporating the QR trick, the full BFGS algorithm can be stated as follows:

\begin{description}
    \item[S0] Choose an initial point $\mathbf{x}_0$, an initial Cholesky factor $\mathbf{L}_0$ (e.g., $\mathbf{L}_0 = \mathbf{I}$), and a tolerance $\epsilon > 0$. Set $k = 0$.
    \item[S1] If $\|\nabla f(\mathbf{x}_k)\| \leq \epsilon$, \textbf{stop}.
    \item[S2] \textbf{(Solve for $\mathbf{d}_k$ in $O(n^2)$)} Solve the system $\mathbf{B}_k \mathbf{d}_k = -\nabla f(\mathbf{x}_k)$ by using the factors $\mathbf{L}_k$:
    \begin{itemize}
        \item Solve $\mathbf{L}_k \mathbf{g}_k = -\nabla f(\mathbf{x}_k)$ for $\mathbf{g}_k$ (forward-substitution).
        \item Solve $\mathbf{L}_k^T \mathbf{d}_k = \mathbf{g}_k$ for $\mathbf{d}_k$ (back-substitution).
    \end{itemize}
    \item[S3] Perform a line search to find a step size $\alpha_k > 0$ that satisfies the Wolfe conditions.
    \item[S4] Set $\delta_k = \alpha_k \mathbf{d}_k$ and update $\mathbf{x}_{k+1} = \mathbf{x}_k + \delta_k$. Compute $\gamma_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)$.
    \item[S5] \textbf{(Update $\mathbf{L}_k$ in $O(n^2)$)}
    \begin{itemize}
        \item Compute the intermediate matrix $\mathbf{J}_{k+1}^T = \mathbf{L}_k^T + \text{(rank-1 update vector)}$.
        \item Compute the QR factorization: $\mathbf{J}_{k+1}^T = \mathbf{Q}_{k+1} \mathbf{R}_{k+1}$.
        \item Set the new Cholesky factor: $\mathbf{L}_{k+1} = \mathbf{R}_{k+1}^T$.
    \end{itemize}
    \item[S6] Set $k \leftarrow k+1$ and go to S1.
\end{description}

\section{Properties and Drawbacks of BFGS}

\subsection{Key Properties}
The BFGS algorithm is highly effective due to several desirable properties:
\begin{itemize}
    \item \textbf{Computational Cost:} It spends only $O(n^2)$ computation time per iteration, which is a significant improvement over the $O(n^3)$ cost of Newton's method.
    \item \textbf{Positive Definiteness:} By updating the Cholesky factor $\mathbf{L}_k$, it guarantees that every Hessian approximation $\mathbf{B}_k$ is symmetric positive definite, ensuring that the search direction is always a descent direction.
    \item \textbf{Convergence Rate:} It has a local \textbf{superlinear} convergence rate. While not quite quadratic like Newton's method, it is very fast in practice.
    \item \textbf{Quadratic Termination:} On a strictly convex quadratic function, BFGS finds the exact minimum in at most $n$ iterations (assuming an exact line search).
\end{itemize}

\subsection{Drawbacks: The Scaling Issue}
While BFGS has excellent convergence properties, it faces significant challenges as the problem dimension $n$ grows large, which is common in machine learning and deep learning.

\subsubsection{The Memory Bottleneck}
The primary drawback is the memory requirement. BFGS requires storing the approximate Hessian $\mathbf{B}_k$ (or its Cholesky factor $\mathbf{L}_k$) as a dense $n \times n$ matrix.
\begin{itemize}
    \item \textbf{Memory Complexity:} The storage cost is $O(n^2)$.
\end{itemize}

\subsubsection{Practical Example}
Consider an optimization problem with $n = 100,000$ variables:
\begin{itemize}
    \item The Hessian approximation matrix contains $(10^5)^2 = 10^{10}$ entries.
    \item Using double precision (8 bytes per entry), storing this matrix requires $8 \times 10^{10}$ bytes $\approx$ \textbf{80 GB of RAM}.
    \item For $n = 1,000,000$, this becomes prohibitive, requiring approximately \textbf{8 TB} of RAM.
\end{itemize}

\section{L-BFGS (Limited-Memory BFGS)}
To tackle large-scale problems, we need a method that approximates Newton steps without the massive memory footprint of standard BFGS. This is the motivation behind the Limited-Memory BFGS (L-BFGS) algorithm.

\subsection{The Core Idea}
Instead of forming and storing the full $n \times n$ matrix for the inverse Hessian approximation $\mathbf{H}_k$, L-BFGS stores only the information needed to implicitly represent this matrix. This is achieved by using a history of the most recent updates.

\subsection{Storage Strategy}
The BFGS update is constructed using "curvature pairs" derived from the last step:
\begin{align*}
\delta_k &= \mathbf{x}_{k+1} - \mathbf{x}_k \\
\gamma_k &= \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)
\end{align*}
L-BFGS stores only the \textbf{most recent $m$ pairs} $\{(\delta_i, \gamma_i)\}_{i=k-m}^{k-1}$. The history size $m$ is a small constant (e.g., $m \approx 10$ to $20$), independent of $n$. All older information is discarded.

\subsection{How L-BFGS Works: The Two-Loop Recursion}
The main challenge is computing the search direction $\mathbf{d}_k = -\mathbf{H}_k \nabla f(\mathbf{x}_k)$ without ever forming the matrix $\mathbf{H}_k$.

The matrix $\mathbf{H}_k$ can be viewed as the result of applying the BFGS inverse update formula $m$ times, starting from an initial seed matrix $\mathbf{H}_k^0$ (typically $\mathbf{H}_k^0 = \mathbf{I}$). L-BFGS uses a specialized algorithm called the **"Two-Loop Recursion"** that "unrolls" this recursive definition to compute the matrix-vector product $\mathbf{H}_k \nabla f(\mathbf{x}_k)$ directly.

This algorithm performs the operation using only the stored $m$ curvature pairs and the current gradient. The operation consists of a backward pass (Loop 1) and a forward pass (Loop 2), which together implicitly apply the sequence of updates.

\subsection{The Payoff}
This strategy leads to significant computational and memory savings:
\begin{itemize}
    \item No $n \times n$ matrices are ever formed or stored.
    \item \textbf{Computation Cost:} The cost per iteration is reduced from $O(n^2)$ to $O(mn)$.
    \item \textbf{Storage Cost:} The memory requirement is reduced from $O(n^2)$ to $O(mn)$.
\end{itemize}

\subsection{Comparison: BFGS vs. L-BFGS}
Here is a summary of the key differences:
\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Standard BFGS} & \textbf{L-BFGS} \\ \hline
Memory Usage & $O(n^2)$ & $O(mn)$ (Linear!) \\
CPU per Iteration & $O(n^2)$ & $O(mn)$ \\
Convergence & Superlinear & Linear (but fast) \\
Use Case & Small/Medium $n$ & Large scale $n > 1000$ \\ \hline
\end{tabular}
\caption{Comparison of Standard BFGS and L-BFGS.}
\end{table}

While L-BFGS technically reduces the convergence rate from superlinear to linear, in practice it is still very fast and is the method of choice for large-scale optimization problems where storing the full Hessian approximation is infeasible.

\newpage
\chapter{Lesson 25-11}

\section{Final Comments \& Summary Table on Optimization Methods}
To contextualize the methods discussed, it is useful to summarize their cost and convergence properties. Let $C(f)$ be the cost of one function evaluation, and $n$ be the number of parameters. The trade-off between computational cost per iteration and the rate of convergence is a central theme in selecting an optimization algorithm.

\begin{table}[h!]
\centering
\caption{Cost-Convergence Summary of Optimization Methods}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Method} & \textbf{Cost per Iteration} & \textbf{Convergence Rate} \\ \hline
Steepest Descent & $O(nC(f))$ & Linear \\ \hline
Quasi-Newton & $O(n^2 + nC(f))$ & Superlinear \\ \hline
Newton-Raphson & $O(n^3 + n^2C(f))$ & Quadratic \\ \hline
\end{tabular}
\end{table}

Steepest Descent, a first-order method, offers a low computational cost per iteration but suffers from a slow (linear) convergence rate. In contrast, Newton's method, a second-order method, achieves a much faster (quadratic) convergence but requires computing and inverting the Hessian matrix, leading to a prohibitive cost of $O(n^3)$ per iteration.

Quasi-Newton methods, such as the BroydenFletcherGoldfarbShanno (BFGS) algorithm, provide a practical compromise. By approximating the Hessian (or its inverse), they achieve a superlinear convergence ratefaster than linear but not quite quadraticwith a more manageable computational cost of $O(n^2)$ per iteration.

\subsection{Scalability: BFGS vs. L-BFGS}
While standard BFGS is a significant improvement over Newton's method, its $O(n^2)$ memory and computational complexity still pose a bottleneck for large-scale problems common in modern machine learning (e.g., when $n > 1000$).

The Limited-memory BFGS (L-BFGS) algorithm addresses this scalability issue. Instead of storing the full $n \times n$ dense matrix for the approximate Hessian, L-BFGS only stores a small number, $m$, of recent correction vectors used to implicitly define the Hessian approximation. This drastically reduces both memory usage and computational complexity per iteration to $O(mn)$, which is linear in $n$ for a small, fixed $m$.

The trade-off is a reduction in convergence rate from superlinear to linear. However, in practice, L-BFGS often exhibits very fast linear convergence, making it a preferred choice for large-scale optimization problems.

\begin{table}[h!]
\centering
\caption{Comparison: BFGS vs. L-BFGS}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Standard BFGS} & \textbf{L-BFGS} \\ \hline
Memory Usage & $O(n^2)$ & $O(mn)$ (Linear!) \\ \hline
CPU per Iteration & $O(n^2)$ & $O(mn)$ \\ \hline
Convergence & Superlinear & Linear (but fast) \\ \hline
Use Case & Small/Medium $n$ & Large scale $n > 1000$ \\ \hline
\end{tabular}
\end{table}

\section{Non-Linear Least Squares: The Levenberg-Marquardt Algorithm}
A common optimization problem is non-linear least squares (LS), where the goal is to find the parameters $\vec{w}$ that best fit a non-linear model $f(\vec{x}, \vec{w})$ to a set of data points $\{(x_i, y_i)\}_{i=1}^m$.

\subsection{Mathematical Formulation}
We aim to minimize the sum of squared residuals. The residual for the $i$-th data point is defined as:
\begin{equation}
r_i(\vec{w}) = f(x_i, \vec{w}) - y_i
\end{equation}
The total cost function $f(\vec{w})$ to minimize is the sum of their squares:
\begin{equation}
f(\vec{w}) = \sum_{i=1}^{m} [r_i(\vec{w})]^2 = \vec{r}(\vec{w})^T \vec{r}(\vec{w}) = \|\vec{r}(\vec{w})\|^2
\end{equation}

\subsection{The Dilemma: Stability vs. Speed}
When solving this problem, we face a classic dilemma:
\begin{itemize}
    \item \textbf{Gradient Descent (GD):} Moves in the direction of steepest descent. It is stable and guaranteed to converge but is often very slow.
    \item \textbf{Newton's Method:} Builds a second-order quadratic model of the cost function and jumps to its minimum. It offers very fast (quadratic) convergence but is computationally expensive and requires computing the full Hessian matrix.
\end{itemize}
The challenge is to find a method that combines the \textbf{stability of Gradient Descent} with the \textbf{speed of a Newton-like method}.

\subsection{The Gauss-Newton Approximation}
The Levenberg-Marquardt (LM) algorithm builds upon an approximation of Newton's method known as the Gauss-Newton (GN) method. To derive this, we first compute the full Hessian $\mat{H}$ of the least-squares cost function $f(\vec{w})$.
\begin{equation}
\mat{H} = 2\mat{J}^T \mat{J} + 2\sum_{i=1}^{m} r_i(\vec{w}) \nabla^2 r_i(\vec{w})
\end{equation}
where $\mat{J}$ is the Jacobian matrix of the residuals $\vec{r}(\vec{w})$. The Hessian consists of a first-order term ($2\mat{J}^T \mat{J}$) and a second-order term involving second derivatives of the residuals.

The core idea of the Gauss-Newton method is to approximate the Hessian by dropping the computationally expensive second-order term:
\begin{equation}
\mat{H} \approx 2\mat{J}^T \mat{J}
\end{equation}
This approximation is effective under two main conditions:
\begin{enumerate}
    \item The residuals $r_i$ are small, which occurs when the model is a good fit and the algorithm is near the minimum.
    \item The model is "almost linear," meaning the second derivatives $\nabla^2 r_i$ are close to zero.
\end{enumerate}

\subsection{The Levenberg-Marquardt (LM) Update Rule}
The LM algorithm adaptively interpolates between Gradient Descent and the Gauss-Newton method. It starts with the Gauss-Newton update rule but modifies it by adding a "damping" term $\lambda \ge 0$. The update step $\vec{\delta}_k$ is found by solving the following linear system:
\begin{equation}
(\mat{J}^T \mat{J} + \lambda \mat{I}) \vec{\delta}_k = -\mat{J}^T \vec{r}
\end{equation}
where $\mat{I}$ is the identity matrix. This added term, $\lambda \mat{I}$, ensures that the matrix $(\mat{J}^T \mat{J} + \lambda \mat{I})$ is always invertible for $\lambda > 0$, thus overcoming the potential singularity or ill-conditioning of the $\mat{J}^T \mat{J}$ matrix in the standard Gauss-Newton method.

The behavior of the algorithm is controlled by the damping parameter $\lambda$:
\begin{itemize}
    \item \textbf{Case 1: $\lambda$ is Small ($\lambda \to 0$):} The update approximates the Gauss-Newton method. This provides fast, Newton-like steps but can be unstable.
    \begin{equation}
    \vec{\delta}_k \approx -(\mat{J}^T \mat{J})^{-1} \mat{J}^T \vec{r} \quad (\text{Gauss-Newton})
    \end{equation}
    \item \textbf{Case 2: $\lambda$ is Large ($\lambda \to \infty$):} The term $\mat{J}^T \mat{J}$ becomes negligible, and the update approximates a Gradient Descent step with a small learning rate. This is slow but stable.
    \begin{equation}
    \vec{\delta}_k \approx -\frac{1}{\lambda} \mat{J}^T \vec{r} \approx -\frac{1}{2\lambda} \nabla f(\vec{w}_k) \quad (\text{Gradient Descent})
    \end{equation}
\end{itemize}

\subsection{The LM Algorithm: An Iterative Process}
The LM algorithm is an iterative process that adaptively adjusts $\lambda$ based on the success of each step.

\begin{enumerate}
    \item Choose an initial guess $\vec{w}_0$ and an initial damping parameter $\lambda_0$.
    \item \textbf{At each iteration $k$:}
    \begin{itemize}
        \item Compute residuals $\vec{r}(\vec{w}_k)$ and Jacobian $\mat{J}(\vec{w}_k)$.
        \item Compute the current error $f(\vec{w}_k)$.
        \item \textbf{Solve} $(\mat{J}^T \mat{J} + \lambda_k \mat{I})\vec{\delta}_k = -\mat{J}^T \vec{r}$ for the step $\vec{\delta}_k$.
        \item Calculate a candidate new point: $\vec{w}_{\text{new}} = \vec{w}_k + \vec{\delta}_k$.
        \item Evaluate the new error $f(\vec{w}_{\text{new}})$.
    \end{itemize}
    \item \textbf{Update Strategy (The Core Logic):} This step determines whether to accept the new point and how to adjust the damping parameter $\lambda$.
    \begin{itemize}
        \item \textbf{If $f(\vec{w}_{\text{new}}) < f(\vec{w}_k)$ (Good Step):}
        \begin{itemize}
            \item \textbf{Accept the step:} The parameters for the next iteration are updated: $\vec{w}_{k+1} \leftarrow \vec{w}_{\text{new}}$.
            \item \textbf{Decrease $\lambda$:} The model is trusted more, so the damping parameter is reduced. This is typically done by dividing by a factor $\nu > 1$ (e.g., $\nu=2$). This reduction moves the algorithm closer to the Gauss-Newton method.
              \begin{equation}
                  \lambda_{k+1} = \lambda_k / \nu
              \end{equation}
        \end{itemize}
        \item \textbf{If $f(\vec{w}_{\text{new}}) \ge f(\vec{w}_k)$ (Bad Step):}
        \begin{itemize}
            \item \textbf{Reject the step:} The parameters are not updated: $\vec{w}_{k+1} \leftarrow \vec{w}_k$.
            \item \textbf{Increase $\lambda$:} The model is trusted less, and the damping parameter is increased by multiplying by the factor $\nu$.
              \begin{equation}
                  \lambda_{k+1} = \lambda_k \times \nu
              \end{equation}
              This makes the algorithm behave more like Gradient Descent, which is more stable.
        \end{itemize}
    \end{itemize}
    \item \textbf{Convergence Check:} Repeat the iterative process until a convergence criterion is met (e.g., the step size $\norm{\vec{\delta}}$ becomes sufficiently small, or the error $f$ stops decreasing significantly).
\end{enumerate}

\subsection{Advantages of Levenberg-Marquardt}
The adaptive nature of the LM algorithm provides several key advantages:
\begin{itemize}
    \item \textbf{Robustness:} It can find a solution even if the initial guess is far from the optimal minimum. By increasing $\lambda$, the algorithm can navigate "difficult" regions where the pure Gauss-Newton method would fail.
    \item \textbf{Efficiency:} It combines the best of both worlds:
    \begin{itemize}
        \item \textbf{Far from the minimum}, when $\lambda$ is large, it acts like the stable but slow Gradient Descent method.
        \item \textbf{Close to the minimum}, when $\lambda$ is small, it acts like the fast-converging Gauss-Newton method.
    \end{itemize}
    \item \textbf{Handles Ill-Conditioning:} The damping term $\lambda\mat{I}$ ensures the matrix $(\mat{J}^T\mat{J} + \lambda\mat{I})$ is always invertible as long as $\lambda > 0$, preventing issues with singular or ill-conditioned $\mat{J}^T\mat{J}$ matrices.
\end{itemize}

\subsection{Drawbacks and Considerations}
Despite its strengths, the LM algorithm has limitations:
\begin{itemize}
    \item \textbf{Computational Cost:} It requires computing the Jacobian matrix $\mat{J}$ at every iteration and solving an $n \times n$ linear system, an $O(n^3)$ operation which is prohibitive for very large models.
    \item \textbf{Local Minimum:} Like most local optimizers, it is only guaranteed to find a local minimum, not the global one.
    \item \textbf{Parameter Tuning:} Performance can be sensitive to the initial $\lambda_0$ and the update factor $\nu$, though standard values often work well.
\end{itemize}

\subsection{Summary}
The Levenberg-Marquardt algorithm is a cornerstone of non-linear optimization. It improves on Gauss-Newton with a damping parameter $\lambda$ that adaptively interpolates between slow-but-stable Gradient Descent (large $\lambda$) and fast-but-unstable Gauss-Newton (small $\lambda$).

\section{An Introduction to Convolution}
Convolution is a fundamental operation in signal processing and machine learning, often best understood through polynomial multiplication.

\subsection{Cyclic vs. Non-Cyclic Convolution}
Consider two vectors, $\vec{p} = (1, 2, 3)$ and $\vec{q} = (4, 5, 0)$, associated with polynomials $p(x) = 1 + 2x + 3x^2$ and $q(x) = 4 + 5x$.

\subsubsection{Non-Cyclic (Standard) Convolution}
This corresponds to the standard product of their associated polynomials.
\begin{align*}
    p(x)q(x) &= (1 + 2x + 3x^2)(4 + 5x) \\
             &= 4 + 13x + 22x^2 + 15x^3
\end{align*}
The resulting coefficient vector is $(4, 13, 22, 15)$.

\subsubsection{Cyclic Convolution}
Cyclic convolution operates under a modulo constraint, such as modulo $(x^3 - 1)$, which implies a "wrap-around" behavior where $x^3 \equiv 1$.
\begin{align*}
    p(x)q(x) &\equiv 4 + 13x + 22x^2 + 15x^3 \pmod{x^3 - 1} \\
             &\equiv 4 + 13x + 22x^2 + 15(1) \\
             &= 19 + 13x + 22x^2
\end{align*}
The resulting coefficient vector for the cyclic convolution is $(19, 13, 22)$.

\subsection{Eigenvalues and Eigenvectors of Circulant Matrices}
A circulant matrix is a special type of matrix where each row is a cyclic shift of the one above it.

\begin{theorem}
The eigenvectors of all $n \times n$ circulant matrices are the same. They are the columns of the $n \times n$ Fourier Matrix $\mat{F}$.
\end{theorem}
Let $\omega = e^{2\pi i / n}$ be the $n$-th primitive root of unity. The $k$-th eigenvector $\vec{v}_k$ (for $k=0, \dots, n-1$) is:
\begin{equation}
    \vec{v}_k = 
    \begin{pmatrix}
        1 & \omega^k & \omega^{2k} & \dots & \omega^{(n-1)k}
    \end{pmatrix}^T
\end{equation}
The corresponding eigenvalue $\lambda_k$ is the $k$-th component of the Discrete Fourier Transform (DFT) of the matrix's first row, $\vec{c} = (c_0, \dots, c_{n-1})$.
\begin{equation}
    \lambda_k = \sum_{j=0}^{n-1} c_j \omega^{-jk}
\end{equation}

\subsection{The Convolution Theorem}
The fact that all circulant matrices share the same eigenvectors leads to a profound result: \textit{Convolution in the time/spatial domain is equivalent to element-wise multiplication in the frequency domain.}

This allows for efficient computation of convolution $\vec{c} * \vec{d}$ using the Fast Fourier Transform (FFT):
\begin{enumerate}
    \item Compute DFTs: $\hat{\vec{c}} = \text{DFT}(\vec{c})$ and $\hat{\vec{d}} = \text{DFT}(\vec{d})$.
    \item Multiply element-wise: $\hat{\vec{r}} = \hat{\vec{c}} \odot \hat{\vec{d}}$.
    \item Compute Inverse DFT: $\vec{r} = \text{IDFT}(\hat{\vec{r}})$.
\end{enumerate}
This reduces the complexity from $O(n^2)$ to $O(n \log n)$.

\subsection{Application in 2D: Image Filtering}
In 2D, a small matrix called a **kernel** slides over an input image. At each position, an element-wise multiplication between the kernel and the overlapping image patch is performed, and the results are summed to produce an output pixel.

\subsubsection{Example: The Blurring (Averaging) Filter}
A blurring kernel averages a pixel's value with its neighbors. A common $3 \times 3$ kernel is:
\begin{equation}
    \text{Blur Kernel} = \frac{1}{9}
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 1 & 1 \\
        1 & 1 & 1
    \end{pmatrix}
\end{equation}

\subsubsection{Example: Edge Detection Filter}
An edge detection kernel highlights sharp changes in intensity. The Sobel operator for detecting vertical edges is:
\begin{equation}
    \text{Vertical Edge Kernel } (G_x) = 
    \begin{pmatrix}
        -1 & 0 & 1 \\
        -2 & 0 & 2 \\
        -1 & 0 & 1
    \end{pmatrix}
\end{equation}
This kernel subtracts pixels on the left from pixels on the right, producing a strong response at vertical edges.

\section{The Universal Approximation Theorem (UAT)}
The UAT explains the expressive power of neural networks, answering why they are chosen as a model class for function approximation.

\begin{theorem}[The Universality Theorem]
A neural network with a single hidden layer can approximate any continuous function to any desired level of precision.
\end{theorem}

This theorem has two critical caveats:
\begin{enumerate}
    \item \textbf{Existence vs. Trainability:} The theorem guarantees a network \textit{exists}, but not that we can find its parameters through training.
    \item \textbf{Network Size:} The number of neurons required might be impractically large.
\end{enumerate}

\subsection{A Constructive and Visual Proof using Sigmoids}
We can build an intuition for the UAT by constructing an approximator.
\subsubsection{Step 1: The Hidden Neuron as a Step Function}
A single neuron computes $\sigma(wx + b)$. Using the sigmoid activation function $\sigma(z) = (1 + e^{-z})^{-1}$, we can create a step function. By making the weight $w$ very large ($w \to \infty$), the sigmoid becomes infinitely steep. The bias $b$ controls the step's location at $x = -b/w$.

\begin{lstlisting}[language=Python, caption={Interactive Sigmoid Function}]
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact

@interact
def sigmoid(w=1.0, b=0.0):
    x = np.linspace(-10, 10, 200)
    z = x*w + b
    y = 1 / (1 + np.exp(-z))
    
    plt.plot(x, y)
    plt.ylabel('Sigmoid(x)')
    plt.xlabel('x')
    plt.title('Sigmoid function.')
    plt.grid()
    plt.show()
\end{lstlisting}

\subsubsection{Step 2: Creating a "Bump" Function}
By combining two hidden neurons (one with a positive weight and one with a negative weight in the output layer), we can subtract one step function from another. This creates a rectangular "bump" of controllable height, width, and location.

\subsubsection{Step 3: Approximating any Function $f(x)$}
With the ability to create arbitrary bumps, we can approximate any continuous function by "gluing" many of these bumps together, similar to a Riemann sum. By making the bumps sufficiently narrow and numerous, we can achieve any desired accuracy.

\subsection{Universality Beyond the Sigmoid Function}
The UAT is not tied exclusively to the sigmoid function. It holds for any non-linear, "S-shaped" activation function and, more importantly, for the widely used Rectified Linear Unit (ReLU).

\subsubsection{The Rectified Linear Unit (ReLU)}
The ReLU function, defined as $f(z) = \max(0, z)$, is also a universal approximator. The proof strategy is similar but uses a different building block: a triangular "hat" function.

\begin{lstlisting}[language=Python, caption={Python code to visualize a single ReLU function.}]
import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interact, fixed, widgets

@interact
def relu(w=widgets.FloatSlider(min=-10.0, max=10.0, step=1, value=1.0),
         b=widgets.FloatSlider(min=-10.0, max=10.0, step=1, value=-5.0)):
    x = np.linspace(-10, 10, 200)
    z = x*w + b
    y = np.maximum(0, z)
    
    plt.plot(x, y)
    plt.ylabel('ReLU(x)')
    plt.xlabel('x')
    plt.title('ReLu function.')
    plt.grid(True)
    plt.show()
\end{lstlisting}

\subsubsection{From ReLUs to a "Hat" Function}
A triangular "hat" function can be constructed using a linear combination of three ReLU neurons. The combination of an up-ramp, a steeper down-ramp, and a final ramp to flatten the tail creates a localized triangle.

\subsubsection{Connection to the Finite Element Method (FEM)}
This construction establishes a powerful connection to numerical approximation theory. In 1D FEM, a function $f(x)$ is approximated as a linear combination of local basis functions $\phi_i(x)$:
\begin{equation}
    f(x) \approx \sum_{i=1}^{N} c_i \phi_i(x)
\end{equation}
The "hat" functions constructed by the ReLU network are precisely the P1 (linear) basis functions used in FEM. By creating a series of these hat functions at different locations (controlled by biases) and summing them with appropriate weights, a neural network can create a piecewise linear approximation of any continuous function. Increasing the number of hidden neurons is equivalent to refining the FEM mesh, improving the approximation accuracy.

In practice, ReLUs are preferred over sigmoids in deep networks for their computational efficiency and ability to mitigate the vanishing gradient problem.

\newpage
\chapter{Lesson 02-12}

\section{Motivation: Why Functional Analysis?}

In machine learning, we often work with familiar objects like finite-dimensional vectors, for example, a feature vector $\vec{x} \in \R^n$. The analysis of these objects is relatively straightforward. However, many problems involve infinite-dimensional objects such as functions, images, or time series, which present a greater analytical challenge.

Functional analysis provides the necessary tools to bridge this gap. A key question we seek to answer is:
\begin{quote}
    Can we extend concepts like length (norm), angle (inner product), and convergence from finite-dimensional spaces like $\R^n$ to infinite-dimensional spaces?
\end{quote}
The answer is yes. Functional analysis gives us the framework to do so, which is essential for understanding foundational results in neural networks, such as the Cybenko theorem. This theorem proves that neural networks can approximate any continuous function, a cornerstone of their expressive power.

\section{Vector Spaces: The Foundation}
The most fundamental structure is the vector space. It provides an abstract framework that allows us to treat functions and other infinite-dimensional objects with the same algebraic rules as familiar vectors.

\subsection{Definition}
A real vector space $V$ is a set equipped with two operations:
\begin{enumerate}
    \item Vector addition: $+ : V \times V \to V$
    \item Scalar multiplication: $\cdot : \R \times V \to V$
\end{enumerate}
These operations must satisfy the following axioms for all $\vec{u}, \vec{v}, \vec{w} \in V$ and $\lambda \in \R$:
\begin{itemize}
    \item \textbf{Commutativity:} $\vec{u} + \vec{v} = \vec{v} + \vec{u}$
    \item \textbf{Associativity:} $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$
    \item \textbf{Distributivity:} $\lambda \cdot (\vec{u} + \vec{v}) = \lambda \cdot \vec{u} + \lambda \cdot \vec{v}$
    \item \textbf{Existence of zero:} $\vec{u} + \vec{0} = \vec{u}$ for all $\vec{u} \in V$
\end{itemize}

\subsection{Basis and Dimension}
A \textbf{basis} of a vector space $V$ is a minimal set of linearly independent vectors that span $V$. The number of elements in the basis is the \textbf{dimension} of the space. Any vector $\vec{v} \in V$ can be uniquely written as a linear combination of basis vectors $\phi_i$:
\begin{equation}
    \vec{v} = c_1\phi_1 + c_2\phi_2 + \dots + c_n\phi_n
\end{equation}

\subsection{Common Examples of Vector Spaces}
\begin{itemize}
    \item $\R^n$: The space of all $n$-dimensional real vectors.
    \item $\mathcal{P}_k(I)$: The space of polynomials of degree $\leq k$ on an interval $I$.
    \item $C^0(I)$: The space of continuous functions on an interval $I$.
    \item $L^2(\Omega)$: The space of square-integrable functions on a domain $\Omega$.
\end{itemize}

\section{Inner Products and Norms}
To discuss concepts like angles and lengths, we must equip our vector space with additional structure.

\subsection{Inner Product: Measuring Angles}
An \textbf{inner product} on a vector space $V$ is a function $\langle \cdot, \cdot \rangle : V \times V \to \R$ that satisfies:
\begin{enumerate}
    \item $\langle \vec{u}, \vec{u} \rangle \ge 0$ and $\langle \vec{u}, \vec{u} \rangle = 0 \iff \vec{u} = \vec{0}$
    \item $\langle \vec{u}, \vec{v} \rangle = \langle \vec{v}, \vec{u} \rangle$ (Symmetry)
    \item $\langle \alpha \vec{u} + \beta \vec{v}, \vec{w} \rangle = \alpha \langle \vec{u}, \vec{w} \rangle + \beta \langle \vec{v}, \vec{w} \rangle$ (Linearity)
\end{enumerate}
Inner products are crucial because they allow us to define angles and orthogonality in infinite-dimensional spaces.

\subsubsection{Examples of Inner Products}
\begin{itemize}
    \item \textbf{In $\R^n$ (Euclidean):} The standard dot product.
    \begin{equation}
        \langle \vec{v}, \vec{w} \rangle = \sum_{i=1}^n v_i w_i = \vec{v} \cdot \vec{w}
    \end{equation}
    
    \item \textbf{In $C([0, 1])$ (Continuous functions):}
    \begin{equation}
        \langle f, g \rangle = \int_0^1 f(x)g(x) \, dx
    \end{equation}
    
    \item \textbf{In $C^1([0, 1])$ (Differentiable functions):} A common inner product includes derivative information.
    \begin{equation}
        \langle f, g \rangle = \int_0^1 \left[ f(x)g(x) + f'(x)g'(x) \right] \, dx
    \end{equation}
\end{itemize}
It is important to note that an inner product \textit{always} induces a norm.

\subsection{Norm: Measuring Length}
A \textbf{norm} on a vector space $V$ is a function $\norm{\cdot} : V \to \R$ that satisfies:
\begin{enumerate}
    \item \textbf{Triangle inequality:} $\norm{\vec{u} + \vec{v}} \le \norm{\vec{u}} + \norm{\vec{v}}$
    \item \textbf{Homogeneity:} $\norm{\lambda \vec{u}} = |\lambda| \norm{\vec{u}}$
    \item \textbf{Positivity:} $\norm{\vec{u}} \ge 0$, with equality if and only if $\vec{u} = \vec{0}$
\end{enumerate}
Norms generalize the concept of "distance" from $\R^n$ to any vector space, which is fundamental for the analysis of convergence and approximation.

\subsubsection{Common Norms}
\begin{itemize}
    \item \textbf{Norms in $\R^n$:}
    \begin{itemize}
        \item Euclidean norm ($L_2$): $\norm{\vec{v}}_2 = \left( \sum_{i=1}^n |v_i|^2 \right)^{1/2}$
        \item Max norm ($L_\infty$): $\norm{\vec{v}}_\infty = \max_{1 \le i \le n} |v_i|$
        \item Manhattan norm ($L_1$): $\norm{\vec{v}}_1 = \sum_{i=1}^n |v_i|$
    \end{itemize}
    \item \textbf{Norms in Function Spaces:}
    \begin{itemize}
        \item Energy norm ($L^2(I)$): $\norm{f}_{L^2(I)} = \left( \int_I |f(x)|^2 \, dx \right)^{1/2}$
        \item Supremum norm ($L^\infty(I)$): $\norm{f}_{L^\infty(I)} = \sup_{x \in I} |f(x)|$
    \end{itemize}
\end{itemize}
Different norms measure different notions of "smallness" or distance, and the choice of norm depends on the specific context of the problem.

\section{Fundamental Theorems and Concepts}

\subsection{Cauchy-Schwarz Inequality}
A key theorem linking inner products and their induced norms is the Cauchy-Schwarz inequality.

\begin{theorem}
For an inner product space with induced norm $\norm{\vec{u}} = \sqrt{\langle \vec{u}, \vec{u} \rangle}$:
\begin{equation}
    |\langle \vec{u}, \vec{v} \rangle| \le \norm{\vec{u}} \cdot \norm{\vec{v}}
\end{equation}
\end{theorem}

\paragraph{Intuition} This is the abstract version of the familiar geometric formula for the dot product: $\vec{u} \cdot \vec{v} = \norm{\vec{u}} \norm{\vec{v}} \cos\theta$. Since $|\cos\theta| \le 1$, the dot product can never exceed the product of the vector lengths.

\paragraph{Example} In the space $C([0, 1])$, the inequality takes the form:
\begin{equation}
    \left| \int_0^1 f(x)g(x) \, dx \right| \le \sqrt{\int_0^1 f^2(x) \, dx} \cdot \sqrt{\int_0^1 g^2(x) \, dx}
\end{equation}

\subsection{Convergence and Completeness}
The concepts of convergence and completeness are essential for ensuring that our analytical processes, like iterative optimization, have well-defined limits within the space we are working in.

\subsubsection{Sequences in Normed Spaces}
\begin{itemize}
    \item \textbf{Cauchy Sequence:} A sequence $\{v_i\}_{i=1}^\infty$ is \textbf{Cauchy} if for all $\epsilon > 0$, there exists an integer $n$ such that:
    \begin{equation}
        \norm{v_i - v_j} \le \epsilon \quad \text{for all } i, j \ge n
    \end{equation}
    Intuitively, the terms of the sequence get arbitrarily close to each other.

    \item \textbf{Convergent Sequence:} A sequence $\{v_i\}_{i=1}^\infty$ \textbf{converges} to $v$ if for all $\epsilon > 0$, there exists an integer $n$ such that:
    \begin{equation}
        \norm{v - v_i} \le \epsilon \quad \text{for all } i \ge n
    \end{equation}
    Intuitively, the terms of the sequence get arbitrarily close to a specific limit point $v$.
\end{itemize}

A key question arises: \textbf{Is every Cauchy sequence convergent?} The answer is not always, which leads to the concept of completeness.

\subsubsection{A Classic Counterexample: Rationals}
Consider the space of rational numbers $\mathbb{Q}$ with the absolute value as the norm. The sequence $\{u_n\}_{n \in \mathbb{N}}$ defined by:
\begin{equation}
    u_n = \sum_{k=0}^{n} \frac{1}{k!} = \frac{1}{0!} + \frac{1}{1!} + \frac{1}{2!} + \dots + \frac{1}{n!}
\end{equation}
is a Cauchy sequence in $\mathbb{Q}$ (all its terms are rational). However, it converges to the number $e$, which is irrational ($e \notin \mathbb{Q}$). Therefore, this Cauchy sequence does not converge to a limit \textit{within} the space $\mathbb{Q}$.

\subsection{Completeness}
A normed vector space is called \textbf{complete} if every Cauchy sequence in the space converges to an element that is also in the space.

\paragraph{Key Insight} Completeness ensures that our analysis is "closed"we don't escape the space when taking limits. This is a critical property for the reliability of numerical algorithms.

\section{Banach and Hilbert Spaces: The Main Characters}
The most important spaces in functional analysis are those that are complete.

\begin{itemize}
    \item \textbf{Banach Space:} A Banach space is a \textbf{complete normed vector space}.
    \item \textbf{Hilbert Space:} A Hilbert space is a \textbf{complete inner product vector space}.
\end{itemize}

\paragraph{The Relationship}
Every Hilbert space is a Banach space. This is because completeness combined with an inner product implies completeness with the norm induced by that inner product. However, not every Banach space is a Hilbert space, as a Banach space might have only a norm and not an inner product.

\paragraph{Why they matter}
These spaces are \textit{closed} under limits. This property guarantees that iterative methods, which generate sequences of approximations, will converge to a valid solution within the space, making the analysis of such methods reliable.

\subsection{Hierarchy of Spaces}
The relationship between these spaces can be visualized as a set of nested concepts:
\begin{enumerate}
    \item The most general is the \textbf{Vector Space}.
    \item A vector space with a norm is a \textbf{Normed Linear Space}.
    \item A complete normed linear space is a \textbf{Banach Space}.
    \item A complete inner product space (which is a special kind of normed space) is a \textbf{Hilbert Space}.
\end{enumerate}
Thus, Hilbert spaces are the most structured, inheriting all the properties of the broader categories.

\section{The Riemann Integral: A Quick Reminder}

The Riemann integral provides an intuitive method for calculating the area under the curve of a "nice" function.

\subsection{The Idea: Lower and Upper Sums}

For a function $f: [a, b] \to \R$, we begin by partitioning the interval $[a, b]$ into a set of smaller subintervals or "boxes." We then approximate the area under the curve using rectangles. This leads to two key concepts: the lower sum and the upper sum.

\begin{itemize}
    \item \textbf{Lower Sum (Darboux Sum):} The lower sum, denoted $L(f, P)$, is the sum of the areas of rectangles that lie entirely beneath the function's curve. For a partition $P = \{x_0, x_1, \dots, x_N\}$, the height of each rectangle on the subinterval $[x_{k-1}, x_k]$ is determined by the infimum (greatest lower bound) of the function on that subinterval.
    \begin{equation}
        L(f, P) = \sum_{k=1}^{N} m_k (x_k - x_{k-1}) \quad \text{where } m_k = \inf_{x \in [x_{k-1}, x_k]} f(x)
    \end{equation}

    \item \textbf{Upper Sum (Darboux Sum):} The upper sum, denoted $U(f, P)$, is the sum of the areas of rectangles that entirely enclose the function's curve. The height of each rectangle is determined by the supremum (least upper bound) of the function on the corresponding subinterval.
    \begin{equation}
        U(f, P) = \sum_{k=1}^{N} M_k (x_k - x_{k-1}) \quad \text{where } M_k = \sup_{x \in [x_{k-1}, x_k]} f(x)
    \end{equation}
\end{itemize}

\subsection{Riemann Integrability}

A function $f$ is defined as Riemann integrable if, as the partition $P$ becomes infinitely fine, the greatest possible lower sum (the supremum of all lower sums) converges to the same value as the smallest possible upper sum (the infimum of all upper sums).
\begin{equation}
    \inf_{P} U(f, P) = \sup_{P} L(f, P)
\end{equation}
This method is simple, intuitive, and works effectively for most "nice" functions, such as continuous or piecewise continuous functions.

\subsection{A Problematic Case: The Dirichlet Function}

The limitations of the Riemann integral become apparent when we consider "pathological" functions. A classic example is the Dirichlet function, which is defined as:
\begin{equation}
    \chi_{\mathbb{Q}}(x) = 
    \begin{cases} 
        1 & \text{if } x \in \mathbb{Q} \text{ (rational)} \\
        0 & \text{if } x \notin \mathbb{Q} \text{ (irrational)} 
    \end{cases}
\end{equation}
This function is discontinuous everywhere. For any partition $P$ of an interval, say $[0, 1]$, every subinterval $[x_{k-1}, x_k]$ will contain both rational and irrational numbers. Consequently:
\begin{itemize}
    \item The infimum on each interval is always 0: $m_k = 0$.
    \item The supremum on each interval is always 1: $M_k = 1$.
\end{itemize}
This leads to a constant result for the lower and upper sums, regardless of the partition's fineness:
\begin{align*}
    L(f, P) &= \sum_{k=1}^{N} 0 \cdot (x_k - x_{k-1}) = 0 \\
    U(f, P) &= \sum_{k=1}^{N} 1 \cdot (x_k - x_{k-1}) = 1
\end{align*}
Since $\sup L(f, P) = 0 \neq \inf U(f, P) = 1$, the Dirichlet function is \textbf{not Riemann integrable}.

\section{Motivation for the Lebesgue Integral}

The failure of the Riemann integral to handle functions like the Dirichlet function motivates the need for a more powerful theory of integration.

\subsection{The Engineering Perspective}
From a practical and engineering standpoint, we need to address several challenges:
\begin{itemize}
    \item We often need to work with \textbf{pathological functions}, such as discontinuous signals or signals corrupted by noise.
    \item The Dirichlet function, while seemingly abstract, appears in theoretical contexts, particularly in the study of \textbf{sets of measure zero}.
    \item We need robust tools to perform integration in \textbf{multiple dimensions} and to be able to reliably \textbf{swap limits with integrals}.
\end{itemize}

\subsection{Three Core Motivations for Lebesgue Integral}
The Lebesgue integral is designed to address these limitations. Its development is driven by three main goals:
\begin{enumerate}
    \item \textbf{Handle Pathological Functions:} To integrate discontinuous and other "weird" functions that are not Riemann integrable.
    \item \textbf{Multidimensional Integration:} To provide a more elegant and powerful framework for multidimensional integration, making theorems like Fubini's theorem easier to prove and apply.
    \item \textbf{Limit Exchange:} To establish mild and clear conditions under which the integral and limit operators can be swapped, which is formalized in the Monotone Convergence Theorem and the Dominated Convergence Theorem. These are essential tools in approximation theory and for analyzing neural networks.
\end{enumerate}

\section{The Lebesgue Approach: Inverting the Perspective}

The fundamental innovation of the Lebesgue integral is to change the way we partition the function's domain and range.

\subsection{The Key Difference: Partitioning the Domain vs. the Range}
\begin{itemize}
    \item \textbf{Riemann Integral:} Partitions the \textbf{domain} (the x-axis) into vertical strips and sums their areas.
    \item \textbf{Lebesgue Integral:} Partitions the \textbf{range} (the y-axis) into horizontal bands and sums the "measure" (a generalized notion of length or size) of the corresponding sets in the domain.
\end{itemize}
Instead of asking "What is the height of the function over this small interval?", Lebesgue integration asks "For this small range of function values, what is the size of the set of x-values that produce them?"

The Lebesgue integral is approximated by summing the areas of these level sets:
\begin{equation}
    \int f \,d\mu \approx \sum_{i} \alpha_i \mu(S_i)
\end{equation}
where $S_i = \{x : f(x) \in [\alpha_i, \alpha_{i+1}]\}$ is the set of points in the domain whose function values fall within the $i$-th horizontal band, and $\mu(S_i)$ is the measure of that set.

\section{Measure: The Foundation of Lebesgue Integration}

The concept of a "measure" generalizes the idea of length, area, or volume to more complex sets.

\begin{itemize}
    \item A \textbf{measure} $\mu$ is a function $\mu: \mathcal{A} \to [0, \infty]$ that assigns a non-negative "size" to sets in a collection $\mathcal{A}$.
    \item It satisfies two key properties:
    \begin{enumerate}
        \item The measure of the empty set is zero: $\mu(\emptyset) = 0$.
        \item \textbf{Countable Additivity:} For any collection of disjoint sets $\{A_i\}$, the measure of their union is the sum of their individual measures:
        \begin{equation}
            \mu\left(\bigcup_i A_i\right) = \sum_i \mu(A_i)
        \end{equation}
    \end{enumerate}
\end{itemize}

\subsection{A Crucial Fact: Measure Zero}
A powerful and crucial result from measure theory is that \textbf{every countable set has measure zero}. This fact is precisely why the Dirichlet function becomes integrable under the Lebesgue framework.

For example, the set of rational numbers $\mathbb{Q}$ is countable. Therefore, the set of rational numbers within the interval $[0, 1]$, denoted $\mathbb{Q} \cap [0, 1]$, is also countable and thus has measure zero.

\subsection{The Dirichlet Function Revisited}
With the Lebesgue integral, we can now integrate the Dirichlet function $\chi_{\mathbb{Q}}(x)$ over $[0, 1]$. We partition the range into values $\{0, 1\}$ and consider the measure of the domain sets corresponding to these values:
\begin{equation}
    \int_0^1 \chi_{\mathbb{Q}}(x) \,dx = 1 \cdot \mu(\mathbb{Q} \cap [0, 1]) + 0 \cdot \mu([0, 1] \setminus \mathbb{Q})
\end{equation}
Since $\mathbb{Q} \cap [0, 1]$ is a countable set, its measure is zero: $\mu(\mathbb{Q} \cap [0, 1]) = 0$. The measure of the irrational numbers in the interval is 1. Therefore, the integral becomes:
\begin{equation}
    \int_0^1 \chi_{\mathbb{Q}}(x) \,dx = 1 \cdot 0 + 0 \cdot 1 = 0
\end{equation}
This demonstrates how Lebesgue integration makes pathological functions manageable.

\section{Spaces of Integrable Functions: \(L^p\) Spaces}
The concept of Lebesgue integration allows us to define powerful function spaces known as \(L^p\) spaces, which are essential in functional analysis and its applications.

\subsection{Definition and \(L^p\) Norm}
For $1 \le p < \infty$, the \(L^p(\Omega)\) space is the set of functions $f: \Omega \to \R$ for which the \(L^p\) norm is finite:
\begin{equation}
    L^p(\Omega) = \{ f: \Omega \to \R \mid \norm{f}_{L^p} < \infty \}
\end{equation}
The \(L^p\) norm is defined as:
\begin{equation}
    \norm{f}_{L^p(\Omega)} = \left( \int_{\Omega} |f(x)|^p \,dx \right)^{1/p}, \quad p = 1, 2, \dots
\end{equation}
For the special case of $p = \infty$, the norm is the essential supremum:
\begin{equation}
    \norm{f}_{L^\infty(\Omega)} = \sup_{x \in \Omega} |f(x)|
\end{equation}

\subsection{Properties of \(L^p\) Spaces}
A key theorem states that for any $1 \le p \le \infty$, the space $L^p(\Omega)$ is a \textbf{Banach space} (a complete normed vector space).

\subsubsection{Special Case: \(L^2\)}
The space $L^2(\Omega)$ is particularly important because it is not just a Banach space, but a \textbf{Hilbert space}. This means it is equipped with an inner product that induces the norm.
\begin{itemize}
    \item \textbf{Inner Product:}
    \begin{equation}
        \langle f, g \rangle_{L^2} = \int_{\Omega} f(x)g(x) \,dx
    \end{equation}
    \item \textbf{Induced Norm:}
    \begin{equation}
        \norm{f}_{L^2} = \sqrt{\langle f, f \rangle_{L^2}} = \sqrt{\int_{\Omega} |f(x)|^2 \,dx}
    \end{equation}
\end{itemize}
Being a Hilbert space endows $L^2$ with a rich geometric structure, including concepts like orthogonality, projections, and Fourier series, which are essential for approximation theory and the Cybenko theorem in neural networks.

\section{Weak Derivatives and Integration by Parts}

The concept of a derivative is central to optimization, but the classical definition is too restrictive for many functions encountered in functional analysis and machine learning. To handle functions that are not classically differentiable (e.g., functions with "kinks"), we introduce the notion of a \textit{weak derivative}, which is a generalization based on integration by parts.

\subsection{Classical Integration by Parts}
The foundation for the weak derivative is the classical integration by parts formula. For two functions $u, \phi \in C^1$ (continuously differentiable functions), where $\phi$ has compact support on a domain $\Omega$, the formula is:
\begin{equation}
\int_{\Omega} u'(x)\phi(x) \,dx = - \int_{\Omega} u(x)\phi'(x) \,dx
\end{equation}
The compact support of $\phi$ ensures that the boundary terms in the integration by parts formula vanish. This identity allows us to "transfer" the derivative from one function ($u$) to another ($\phi$) at the cost of a sign change.

\subsection{Definition of the Weak Derivative}
We can use the structure of the integration by parts formula to define a derivative for functions that are not necessarily smooth. A function $u$ might not have a classical derivative, but if we can find another function $g$ that behaves like its derivative under integration, we can define $g$ as the weak derivative of $u$.

\begin{definition}[Weak Derivative]
Let $u$ be a function in $L^1_{loc}(\Omega)$, the space of locally integrable functions on a domain $\Omega$. We say that a function $g \in L^1_{loc}(\Omega)$ is the \textbf{weak derivative} of $u$ if the following identity holds:
\begin{equation}
\int_{\Omega} g(x)\phi(x) \,dx = - \int_{\Omega} u(x)\phi'(x) \,dx
\end{equation}
for all \textbf{test functions} $\phi \in \mathcal{D}(\Omega)$. The space $\mathcal{D}(\Omega)$ consists of functions that are infinitely differentiable (smooth) and have compact support within $\Omega$.
\end{definition}

If such a function $g$ exists, we write $g = \frac{du}{dx}$ in the weak sense. This definition generalizes the concept of a derivative to a much broader class of functions, including those with jumps or kinks, by relying on an integral identity rather than a point-wise limit. The integrals here are to be understood in the Lebesgue sense.

\subsection{Example: The Kink Function}
A key motivating example is a function that is continuous but not classically differentiable at a point. Consider the function $u(x) = 3 - |x|$ on the interval $\Omega = (-1, 1)$. This function has a "kink" at $x=0$ and is therefore not differentiable at that point in the classical sense.

However, it does possess a weak derivative. The weak derivative is given by the piecewise constant function:
\begin{equation}
g(x) =
\begin{cases}
    1 & \text{for } -1 < x \leq 0 \\
    -1 & \text{for } 0 < x < 1
\end{cases}
\end{equation}
Even though $u$ is not classically differentiable at $x=0$, the weak derivative $g(x)$ exists and captures the slope of $u(x)$ almost everywhere. The key insight is that for the integral-based definition, a single point of non-differentiability (the kink) does not matter. What matters is the jump in the derivative, which is captured by the piecewise nature of $g(x)$.

\subsection{Properties of Weak Derivatives}
Weak and classical derivatives share many important properties, which makes calculus with weak derivatives feasible:
\begin{itemize}
    \item \textbf{Linearity:} The derivative of a linear combination of functions is the linear combination of their derivatives.
    \item \textbf{Product Rule and Chain Rule:} These rules from classical calculus also apply to weak derivatives under appropriate conditions.
    \item \textbf{Equivalence for Differentiable Functions:} If a function is classically differentiable, its weak derivative equals its classical derivative. This ensures that the weak derivative is a true generalization.
\end{itemize}

\section{Sobolev Spaces: Functions with Weak Derivatives}
The concept of weak derivatives allows us to define new function spaces, known as Sobolev spaces, which are fundamental in the study of partial differential equations (PDEs) and the analysis of neural networks. These spaces group functions together based on the integrability of their weak derivatives.

\begin{definition}[Sobolev Space]
The \textbf{Sobolev space} $W^{k,p}(\Omega)$ is the set of functions $u$ in $L^p(\Omega)$ such that for every multi-index $\alpha$ with $|\alpha| \leq k$, the weak derivative $D^\alpha u$ exists and is in $L^p(\Omega)$. Formally:
\begin{equation}
W^{k,p}(\Omega) = \left\{ u \in L^p(\Omega) \mid D^\alpha u \in L^p(\Omega) \text{ for all } |\alpha| \leq k \right\}
\end{equation}
This space is equipped with the norm:
\begin{equation}
\norm{u}_{W^{k,p}} = \left( \sum_{|\alpha| \leq k} \norm{D^\alpha u}_{L^p}^p \right)^{1/p}
\end{equation}
\end{definition}
The parameter $k$ indicates the order of derivatives that are required to be in $L^p$, and $p$ indicates the type of integrability.

\subsection{Important Case: \texorpdfstring{$p=2$}{p=2} (Hilbert Spaces)}
A particularly important case is when $p=2$, as this endows the Sobolev space with a Hilbert space structure. The notation $H^k(\Omega)$ is standard for $W^{k,2}(\Omega)$. For $k=1$, we have:
\begin{equation}
H^1(\Omega) = \left\{ u \in L^2(\Omega) \mid \grad u \in [L^2(\Omega)]^d \right\}
\end{equation}
where $d$ is the dimension of the domain $\Omega$. The space $H^1(\Omega)$ is a Hilbert space with the inner product:
\begin{equation}
\langle u, v \rangle_{H^1} = \int_{\Omega} (uv + \grad u \cdot \grad v) \,dx
\end{equation}
The corresponding norm is:
\begin{equation}
\norm{u}_{H^1} = \sqrt{\norm{u}_{L^2}^2 + \norm{\grad u}_{L^2}^2}
\end{equation}

\section{The Riesz Representation Theorem}
The Riesz Representation Theorem is a cornerstone of functional analysis that establishes a profound connection between abstract linear functionals and concrete inner products in Hilbert spaces.

\subsection{Linear Functionals and Dual Spaces}
First, we define two key concepts:
\begin{itemize}
    \item \textbf{Linear Functional:} A linear functional $\ell: X \to \R$ is a linear operator that maps elements of a vector space $X$ to scalars.
    \item \textbf{Dual Space:} The dual space $X'$ of a normed space $X$ is the set of all bounded linear functionals on $X$.
    \begin{equation}
        X' = \{ \ell: X \to \R \mid \ell \text{ is linear and bounded} \}
    \end{equation}
    The dual norm is defined as:
    \begin{equation}
        \norm{\ell}_{X'} = \sup_{u \neq 0} \frac{|\ell(u)|}{\norm{u}_X}
    \end{equation}
\end{itemize}
The dual space is itself a Banach space and can be thought of as the space of all possible "measurements" we can make on the original space $X$.

\subsection{The Theorem}
\begin{theorem}[Riesz Representation]
Let $H$ be a Hilbert space and $\ell$ a bounded linear functional on $H$. Then there exists a \textbf{unique} element $u \in H$ such that:
\begin{equation}
\ell(v) = \langle v, u \rangle \quad \text{for all } v \in H
\end{equation}
Moreover, the norm of the functional is equal to the norm of the representing element:
\begin{equation}
\norm{\ell}_{H'} = \norm{u}_H
\end{equation}
\end{theorem}

This theorem is profound because it states that in a Hilbert space, every abstract linear functional (a mapping) can be represented by a concrete operation: the inner product with a specific, unique element of the space. This creates a bridge between abstract functionals and concrete inner products.

\section{Approximation Theory}

A central question in both numerical analysis and machine learning is whether we can approximate arbitrary functions from a complex class using simpler functions. This is the core of approximation theory.

\subsection{Why This Matters}
This question is fundamental for several reasons:
\begin{itemize}
    \item \textbf{Neural Networks:} We approximate complex functions with compositions of simple nonlinearities (activation functions). The universal approximation theorems are a direct result of this line of inquiry.
    \item \textbf{Numerical Methods:} We approximate solutions to differential equations with polynomials (e.g., in finite element methods).
    \item \textbf{Compression:} We approximate high-dimensional data with low-rank structures (e.g., via SVD/PCA).
\end{itemize}

\subsection{Key Theorem: Weierstrass Approximation}
A foundational result in this area is the Weierstrass Approximation Theorem, which guarantees that polynomials can approximate any continuous function on a closed interval.

\begin{theorem}[Weierstrass]
Any continuous function on a closed interval can be uniformly approximated by polynomials.

More formally: For any function $f \in C([a, b])$ and any $\epsilon > 0$, there exists a polynomial $p(x)$ such that:
\begin{equation}
\norm{f - p}_{\infty} < \epsilon
\end{equation}
\end{theorem}

\subsection{Density and Closure}
The concept of approximation is formally captured by the idea of a dense subset.
\begin{definition}[Density]
A subset $S \subseteq X$ is \textbf{dense} in a normed space $X$ if for every element $u \in X$ and every $\epsilon > 0$, there exists an element $s \in S$ such that:
\begin{equation}
\norm{u - s} < \epsilon
\end{equation}
\end{definition}
In simpler terms, a family of functions $S$ is dense in a space $X$ if, no matter which target function you pick from $X$ and no matter how small an error you allow, you can always find a function from the family $S$ that is as close as you like to the target.

\subsubsection{Examples of Density}
\begin{itemize}
    \item \textbf{Polynomials} are dense in $C([a, b])$ (Weierstrass Theorem).
    \item \textbf{Trigonometric polynomials} are dense in $L^2(0, 2\pi)$ (the basis for Fourier series).
    \item \textbf{Continuous functions} are dense in $L^p(\Omega)$.
\end{itemize}
These density results are what guarantee that we can use simpler classes of functions (like polynomials or the outputs of neural networks) to approximate more complex ones to any desired degree of accuracy.

\newpage
\chapter{Lesson 09-12}

\section{Function Approximation with Neural Networks}

The Universal Approximation Theorem (UAT) provides the theoretical foundation for the expressive power of neural networks. It states that a neural network with a single hidden layer can approximate any continuous function to an arbitrary degree of precision, given enough neurons. Before delving into the formal proof, it is instructive to build an intuition for how this is achieved.

\subsection{Constructing Basis Functions with ReLUs}
A neural network constructs complex functions by creating and combining simpler "basis" functions. With the Rectified Linear Unit (ReLU) activation function, we can construct piecewise linear basis functions. A classic example is the "hat function," a triangular pulse that is zero everywhere except for a small interval.

A hat function can be formed by a linear combination of shifted ReLUs. For instance, a hat function centered at $c$ with width $2\delta$ can be constructed as:
\begin{equation}
    \text{hat}(x) = \text{ReLU}(x - (c-\delta)) - 2\cdot\text{ReLU}(x - c) + \text{ReLU}(x - (c+\delta))
\end{equation}
This demonstrates that even simple ReLU units can be combined to create localized, non-zero functions.

\begin{figure}[h!]
    \centering
    \includegraphics[draft, width=0.6\linewidth]{hatt.png} % Placeholder name for the hat function plot
    \caption{A "hat function" (solid black) constructed as a linear combination of three ReLU functions (dashed lines).}
\end{figure}

\subsection{Approximation via Hat Functions}
By summing multiple scaled and shifted hat functions, we can construct a piecewise linear function that approximates any continuous target function. The approximation works by placing hat functions at various "interpolation nodes" along the target function and scaling their heights to match the function's value at those points. As the number of hat functions increases, the piecewise linear approximation can become arbitrarily close to the target function. This intuitive process illustrates the core principle of the UAT: a neural network approximates a complex function by summing many simple, non-linear basis functions.

\begin{figure}[h!]
    \centering
    \includegraphics[draft, width=0.7\linewidth]{approximation.png} % Placeholder name for the approximation plot
    \caption{Approximating a continuous target function (red) with a sum of scaled hat functions (thin green lines), resulting in a piecewise linear approximation (black dashed line).}
\end{figure}

\section{Formal Proof of the Universal Approximation Theorem}

To formally prove the Universal Approximation Theorem, we first establish the mathematical framework.

\subsection{The Mathematical Framework}
Let us define the components of our approximation problem:
\begin{itemize}
    \item Let $\mathbf{x}$ be the input variable (feature vector) and $z$ be the target output.
    \item The target function to be approximated is denoted by $z = f(\mathbf{x})$. This function belongs to a certain function space $S$.
    \item The function space $S$ is equipped with a distance metric $d$, which allows us to measure the "closeness" between two functions.
    \item We define the input space as the unit hypercube $I_n = [0, 1]^n$, where $n$ is the dimension of the input $\mathbf{x}$.
\end{itemize}

With this framework, we can formally define what it means for a class of functions to be a "universal approximator."

\begin{proof}[Definition (Universal Approximator)]
We say that a class of functions $\mathcal{U}$ (representing all possible outcomes of a neural network architecture) is a \textbf{universal approximator} for the space $(S, d)$ if $\mathcal{U}$ is $d$-dense in $S$. That is:
\begin{equation}
    \forall f \in S, \forall \epsilon > 0, \exists g \in \mathcal{U} : d(f, g) < \epsilon
\end{equation}
This means that for any target function $f$ in our space $S$, and for any arbitrarily small error tolerance $\epsilon$, we can find a function $g$ that our neural network can represent, such that the distance between $f$ and $g$ is less than $\epsilon$.
\end{proof}

\subsection{The Discriminatory Property of Activation Functions}
The validity of the UAT depends critically on the properties of the activation function used in the hidden layer.

\subsubsection{Sigmoidal Functions}
The original proofs of the UAT were based on sigmoidal activation functions.
\begin{proof}[Definition (Sigmoidal Function)]
A function $\sigma: \mathbb{R} \to [0, 1]$ is called \textbf{sigmoidal} if it satisfies the following asymptotic properties:
\begin{align}
    \lim_{x \to -\infty} \sigma(x) &= 0 \\
    \lim_{x \to \infty} \sigma(x) &= 1
\end{align}
Typical examples include the logistic sigmoid function and the hyperbolic tangent function (tanh).
\end{proof}

\subsubsection{The Discriminatory Property}
A more general and crucial property is that of being "discriminatory."
\begin{proof}[Definition (n-discriminatory)]
Let $n$ be a natural number. An activation function $f: \mathbb{R} \to \mathbb{R}$ is \textbf{n-discriminatory} if the only finite signed Borel measure $\mu$ such that
\begin{equation}
    \int_{I_n} f(\mathbf{y} \cdot \mathbf{x} + \theta) d\mu(\mathbf{x}) = 0, \quad \forall \mathbf{y} \in \mathbb{R}^n, \theta \in \mathbb{R}
\end{equation}
is the zero measure (i.e., $\mu = 0$).
\end{proof}

\begin{proof}[Definition (Discriminatory)]
An activation function $f$ is \textbf{discriminatory} if it is n-discriminatory for any natural number $n$.
\end{proof}

\noindent \textbf{Remark:} A discriminatory function is "volumetrically non-destructive." This means that when it acts on linear transformations of the input, it preserves enough information so that no non-zero measure can cause its integral to vanish across all possible transformations. This property is essential for the function to be able to distinguish different regions of the input space.

\subsection{Proof by Contradiction}
The proof of the UAT proceeds by contradiction. We will leverage key theorems from functional analysis to show that assuming the contrary leads to an impossible conclusion. We rely on two fundamental results: the Riesz Representation Theorem and the Hahn-Banach Theorem.

\begin{itemize}
    \item Let $K$ be a compact set in $\mathbb{R}^n$ (for our purposes, $K = I_n$).
    \item Let $C(K)$ denote the space of all real-valued continuous functions on $K$.
    \item Let $M(I_n)$ be the space of finite signed regular measures on $I_n$.
\end{itemize}

\begin{theorem}[Riesz Representation Theorem]
Let $F$ be a bounded linear functional on $C(K)$. Then there exists a unique finite signed Borel measure $\mu$ on $K$ such that:
\begin{equation}
    F(f) = \int_K f(x) d\mu(x), \quad \forall f \in C(K)
\end{equation}
This theorem establishes a powerful duality between abstract linear functionals on $C(K)$ and concrete integrals against a measure.
\end{theorem}

\vspace{2ex}
\noindent
\textbf{Reformulation of Hahn-Banach Separation}
Let $U$ be a linear, \textbf{non-dense} subspace of a normed linear space $X$. Then there exists a bounded linear functional $L$ on $X$ such that $L \neq 0$ on $X$ and $L|_{U} = 0$ (L vanishes on $U$).\\[2ex]
This lemma is a separation principle. If a subspace $U$ does not "fill" the entire space $X$, we can find a linear functional $L$ that is zero for every element in $U$ but is not the zero functional on the larger space $X$.

\begin{proof}[Proposition (Density of Neural Network Outputs)]
Let $\sigma$ be any continuous \textbf{discriminatory} function. Then the set of finite sums of the form:
\begin{equation}
    G(\mathbf{x}) = \sum_{j=1}^{N} \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + \theta_j)
\end{equation}
are dense in $C(I_n)$. Here, $\mathbf{w}_j \in \mathbb{R}^n$, $\theta_j \in \mathbb{R}$, and $\alpha_j \in \mathbb{R}$. The function $G(\mathbf{x})$ represents the output of a neural network with a single hidden layer.
\end{proof}

\textbf{Proof Strategy:}
The proof proceeds by contradiction.
\begin{enumerate}
    \item \textbf{Assumption:} Assume that the set of all possible network outputs, let's call it $U$, is \textbf{not} dense in $C(I_n)$.
    
    \item \textbf{The Crucial Link:} Since $U$ is a linear, non-dense subspace of $C(I_n)$, we can combine the Hahn-Banach and Riesz Representation theorems. By the Hahn-Banach theorem (Lemma 1), there must exist a non-zero linear functional $L$ that vanishes on all of $U$. By the Riesz Representation Theorem, this functional $L$ can be represented as an integral against a non-zero measure $\mu \in M(I_n)$. This leads to the crucial conclusion:
    \begin{equation}
        \int_{I_n} h(\mathbf{x}) d\mu(\mathbf{x}) = 0, \quad \forall h \in U, \quad \text{where } \mu \neq 0.
    \end{equation}
    
    \item \textbf{Reaching a Contradiction:} We know that any function $h \in U$ has the form of $G(\mathbf{x})$. Therefore, the condition above means:
    \begin{equation}
        \int_{I_n} \left( \sum_{j=1}^{N} \alpha_j \sigma(\mathbf{w}_j^T \mathbf{x} + \theta_j) \right) d\mu(\mathbf{x}) = 0
    \end{equation}
    This must hold for \textit{all possible choices} of parameters ($N$, $\alpha_j$, $\mathbf{w}_j$, $\theta_j$). We can choose a simple case where $N=1$ and $\alpha_1=1$. This simplifies the condition to:
    \begin{equation}
        \int_{I_n} \sigma(\mathbf{w}^T \mathbf{x} + \theta) d\mu(\mathbf{x}) = 0, \quad \forall \mathbf{w} \in \mathbb{R}^n, \theta \in \mathbb{R}
    \end{equation}
    However, our initial hypothesis for the proposition is that the activation function $\sigma$ is \textbf{discriminatory}. By the very definition of a discriminatory function, the equation above can only hold if the measure $\mu$ is the zero measure ($\mu=0$).
    
    \item \textbf{Conclusion:} This is a direct contradiction. Our assumption that $U$ is not dense led us to conclude that there exists a non-zero measure $\mu$ satisfying the integral condition. But the discriminatory property of $\sigma$ implies that for this condition to hold, $\mu$ must be zero. Therefore, our initial assumption must be false. The space of network outputs $U$ must be dense in $C(I_n)$.
\end{enumerate}

\section{Verifying the Discriminatory Property}
The UAT proof hinges on the activation function being "discriminatory." We now show that common activation functions satisfy this crucial property.

\subsection{Sigmoidal Functions are Discriminatory}
\begin{theorem}
Any continuous \textbf{sigmoidal function} is discriminatory for all measures $\mu \in M(I_n)$.
\end{theorem}
To prove this, we require a geometric lemma concerning measures. First, we define the geometric structures induced by the affine transformation inside the activation function.
\begin{itemize}
    \item \textbf{Hyperplane:} $P_{\mathbf{w},\theta} = \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{w}^T\mathbf{x} + \theta = 0\}$
    \item \textbf{Open Half-Spaces:}
        \begin{align*}
            H^+_{\mathbf{w},\theta} &= \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{w}^T\mathbf{x} + \theta > 0\} \\
            H^-_{\mathbf{w},\theta} &= \{\mathbf{x} \in \mathbb{R}^n \mid \mathbf{w}^T\mathbf{x} + \theta < 0\}
        \end{align*}
\end{itemize}
    Let $\mu \in M(I_n)$. If $\mu$ vanishes on all hyperplanes ($P$) and open half-spaces ($H$) in $\mathbb{R}^n$, then $\mu$ is the zero measure.

\begin{proof}[Proof Sketch: Sigmoidal $\implies$ Discriminatory]
We begin by assuming the integral condition for a fixed measure $\mu$: $\int_{I_n} \sigma(\mathbf{w}^T\mathbf{x} + \theta) \,d\mu(\mathbf{x}) = 0$. Our goal is to show this implies $\mu = 0$.
\begin{enumerate}
    \item \textbf{Construct scaled functions:} We introduce a scaling parameter $\lambda \in \mathbb{R}^+$ and a shifting parameter $\phi \in \mathbb{R}$ to define a family of functions:
    $\sigma_\lambda(\mathbf{x}) = \sigma(\lambda(\mathbf{w}^T\mathbf{x} + \theta) + \phi)$.
    \item \textbf{Pointwise Convergence:} As $\lambda \to \infty$, the sigmoidal function $\sigma_\lambda$ sharpens and converges pointwise to a step-like function $\gamma(\mathbf{x})$.
    \item \textbf{Swap Limit and Integral:} By the Bounded Convergence Theorem, we can swap the limit and the integral:
    \begin{align*}
        0 &= \lim_{\lambda\to\infty} \int_{I_n} \sigma_\lambda(\mathbf{x}) \,d\mu(\mathbf{x}) = \int_{I_n} \gamma(\mathbf{x}) \,d\mu(\mathbf{x}) \\
        &= \mu(H^+_{\mathbf{w},\theta}) + \sigma(\phi)\mu(P_{\mathbf{w},\theta})
    \end{align*}
    This holds for any choice of $\phi \in \mathbb{R}$.
    \item \textbf{Taking limits on $\phi$:} Using the properties of sigmoidal functions ($\lim_{z\to\infty}\sigma(z)=1$ and $\lim_{z\to-\infty}\sigma(z)=0$):
    \begin{itemize}
        \item As $\phi \to +\infty$, we get: $\mu(H^+_{\mathbf{w},\theta}) + \mu(P_{\mathbf{w},\theta}) = 0$.
        \item As $\phi \to -\infty$, we get: $\mu(H^+_{\mathbf{w},\theta}) = 0$.
    \end{itemize}
    \item \textbf{Implication for Measures:} This implies $\mu(P_{\mathbf{w},\theta}) = 0$. By symmetry, $\mu(H^-_{\mathbf{w},\theta})=0$. Therefore, $\mu$ vanishes on all open half-spaces and hyperplanes.
    \item \textbf{Conclusion:} By the lemma, a measure that vanishes on all such sets must be the zero measure, i.e., $\mu=0$. This completes the proof.
\end{enumerate}
\end{proof}

\subsection{ReLU Activation is Discriminatory}
Modern neural networks often use the Rectified Linear Unit (ReLU), which is not sigmoidal because it is not bounded. We can still prove that the UAT holds for ReLU by showing it is discriminatory.

\begin{theorem}
The ReLU function is 1-discriminatory.
\end{theorem}
\begin{proof}[Proof Idea]
The core idea is to show that a known discriminatory function (a sigmoidal one) can be constructed from a linear combination of ReLU functions. If the integral of any ReLU function is zero, then the integral of this constructed sigmoidal function must also be zero, which in turn implies the measure is zero.
\begin{enumerate}
    \item \textbf{Construct a Sigmoidal Function from ReLUs:} The function $f(x) = \text{ReLU}(x) - \text{ReLU}(x-1)$ is bounded, continuous, and sigmoidal in nature.
    \item \textbf{Integrate the Constructed Function:} Assume that for a non-zero measure $\mu$, we have $\int \text{ReLU}(yx+\theta)d\mu(x) = 0$ for all $y, \theta \in \mathbb{R}$. Then the integral of a scaled and shifted version of our constructed function $f$ is:
    \begin{align*}
        \int f(yx+\theta)d\mu(x) &= \int \left( \text{ReLU}(yx+\theta_1) - \text{ReLU}(yx+\theta_2) \right) d\mu(x) \\
        &= \int \text{ReLU}(yx+\theta_1)d\mu(x) - \int \text{ReLU}(yx+\theta_2)d\mu(x) = 0 - 0 = 0
    \end{align*}
    \item \textbf{Conclusion:} We have shown that if a measure annihilates all ReLU functions, it must also annihilate the constructed function $f$. Since $f$ is sigmoidal, it is discriminatory. Therefore, this condition implies $\mu=0$. This proves that ReLU is 1-discriminatory.
\end{enumerate}
This result can be extended from 1-discriminatory to n-discriminatory through a "dimensional lifting" argument.
\end{proof}

\section{Beyond Universality: The Advantage of Depth}
The Universal Approximation Theorem is a profound result, but it has important practical limitations which motivate the use of deep, rather than shallow, networks.

\subsection{Existence vs. Efficiency: The Curse of Dimensionality}
\begin{itemize}
    \item \textbf{Existence vs. Training:} The theorem guarantees that a shallow network \textit{can} approximate any continuous function. However, it provides no guarantee that our training algorithms (like gradient descent) are capable of \textit{finding} the specific weights and biases required.
    \item \textbf{Efficiency:} The theorem does not address the efficiency of the approximation. The required number of neurons (width $W$) for a shallow network might be astronomically large, especially for high-dimensional inputs. This phenomenon is known as the \textbf{"Curse of Dimensionality."} For a function $f$ with smoothness $r$ and input dimension $d$, the approximation error for a shallow network with $N_s$ neurons behaves roughly like:
    \begin{equation}
        \|f - \hat{f}_{N_s}\| \approx O\left(N_s^{-\frac{r}{d}}\right)
    \end{equation}
    To achieve a desired error $\epsilon$, the number of neurons needed is approximately:
    \begin{equation}
        N_s \approx \epsilon^{-\frac{d}{r}}
    \end{equation}
    The exponential dependence on the input dimension $d$ makes shallow networks impractical for high-dimensional problems.
\end{itemize}

\subsection{The Compositional Structure of Real-World Functions}
The advantage of deep networks lies in their ability to efficiently approximate a specific class of functions that are prevalent in real-world applications: those that are \textbf{compositional} or \textbf{hierarchical}. This means they can be represented as a composition of simpler functions, where each function in the hierarchy depends on only a small number of variables.

Formally, a function $f(x)$ is compositional if it can be written as:
\begin{equation}
    f(x) = h_L \circ h_{L-1} \circ \dots \circ h_1(x)
\end{equation}
where each constituent function $h_i$ depends on only a few variables. For example, a function on 8 variables might be structured as a binary tree, where each sub-function takes only two inputs.

\begin{figure}[h!]
    \centering
    \includegraphics[draft, width=0.5\textwidth]{tree_structure.png}
    \caption{A binary tree representation of a compositional function. Each node represents a function $h_i$ that takes two inputs.}
\end{figure}

\subsection{Approximation Rates for Shallow vs. Deep Networks}
The benefit of deep networks becomes clear when analyzing the number of parameters required to achieve a certain approximation error $\epsilon$.

\begin{theorem}[Poggio et al., 2017]
    Let $\sigma$ be an infinitely differentiable activation function that is not a polynomial. For compositional functions, a Deep Network can achieve an approximation error $\epsilon$ with a size $N_d$ that scales as:
    \begin{equation}
        N_d \approx O\left((d - 1) \cdot \epsilon^{-\tilde{d}/r}\right)
    \end{equation}
    where $d$ is the input dimension, $\tilde{d}$ is the local dimension of the constituent functions, and $r$ is the smoothness of the function.
\end{theorem}

This advantage persists even for non-smooth activation functions like ReLU.
\paragraph{Complexity Summary}
\begin{itemize}
    \item \textbf{Shallow:} $N_s \approx \epsilon^{-d/r}$ (Exponential in $d$)
    \item \textbf{Deep (Compositional):} $N_d \approx d \cdot \epsilon^{-\tilde{d}/r}$ (Linear in $d$)
\end{itemize}
Depth effectively turns the input dimension $d$ from an exponential bottleneck into a linear, manageable factor, thereby "curing" the curse of dimensionality for compositional functions.

\subsection{A Numerical Example}
Let's quantify this difference. Suppose we have a compositional function with the following parameters:
\begin{itemize}
    \item Input dimension $d = 10^3$
    \item Smoothness $r = 10$
    \item Local (binary) dimension $\tilde{d} = 2$
    \item Target error $\epsilon = 10^{-2}$
\end{itemize}

Using the formulas, we can estimate the number of neurons required:
\begin{itemize}
    \item \textbf{Shallow case:} $N_s \approx (10^{-2})^{-1000/10} = (10^2)^{100} = 10^{200}$. This is an infeasibly large number.
    \item \textbf{Deep case (compositional):} $N_d \approx 1000 \cdot (10^{-2})^{-2/10} = 1000 \cdot (10^2)^{1/5} \approx 1000 \cdot 2.51 \approx 2500$. This is a perfectly manageable number of parameters.
\end{itemize}
This example starkly illustrates the exponential advantage of using a deep architecture for functions that exhibit compositional structure.

\section{Conclusion}
\begin{itemize}
    \item \textbf{Universality:} Both shallow and deep networks are universal approximators. The existence of an approximation is therefore not what distinguishes them.
    
    \item \textbf{Efficiency:} The crucial difference lies in efficiency. Deep networks are \textit{exponentially more efficient} at representing certain classes of functions, particularly those that are compositional, high-frequency, or symmetrical.
    
    \item \textbf{The Trade-off:}
    \begin{itemize}
        \item \textbf{Shallow:} Benefits from an easier optimization theory but requires a massive width (number of neurons) for complex, high-dimensional data, falling victim to the curse of dimensionality.
        \item \textbf{Deep:} Achieves high expressivity with far fewer parameters but introduces a much harder, highly non-convex optimization problem. This difficulty manifests as practical training challenges like vanishing gradients and complex loss landscapes.
    \end{itemize}
\end{itemize}

\chapter{Lesson 16-12}

\section{Introduction to Scientific Machine Learning (SciML)}

Scientific Machine Learning (SciML) represents a paradigm that integrates domain knowledge, often in the form of physical laws, with data-driven machine learning models. A prominent subfield within SciML is the development of Physics-Informed Neural Networks (PINNs), which embed the governing equations of a system directly into the training process of a neural network.

\subsection{The Fusion of Data and Physics}

Traditional machine learning methodologies are primarily data-driven. Given a dataset, algorithms such as least squares or standard neural networks are trained to learn a functional mapping or a surrogate model that captures the underlying patterns in the data.

The innovation of the physics-informed approach is the inclusion of physical laws, typically expressed as Partial Differential Equations (PDEs), as a form of regularization or a constraint within the model itself. This creates a powerful bridge between two distinct fields:
\begin{itemize}
    \item \textbf{Classical Numerical Methods:} Techniques like the Finite Difference Method (FDM) or Finite Element Method (FEM) rely on discretizing the problem domain into a mesh and solving the governing equations on that grid.
    \item \textbf{Data-Driven Machine Learning:} These methods often require large volumes of data to infer relationships without explicit knowledge of the underlying physics.
\end{itemize}

By combining these, we can construct models that are more reliable, require less training data, and have better generalization properties, as they are constrained to adhere to known physical principles. This approach is not only useful for modeling existing physical systems but also holds promise for the discovery of new physics from limited data.

\subsection{Traditional vs. Deep Learning Approaches to PDEs}

The deep learning approach for solving PDEs offers several key advantages over traditional mesh-based methods.

\begin{table}[h!]
\centering
\begin{tabular}{p{0.45\textwidth} | p{0.45\textwidth}}
\hline
\textbf{Traditional Approaches} & \textbf{Deep Learning Approach} \\
\hline
\begin{itemize}
    \item Finite Difference Method (FDM)
    \item Finite Element Method (FEM)
    \item Fundamentally mesh-based
    \item Typically constrained to a fixed domain geometry
\end{itemize}
&
\begin{itemize}
    \item Mesh-free approach
    \item Relies on Automatic Differentiation
    \item Potentially breaks the curse of dimensionality
    \item Flexible handling of complex or moving domains
\end{itemize}
\\
\hline
\end{tabular}
\end{table}

The mesh-free nature of neural networks is particularly advantageous for problems in high dimensions or those with complex, evolving geometries, where mesh generation can become a significant bottleneck.

\section{Core Components of PINNs}

\subsection{Feed-Forward Neural Networks (FNN) as Function Approximators}

At its core, a PINN uses a standard Feed-Forward Neural Network (FNN) as a universal function approximator. The network learns a continuous function that represents the solution to the PDE.

\newtheorem{definition}{Definition  }
\begin{definition}
An $L$-layer neural network $\mathcal{N}^L(\mathbf{x}): \mathbb{R}^{d_{in}} \to \mathbb{R}^{d_{out}}$ with $N_l$ neurons in layer $l$ is defined recursively:
\begin{itemize}
    \item \textbf{Input Layer:} $\mathcal{N}^0(\mathbf{x}) = \mathbf{x} \in \mathbb{R}^{d_{in}}$
    \item \textbf{Hidden Layers:} For $l = 1, \dots, L-1$, the output of the $l$-th layer is:
    \begin{equation}
    \mathcal{N}^l(\mathbf{x}) = \sigma(\mathbf{W}^l \mathcal{N}^{l-1}(\mathbf{x}) + \mathbf{b}^l) \in \mathbb{R}^{N_l}
    \end{equation}
    where $\mathbf{W}^l \in \mathbb{R}^{N_l \times N_{l-1}}$ is the weight matrix and $\mathbf{b}^l \in \mathbb{R}^{N_l}$ is the bias vector. $\sigma(\cdot)$ is a non-linear activation function.
    \item \textbf{Output Layer:} The final output is typically a linear combination of the last hidden layer's activations:
    \begin{equation}
    \mathcal{N}^L(\mathbf{x}) = \mathbf{W}^L \mathcal{N}^{L-1}(\mathbf{x}) + \mathbf{b}^L \in \mathbb{R}^{d_{out}}
    \end{equation}
\end{itemize}
Common activation functions $\sigma(x)$ include Sigmoid, Hyperbolic Tangent (Tanh), and the Rectified Linear Unit (ReLU).
\end{definition}

\subsection{The Role of Automatic Differentiation}
The key innovation that enables PINNs is the use of Automatic Differentiation (AD). AD is a technique to numerically compute the exact derivatives of a function specified by a computer program. Since a neural network is a composition of differentiable functions, AD can be used to compute any derivative of the network's output $\hat{u}(\mathbf{x}; \theta)$ with respect to its inputs $\mathbf{x}$. This allows for the direct and accurate evaluation of the PDE residual, which involves terms like $\frac{\partial \hat{u}}{\partial x_i}$ and $\frac{\partial^2 \hat{u}}{\partial x_i \partial x_j}$, without resorting to numerical approximations like finite differences.

\section{The PINN Algorithm}

\subsection{General PDE Formulation}
Consider a general PDE parameterized by $\lambda$, defined over a domain $\Omega \subset \mathbb{R}^d$:
\begin{equation}
f\left(\mathbf{x}, u(\mathbf{x}), \nabla u(\mathbf{x}), \nabla^2 u(\mathbf{x}), \dots; \lambda\right) = 0, \quad \mathbf{x} \in \Omega
\end{equation}
subject to boundary conditions on $\partial\Omega$:
\begin{equation}
\mathcal{B}(u, \mathbf{x}) = 0, \quad \mathbf{x} \in \partial\Omega
\end{equation}
Here, $u(\mathbf{x})$ is the unknown solution, and $\mathcal{B}$ is a boundary operator (e.g., Dirichlet, Neumann). For time-dependent problems, time $t$ is treated as an additional component of the input vector $\mathbf{x}$.

\subsection{Data vs. Physics Trade-off}
PINNs are uniquely positioned to handle problems across a spectrum of data availability. The balance between reliance on data and reliance on physical laws can be conceptualized as follows:
\begin{itemize}
    \item \textbf{Big Data Regime:} With a large amount of labeled data, the physical laws may not be strictly necessary, as the network can infer the solution directly from the data (traditional supervised learning).
    \item \textbf{Small Data Regime:} When data is scarce or unavailable, the model must rely heavily on the governing physical equations to find a solution.
    \item \textbf{Intermediate Regime:} The most common and powerful use case for PINNs is where some data and some knowledge of the physics are available. The two sources of information work together to constrain the solution space.
\end{itemize}

\subsection{PINNs Algorithm and Loss Function}
The PINN algorithm trains a neural network $\hat{u}(\mathbf{x}; \theta)$ to approximate the true solution $u(\mathbf{x})$ by minimizing a composite loss function.

The overall scheme is as follows:
\begin{enumerate}
    \item \textbf{Construct a surrogate model:} Define a neural network $\hat{u}(\mathbf{x}; \theta)$ that takes coordinates as input and outputs the predicted solution. The parameters of the network are denoted by $\theta$.
    \item \textbf{Define a composite loss function:} The total loss $L(\theta)$ is a weighted sum of three distinct components:
    \begin{equation}
    L(\theta) = w_f L_f(\theta) + w_b L_b(\theta) + w_u L_u(\theta)
    \end{equation}
    where $w_f, w_b, w_u$ are weights.
    
    \begin{itemize}
        \item \textbf{PDE Residual Loss ($L_f$):} This term enforces the physical law. It is the mean squared error of the PDE residual evaluated at a set of collocation points $\mathcal{T}_f$ sampled from the domain $\Omega$:
        \begin{equation}
        L_f(\theta) = \frac{1}{|\mathcal{T}_f|} \sum_{\mathbf{x} \in \mathcal{T}_f} \left\| f\left(\mathbf{x}, \hat{u}(\mathbf{x};\theta), \dots \right) \right\|^2_2
        \end{equation}
        The derivatives within $f$ are computed using Automatic Differentiation.
        
        \item \textbf{Boundary/Initial Condition Loss ($L_b$):} This term enforces the boundary and/or initial conditions. It is the mean squared error of the boundary operator evaluated at points $\mathcal{T}_b$ on the boundary $\partial\Omega$:
        \begin{equation}
        L_b(\theta) = \frac{1}{|\mathcal{T}_b|} \sum_{\mathbf{x} \in \mathcal{T}_b} \left\| \mathcal{B}(\hat{u}(\mathbf{x};\theta), \mathbf{x}) \right\|^2_2
        \end{equation}
        
        \item \textbf{Labeled Data Loss ($L_u$):} If labeled data points (measurements) are available, this term measures the mismatch between the network's prediction and the true values:
        \begin{equation}
        L_u(\theta) = \frac{1}{|\mathcal{T}_u|} \sum_{(\mathbf{x}_i, u_i) \in \mathcal{T}_u} \left\| \hat{u}(\mathbf{x}_i;\theta) - u_i \right\|^2_2
        \end{equation}
    \end{itemize}
    
    \item \textbf{Train the network:} Minimize the total loss function $L(\theta)$ with respect to the network parameters $\theta$ using a gradient-based optimizer (e.g., Adam, L-BFGS). The optimal parameters are found via:
    \begin{equation}
    \theta^* = \arg \min_{\theta} L(\theta)
    \end{equation}
\end{enumerate}

This training process constitutes a feedback mechanism where the residuals from the physical laws and data continuously guide the network's parameters toward a state that satisfies all constraints simultaneously.

\section{Error Analysis and Key Considerations}

\subsection{Error Decomposition}
The total error in a PINN solution can be decomposed into three primary components, which illustrates the journey from the true, unknown solution $u$ to the final trained network output $\tilde{u}_T$.

\begin{center}
    \fbox{
        \begin{picture}(300,60)
            \put(10,30){$u$}
            \put(25,32){\vector(1,0){50}}
            \put(30,40){Approximation}
            \put(40,20){error $\mathcal{E}_{app}$}
            
            \put(80,30){$u_{\mathcal{F}}$}
            \put(95,32){\vector(1,0){50}}
            \put(100,40){Generalization}
            \put(110,20){error $\mathcal{E}_{gen}$}
            
            \put(150,30){$u_T$}
            \put(165,32){\vector(1,0){50}}
            \put(170,40){Optimization}
            \put(180,20){error $\mathcal{E}_{opt}$}
            
            \put(220,30){$\tilde{u}_T$}
        \end{picture}
    }
\end{center}
The total error $E$ is bounded by the sum of these components:
\begin{equation}
    E := \| \tilde{u}_T - u \| \le \underbrace{\| \tilde{u}_T - u_T \|}_{\text{Opt. error}} + \underbrace{\| u_T - u_{\mathcal{F}} \|}_{\text{Gen. error}} + \underbrace{\| u_{\mathcal{F}} - u \|}_{\text{App. error}}
\end{equation}

\begin{itemize}
    \item \textbf{Approximation error ($\mathcal{E}_{app}$):} This error measures how well the chosen function family $\mathcal{F}$ (i.e., the neural network architecture) can approximate the true solution $u$.
    \item \textbf{Generalization error ($\mathcal{E}_{gen}$):} This error depends on the number of residual points used for training and the network's capacity. It reflects the model's ability to generalize from the discrete training points to the continuous domain.
    \item \textbf{Optimization error ($\mathcal{E}_{opt}$):} This error arises from the inability of the gradient-based optimizer to find the global minimum of the loss function, often getting stuck in local minima.
\end{itemize}

\subsection{Error Analysis: Key Points}
Several practical considerations arise from this error decomposition:
\begin{itemize}
    \item \textbf{Approximation Error:} Smaller networks (fewer layers or neurons) have limited expressive power, leading to larger approximation errors.
    \item \textbf{Generalization Error:} Larger networks, while reducing approximation error, may lead to overfitting. This is a manifestation of the classic bias-variance tradeoff.
    \item \textbf{No Current Error Bounds:} Unlike FEM, where a priori error estimates are often available, there are currently no guaranteed error bounds for PINNs.
    \item \textbf{Non-uniqueness:} Due to the non-convex nature of the loss landscape, PINNs may converge to different solutions depending on the random initialization of weights. A common practice is to train the network multiple times and select the best-performing model.
    \item \textbf{Hyperparameter Tuning:} Achieving good performance with PINNs requires careful tuning of several hyperparameters, including network size (depth and width), the number and distribution of residual points, the learning rate, and the weights for the different loss components ($w_f, w_b$).
\end{itemize}

\section{Comparison and Extensions of PINNs}

\subsection{PINNs vs. Finite Element Method (FEM)}
PINNs and FEM represent fundamentally different approaches to solving PDEs.

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{PINN} & \textbf{FEM} \\
\hline
\textbf{Basis function} & Neural network (nonlinear) & Polynomial (linear) \\
\textbf{Parameters} & Weights and biases & Point values \\
\textbf{Training points} & Scattered (mesh-free) & Mesh points \\
\textbf{PDE embedding} & Loss function & Algebraic system \\
\textbf{Parameter solver} & Gradient-based optimizer & Linear solver \\
\textbf{Errors} & $E_{app}, E_{gen}, E_{opt}$ & Approximation/quadrature \\
\textbf{Error bounds} & Not available yet & Partially available \\
\hline
\end{tabular}
\caption{Comparison of PINN and FEM approaches.}
\end{table}

The fundamental difference is that PINNs use \textbf{nonlinear} function approximation, whereas FEM relies on \textbf{linear} approximation through piecewise polynomial basis functions.

\subsection{Extensions to Other Equation Types}
The PINN framework is flexible and can be extended beyond standard PDEs.

\subsubsection{Integro-Differential Equations (IDEs)}
Consider an IDE with an integral term:
\begin{equation}
    \frac{dy}{dx} + y(x) = \int_0^x e^{t-x} y(t) dt
\end{equation}
The PINN approach handles this by:
\begin{enumerate}
    \item Using Automatic Differentiation (AD) for the analytical derivative term $\frac{\partial \hat{u}}{\partial x}$.
    \item Approximating the integral term numerically, for example, using Gaussian quadrature.
\end{enumerate}
This introduces a new error component, the \textbf{discretization error} ($\mathcal{E}_{dis}$), from the numerical integration. The discretization example is:
\begin{equation}
    \int_0^x e^{t-x} y(t) dt \approx \sum_{i=1}^n w_i e^{t_i(x)-x} y(t_i(x))
\end{equation}
This same approach can be extended to solve Fractional Differential Equations (FDEs) and Stochastic Differential Equations (SDEs).

\subsection{Residual-Based Adaptive Refinement (RAR)}
For PDEs with solutions containing steep gradients or sharp fronts, uniform sampling of residual points is inefficient. The idea behind RAR is to add more residual points in regions where the PDE residual is large.

The RAR algorithm follows these steps:
\begin{enumerate}
    \item Select an initial set of residual points and train for a limited number of iterations.
    \item Estimate the mean PDE residual, for instance, via Monte Carlo sampling.
    \item If the mean residual is greater than a threshold $E_0$, add $m$ new points in the regions with the largest residuals.
    \item Repeat the process until convergence.
\end{enumerate}
This adaptive sampling strategy allows the model to focus on high-residual regions, leading to a better capture of discontinuities and sharp fronts, such as those in the Burgers' equation.

\section{Hard vs. Soft Boundary Conditions}

In the context of Physics-Informed Neural Networks (PINNs), the enforcement of boundary conditions (BCs) is a critical step. The approach taken to impose these conditions significantly impacts the network's training, flexibility, and accuracy. There are two primary methods for this: soft constraints and hard constraints.

\subsection{Soft Constraints}
The most common method for enforcing boundary conditions in PINNs is through \textbf{soft constraints}. This approach incorporates the boundary conditions into the training process by defining a specific loss term, denoted as $L_b$. The total loss function to be minimized is a weighted sum of the physics-based residual loss ($L_f$) and this boundary condition loss ($L_b$):
\begin{equation}
L(\theta) = w_f L_f(\theta) + w_b L_b(\theta)
\end{equation}
Here, $L_b$ measures the mean squared error between the neural network's output and the true boundary values at a set of sampled boundary points. By minimizing this term, the network is incentivized, but not forced, to satisfy the boundary conditions.

This method offers several key advantages:
\begin{itemize}
    \item \textbf{Flexibility and Generality:} It is straightforward to apply and does not require any modification to the neural network architecture.
    \item \textbf{Complex Domains:} It works well for problems with intricate or complex geometries, where defining manual constraints would be infeasible.
    \item \textbf{Versatility:} It can handle any type of boundary condition, including Dirichlet, Neumann, Robin, and periodic conditions, simply by defining the appropriate residual in the loss term.
\end{itemize}
The main drawback is that this "soft" imposition does not guarantee that the boundary conditions will be met exactly. The final solution will only approximate the BCs to a precision determined by the optimization process and the weighting of the loss term.

\subsection{Hard Constraints}
An alternative approach is to use \textbf{hard constraints}, which involves manually constructing the neural network's output to ensure that the boundary conditions are satisfied automatically by construction. This reformulates the network's output, or \textit{ansatz}, so that it is mathematically guaranteed to fulfill the BCs, regardless of the values of the network's trainable parameters.

For example, consider a 1D problem on the domain $x \in [0, 1]$ with homogeneous Dirichlet boundary conditions, i.e., $u(0) = 0$ and $u(1) = 0$. Let $N(x; \theta)$ be the direct output of a neural network. We can construct an approximate solution $\hat{u}(x)$ as:
\begin{equation}
\hat{u}(x) = x(x-1) N(x; \theta)
\end{equation}
By multiplying the network output by the term $x(x-1)$, which vanishes at $x=0$ and $x=1$, we ensure that $\hat{u}(0) = 0$ and $\hat{u}(1) = 0$ for any function $N(x; \theta)$ produced by the network. This technique is analogous to using a \textit{lifting operator} in traditional numerical methods for partial differential equations (PDEs).

The advantages of this method are:
\begin{itemize}
    \item \textbf{Automatic Satisfaction:} The BCs are perfectly satisfied by construction.
    \item \textbf{Reduced Degrees of Freedom:} The optimizer no longer needs to learn how to satisfy the boundary conditions, potentially simplifying the optimization landscape.
\end{itemize}
However, hard constraints have significant limitations:
\begin{itemize}
    \item \textbf{Simplicity Required:} They are generally only feasible for simple geometries (e.g., rectangles, circles) and simple, homogeneous BCs (i.e., where the boundary value is zero).
    \item \textbf{Complexity:} Formulating the correct ansatz for non-homogeneous BCs or complex domains can be extremely difficult or impossible.
\end{itemize}

\subsection{Best Practice}
For general-purpose applications, \textbf{soft constraints are the preferred method} due to their flexibility and ease of implementation. If the solution's accuracy on the boundary is insufficient, one can typically improve it by increasing the number of training points sampled on the boundary. Hard constraints should be reserved for specific cases where the domain and BCs are simple enough to allow for a straightforward analytical construction.

\section{Summary: Key Takeaways on PINNs}

This lecture has provided an overview of Physics-Informed Neural Networks, a key development in the emerging field of Scientific Machine Learning. The main points are summarized below.

\begin{itemize}
    \item \textbf{Core Concept:} PINNs embed PDEs directly into the neural network's loss function. This is made possible by automatic differentiation, which allows for the computation of derivatives of the network's output with respect to its inputs, enabling the evaluation of the PDE residual.
    
    \item \textbf{Advantages:}
    \begin{itemize}
        \item PINNs are \textbf{mesh-free} and highly flexible, making them suitable for complex geometries.
        \item The framework naturally \textbf{unifies forward and inverse problems}. The same algorithm can be used to solve a PDE (forward problem) or to infer unknown physical parameters from data (inverse problem).
        \item The methodology extends to a wide variety of PDE types.
    \end{itemize}
    
    \item \textbf{Theoretical Foundation:} The theoretical justification for PINNs lies in universal approximation theorems, which state that a neural network can approximate any continuous function to an arbitrary degree of accuracy. The total error can be decomposed into approximation, generalization, and optimization errors. A major current limitation is the \textbf{lack of guaranteed error bounds}, which distinguishes PINNs from classical numerical methods like the Finite Element Method (FEM).
    
    \item \textbf{Training Challenges:}
    \begin{itemize}
        \item Training PINNs involves navigating a \textbf{non-convex optimization} landscape, which can be challenging.
        \item Significant \textbf{hyperparameter tuning} is often required (e.g., network size, loss term weights, learning rate).
        \item Advanced methods like \textbf{Residual-based Adaptive Refinement (RAR)} can improve training efficiency by focusing sampling points in regions of high PDE residual.
    \end{itemize}
    
    \item \textbf{Emerging Field: Scientific Machine Learning:} PINNs are a cornerstone of Scientific Machine Learning, a field that bridges classical numerical methods with modern deep learning. This synergy is creating powerful new tools for scientific discovery. Libraries like \textbf{DeepXDE} are democratizing this field by providing well-documented, easy-to-use implementations that support both TensorFlow and PyTorch backends. Other high-performance libraries, such as those from NVIDIA, are also available but may require specialized hardware.
\end{itemize}

\end{document}