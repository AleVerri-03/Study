{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7q1DhWpV-Dw"
   },
   "source": [
    "# Auto-diff with JAX\n",
    "\n",
    "JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning.\n",
    "\n",
    "See:\n",
    "\n",
    "- https://github.com/google/jax\n",
    "- https://jax.readthedocs.io/en/latest/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX's syntax is (for the most part) same as NumPy's!\n",
    "# There is also a SciPy API support (jax.scipy)\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# JAX's low level API\n",
    "# (lax is just an anagram for XLA, not completely sure how they came up with name JAX)\n",
    "from jax import lax\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX's augmented numpy lives at `jax.numpy`. With a few exceptions, you can think of `jax.numpy` as directly interchangeable with `numpy`. As a general rule, you should use `jax.numpy` whenever you plan to use any of JAX's transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact 1: JAX's syntax is remarkably similar to NumPy's\n",
    "x_jnp = jnp.linspace(0, 10, 1000)\n",
    "y_jnp = 2 * jnp.sin(x_jnp) * jnp.cos(x_jnp)\n",
    "plt.plot(x_jnp, y_jnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact 2: JAX arrays are immutable!\n",
    "size = 10\n",
    "index = 0\n",
    "value = 23\n",
    "\n",
    "# In NumPy arrays are mutable\n",
    "x = np.arange(size)\n",
    "print(x)\n",
    "x[index] = value\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In JAX we have to deal with immutable arrays\n",
    "x = jnp.arange(size)\n",
    "print(x)\n",
    "x[index] = value  # ERROR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this seems wasteful to you, it is indeed true (in normal settings).\n",
    "# The thing is, if the input value x of x.at[idx].set(y) is not reused,\n",
    "# you can tell JAX to optimize the array update to occur in-place.\n",
    "# We will see the details after\n",
    "\n",
    "jax_array = jnp.zeros((3, 3), dtype=jnp.float32)\n",
    "updated_array = jax_array.at[1, :].set(1.0)\n",
    "\n",
    "print(\"original array unchanged:\\n\", jax_array)\n",
    "print(\"updated array:\\n\", updated_array)\n",
    "\n",
    "# The expresiveness of NumPy is still there!\n",
    "\n",
    "print(\"new array post-addition:\")\n",
    "new_jax_array = jax_array.at[::2, 1:].add(7.0)\n",
    "print(new_jax_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact 3: JAX handles random numbers differently (for the same reason arrays are immutable)\n",
    "seed = 0\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "x = jax.random.normal(key, (10,))  # you need to explicitly pass the key i.e. PRNG state\n",
    "print(type(x), x)  # notice the DeviceArray type - that leads us to the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact 4: JAX is AI accelerator agnostic. Same code runs everywhere!\n",
    "\n",
    "size = 3000\n",
    "\n",
    "# Data is automatically pushed to the AI accelerator (GPU, TPU)\n",
    "x_jnp = jax.random.normal(key, (size, size), dtype=jnp.float32)\n",
    "x_np = np.random.normal(size=(size, size)).astype(np.float32)  # some diff in API exists!\n",
    "\n",
    "print(\"[1] GPU\")\n",
    "%timeit jnp.dot(x_jnp, x_jnp.T).block_until_ready()  # 1) on GPU - fast\n",
    "print(\"[2] Pure numpy (CPU)\")\n",
    "%timeit np.dot(x_np, x_np.T)  # 2) on CPU - slow (NumPy only works with CPUs)\n",
    "print(\"[3] GPU + data transfer\")\n",
    "%timeit jnp.dot(x_np, x_np.T).block_until_ready()  # 3) on GPU with transfer overhead\n",
    "\n",
    "x_np_device = jax.device_put(x_np)  # push NumPy explicitly to GPU\n",
    "print(\"[4] GPU + explicit pre-data transfer (like [1] but explicit)\")\n",
    "%timeit jnp.dot(x_np_device, x_np_device.T).block_until_ready()  # same as 1)\n",
    "\n",
    "# Note1: I'm using GPU as a synonym for AI accelerator. \n",
    "# In reality, especially in Colab, this can also be a TPU, etc.\n",
    "\n",
    "# Note2: block_until_ready() -> asynchronous dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX transform functions\n",
    "\n",
    "`jit` compiles your functions using XLA and caches them to reach the best speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fn(fn, l=-10, r=10, n=1000):\n",
    "  x = np.linspace(l, r, num=n)\n",
    "  y = fn(x)\n",
    "  plt.plot(x, y); plt.show()\n",
    "\n",
    "# Define a function\n",
    "def selu(x, alpha=1.67, lmbda=1.05):  # note: SELU is an activation function\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jax.jit(selu)  # let's jit it\n",
    "\n",
    "# Visualize SELU (just for your understanding, it's always a good idea to visualize stuff)\n",
    "visualize_fn(selu)\n",
    "\n",
    "# Benchmark non-jit vs jit version\n",
    "data = jax.random.normal(key, (1000000,))\n",
    "\n",
    "print('non-jit version:')\n",
    "%timeit selu(data).block_until_ready()\n",
    "print('jit version:')\n",
    "%timeit selu_jit(data).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Snjf4u7vf0sa"
   },
   "source": [
    "## Automatic differentiation in JAX\n",
    "\n",
    "JAX augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs.\n",
    "\n",
    "The function `df = jax.grad(f, argnums = 0)` takes the callable object `f` and returns another callable object, `df`, evaluating the gradient of `f` w.r.t. the argument(s) of index(es) `argnums`. For more information, check out the [documentation](https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.grad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1668116008599,
     "user_tz": -60
    },
    "id": "pJsIHEuC0BwB",
    "outputId": "1d82508a-2f51-48ee-96cd-66cf85dce20e"
   },
   "outputs": [],
   "source": [
    "x = 1.0  # example input\n",
    "\n",
    "f = lambda x: x**2 + x + 4  # simple 2nd order polynomial fn\n",
    "visualize_fn(f, l=-1, r=2, n=100)\n",
    "\n",
    "dfdx = jax.grad(f)  # 2*x + 1\n",
    "d2fdx = jax.grad(dfdx)  # 2\n",
    "d3fdx = jax.grad(d2fdx)  # 0\n",
    "\n",
    "print(f(x), dfdx(x), d2fdx(x), d3fdx(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we had two inputs?\n",
    "\n",
    "We use `jacrev` and `jacfwd`:\n",
    "\n",
    "These two functions compute the same values (up to machine numerics), but differ in their implementation: `jacfwd` uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while `jacrev` uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, `jacfwd` probably has an edge over `jacrev`.\n",
    "\n",
    "To implement hessian, we could have used `jacfwd(jacrev(f))` or `jacrev(jacfwd(f))` or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function $f: \\mathbb{R}^n \\to \\mathbb{R}$), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since $\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n$), which is where forward-mode wins out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x, y: x**2 + y**2  # simple paraboloid\n",
    "\n",
    "# df/dx = 2x\n",
    "# df/dy = 2y\n",
    "# J = [df/dx, df/dy]\n",
    "\n",
    "# d2f/dx = 2\n",
    "# d2f/dy = 2\n",
    "# d2f/dxdy = 0\n",
    "# d2f/dydx = 0\n",
    "# H = [[d2f/dx, d2f/dxdy], [d2f/dydx, d2f/dy]]\n",
    "\n",
    "\n",
    "def eval_hessian(f):\n",
    "    return jax.jit(jax.jacfwd(jax.jacrev(f, argnums=(0, 1)), argnums=(0, 1)))\n",
    "\n",
    "\n",
    "jacobian = jax.jacrev(f, argnums=(0, 1))(1.0, 1.0)\n",
    "hessian = eval_hessian(f)(1.0, 1.0)\n",
    "print(f\"Jacobian = {np.asarray(jacobian)}\")\n",
    "print(f\"Hessian = {np.asarray(hessian)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naturally, the function passed to grad can work with array inputs\n",
    "f = lambda x: x[0] * x[0] + x[1] * x[1]\n",
    "\n",
    "\n",
    "def eval_hessian(f):\n",
    "    return jax.jit(jax.jacfwd(jax.jacrev(f)))\n",
    "\n",
    "\n",
    "x0 = jnp.array([2.0, 1.0])\n",
    "jacobian = jax.jacrev(f)(x0)\n",
    "hessian = eval_hessian(f)(x0)\n",
    "print(f\"Jacobian = {np.asarray(jacobian)}\")\n",
    "print(f\"Hessian = {np.asarray(hessian)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge case |x|, how does JAX handle it?\n",
    "\n",
    "f = lambda x: abs(x)\n",
    "visualize_fn(f)\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "print(f\"dfdx(0.0)   = {dfdx(0.0)  :.17e}\")\n",
    "print(f\"dfdx(+1e-5) = {dfdx(+1e-5):.17e}\")\n",
    "print(f\"dfdx(-1e-5) = {dfdx(-1e-5):.17e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize functions with `vmap()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dot(x, y):\n",
    "    return jnp.dot(x, y) ** 2\n",
    "\n",
    "def naive_custom_dot(x_batched, y_batched):\n",
    "    return jnp.stack([\n",
    "        custom_dot(v1, v2)\n",
    "        for v1, v2 in zip(x_batched, y_batched)\n",
    "    ])\n",
    "\n",
    "@jax.jit\n",
    "def jit_naive_custom_dot(x_batched, y_batched):\n",
    "    return jnp.stack([\n",
    "        custom_dot(v1, v2)\n",
    "        for v1, v2 in zip(x_batched, y_batched)\n",
    "    ])\n",
    "\n",
    "batched_custom_dot = jax.vmap(custom_dot, in_axes=[0, 0])\n",
    "jit_batched_custom_dot = jax.jit(jax.vmap(custom_dot, in_axes=[0, 0]))\n",
    "\n",
    "x = jnp.asarray(np.random.rand(1000, 50))\n",
    "y = jnp.asarray(np.random.rand(1000, 50))\n",
    "\n",
    "print(\"Naive\")\n",
    "%timeit naive_custom_dot(x, y)\n",
    "print(\"Vectorized\")\n",
    "%timeit batched_custom_dot(x, y)\n",
    "print(\"JIT\")\n",
    "%timeit jit_naive_custom_dot(x, y)\n",
    "print(\"Vectorized + JIT\")\n",
    "%timeit jit_batched_custom_dot(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nitty-gritty: JAX API structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: lax is stricter\n",
    "\n",
    "print(jnp.add(1, 1.0))  # jax.numpy API implicitly promotes mixed types\n",
    "print(lax.add(1.0, 1.0))  # works\n",
    "print(lax.add(1, 1.0))  # ERROR! jax.lax API requires explicit type promotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: lax is more powerful (but as a tradeoff less user-friendly)\n",
    "\n",
    "x = jnp.array([1, 2, 1])\n",
    "y = jnp.ones(10)\n",
    "\n",
    "# NumPy API\n",
    "result1 = jnp.convolve(x, y)\n",
    "\n",
    "# lax API\n",
    "result2 = lax.conv_general_dilated(\n",
    "    x.reshape(1, 1, 3).astype(float),  # note: explicit promotion\n",
    "    y.reshape(1, 1, 10),\n",
    "    window_strides=(1,),\n",
    "    padding=[(len(y) - 1, len(y) - 1)],\n",
    ")  # equivalent of padding='full' in NumPy\n",
    "\n",
    "print(result1)\n",
    "print(result2[0][0])\n",
    "assert np.allclose(result1, result2[0][0], atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha #1: The limitations of JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used boolean mask extensively, what happens if we use them in JAX?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(x):\n",
    "    return x[x < 0]\n",
    "\n",
    "\n",
    "x = jax.random.normal(key, (10,), dtype=jnp.float32)\n",
    "print(get_negatives(x))\n",
    "print(jax.jit(get_negatives)(x))  # ERROR!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue comes from the fact that `x[x < 0]` returns an array whose size depends on the values within x; another way of saying this is that `x[x < 0]` has a dynamic shape. JAX's transform model does not currently support dynamically-shaped arrays, and the result is the error you see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So how does it work in the background? -> tracing on different levels of abstraction\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, y):\n",
    "    print(\"Running f():\")\n",
    "    print(f\"  x = {x}\")\n",
    "    print(f\"  y = {y}\")\n",
    "    result = jnp.dot(x + 1, y + 1)\n",
    "    print(f\"  result = {result}\")\n",
    "    return result\n",
    "\n",
    "\n",
    "x = np.random.randn(3, 4)\n",
    "y = np.random.randn(4)\n",
    "print(f(x, y))  # The first call has the overhead of compiling the function\n",
    "\n",
    "\n",
    "x2 = np.random.randn(3, 4)\n",
    "y2 = np.random.randn(4)\n",
    "print(\"Second call:\")\n",
    "print(f(x2, y2))  # Oops! Side effects (like print) are not compiled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same function as above just without the print statements\n",
    "def f(x, y):\n",
    "    return jnp.dot(x + 1, y + 1)\n",
    "\n",
    "\n",
    "print(jax.make_jaxpr(f)(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd example of a failure:\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, neg):  # result depends on the value - remember tracer cares about shapes and types!\n",
    "    return -x if neg else x\n",
    "\n",
    "@jax.jit\n",
    "def f2(x, neg):\n",
    "    return x * (1.0 - 2.0 * neg)\n",
    "\n",
    "f2(x, False)\n",
    "f(1, True) # ERROR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround: the \"static\" arguments\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(1,))\n",
    "def f(x, neg):\n",
    "    print(x)\n",
    "    return -x if neg else x\n",
    "\n",
    "\n",
    "print(f(1, True))\n",
    "print(f(2, True))\n",
    "# Here we do another jit compilation\n",
    "print(f(2, False))\n",
    "print(f(23, False))\n",
    "# Here we do another jit compilation\n",
    "print(f(44, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is designed to work only on pure functions!\n",
    "\n",
    "Pure function? Informal definition:\n",
    "\n",
    "- All the input data is passed through the function parameters, all the results are output through the function results.\n",
    "- A pure function will always return the same result if invoked with the same inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3\n",
    "\n",
    "g = 0.0\n",
    "\n",
    "\n",
    "def impure_uses_globals(x):\n",
    "    return x + g  # Violating both #1 and #2\n",
    "\n",
    "\n",
    "# JAX captures the value of the global during the first run\n",
    "print(\"First call: \", jax.jit(impure_uses_globals)(4.0))\n",
    "\n",
    "# Let's update the global!\n",
    "g = 10.0\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print(\"Second call: \", jax.jit(impure_uses_globals)(5.0))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print(\"Third call, different type: \", jax.jit(impure_uses_globals)(jnp.array([4.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha #2: Out-of-Bounds Indexing\n",
    "\n",
    "Due to JAX's accelerator agnostic approach JAX had to make a non-error behaviour for out of bounds indexing (similarly to how invalid fp arithmetic results in NaNs and not an exception).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX behavior\n",
    "# 1) updates at out-of-bounds indices are skipped\n",
    "# 2) retrievals result in index being clamped\n",
    "# in general there are currently some bugs so just consider the behavior undefined!\n",
    "\n",
    "print(jnp.arange(10).at[11].add(23))  # example of 1)\n",
    "print(jnp.arange(10)[11])  # example of 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha #3: random numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy - PRNG is stateful!\n",
    "seed = 0\n",
    "\n",
    "# Let's sample calling the same function twice\n",
    "np.random.seed(seed)\n",
    "\n",
    "rng_state = np.random.get_state()\n",
    "print(\"numpy random state\", rng_state[2:])\n",
    "\n",
    "print(\"A random number\", np.random.random())\n",
    "rng_state = np.random.get_state()\n",
    "print(\"numpy random state\",rng_state[2:])\n",
    "\n",
    "print(\"A random number\", np.random.random())\n",
    "rng_state = np.random.get_state()\n",
    "print(\"numpy random state\",rng_state[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX's random functions can't modify PRNG's state!\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "print(\"JAX rng state\", key)  # key defines the state (2 unsigned int32s)\n",
    "\n",
    "# Let's again sample calling the same function twice\n",
    "print(\"A random number\", jax.random.normal(key, shape=(1,)))\n",
    "print(\"JAX rng state\",key)  # verify that the state hasn't changed\n",
    "\n",
    "print(\"A random number\", jax.random.normal(key, shape=(1,)))  # oops - same results?\n",
    "print(\"JAX rng state\",key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution? -> Split every time you need a pseudorandom number.\n",
    "\n",
    "print(\"old key\", key)\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "normal_pseudorandom = jax.random.normal(subkey, shape=(1,))\n",
    "print(\"    \\---SPLIT --> new key   \", key)\n",
    "print(\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this design?\n",
    "With current design can the code be easily reproducible in a parallel environment?\n",
    "\n",
    "E.g. imagine two functions that run on two different CPU/GPU (in a parallel env, where you are separately training a neural network), than the result may depend on who first calls the np.random.uniform between the two CPU/GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "# simulate the training on CPU/GPU #1\n",
    "def worker1(result_container):\n",
    "    for _ in range(10):\n",
    "        result_container[\"worker1\"].append(np.random.uniform())\n",
    "        time.sleep(0.001)\n",
    "\n",
    "# simulate the training on CPU/GPU #2\n",
    "def worker2(result_container):\n",
    "    for _ in range(10):\n",
    "        result_container[\"worker2\"].append(np.random.uniform())\n",
    "        time.sleep(0.002)\n",
    "\n",
    "\n",
    "def do_parallel_work():\n",
    "    result = {\"worker1\": [], \"worker2\": []}\n",
    "    t1 = threading.Thread(target=worker1, args=(result,))\n",
    "    t2 = threading.Thread(target=worker2, args=(result,))\n",
    "\n",
    "    # Start both threads \"at the same time\"\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "\n",
    "    # Wait for both to finish\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "\n",
    "    # combine the work\n",
    "    return (np.array(result[\"worker1\"]) + 2 * np.array(result[\"worker2\"])).sum()\n",
    "\n",
    "\n",
    "# What if we want to parallelize this code? NumPy assumes too much.\n",
    "# Run do_parallel_work() multiple times to show non-determinism\n",
    "for i in range(5):\n",
    "    np.random.seed(42)\n",
    "    print(f\"Run {i+1}: {do_parallel_work()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gotcha #4: Control Flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python control flow + grad() -> everything is ok\n",
    "def f(x):\n",
    "    if x < 3:\n",
    "        return 3.0 * x**2\n",
    "    else:\n",
    "        return -4 * x\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = [f(el) for el in x]\n",
    "\n",
    "print(jax.grad(f)(2.0))  # ok!\n",
    "print(jax.grad(f)(4.0))  # ok!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python control flow + jit() -> problems in paradise.\n",
    "\n",
    "# \"The tradeoff is that with higher levels of abstraction we gain a more general view\n",
    "# of the Python code (and thus save on re-compilations),\n",
    "# but we require more constraints on the Python code to complete the trace.\"\n",
    "\n",
    "# Solution (recall: we already have seen this)\n",
    "f_jit = jax.jit(f, static_argnums=(0,))\n",
    "x = 2.0\n",
    "\n",
    "print(jax.make_jaxpr(f_jit, static_argnums=(0,))(x))\n",
    "print(f_jit(x))\n",
    "\n",
    "# WARNING: still for each x we have to jit a new function, this might be expensive!\n",
    "\n",
    "f_jit_error = jax.jit(f)\n",
    "f_jit_error(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# native jax functions give you powerfull alternatives\n",
    "\n",
    "def f(x):\n",
    "    return jnp.where(x < 3.0, 3.0 * x**2, -4 * x)\n",
    "\n",
    "f_jit = jax.jit(f)\n",
    "fgrad_jit = jax.jit(jax.grad(f))\n",
    "\n",
    "print(f_jit(2.0))\n",
    "print(f_jit(4.0))\n",
    "print(fgrad_jit(2.0))\n",
    "print(fgrad_jit(4.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: range depends on value again\n",
    "\n",
    "\n",
    "def f(x, n):\n",
    "    y = 0.0\n",
    "    for i in range(n):\n",
    "        y = y + x[i]\n",
    "    return y\n",
    "\n",
    "\n",
    "f_jit = jax.jit(f, static_argnums=(1,))\n",
    "x = (jnp.array([2.0, 3.0, 4.0]), 3)\n",
    "\n",
    "print(jax.make_jaxpr(f_jit, static_argnums=(1,))(*x))  # notice how for loop gets unrolled\n",
    "print(f_jit(*x))\n",
    "\n",
    "# Note: there is a catch - static args should not change a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even \"better\" solution is to use low level API\n",
    "def f_fori(x, n):\n",
    "    body_fun = lambda i, val: val + x[i]\n",
    "    return lax.fori_loop(0, n, body_fun, 0.0)\n",
    "\n",
    "\n",
    "f_fori_jit = jax.jit(f_fori)\n",
    "\n",
    "print(jax.make_jaxpr(f_fori_jit)(*x))\n",
    "print(f_fori_jit(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other gotchas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to debug where the NaNs are coming from, there are multiple ways\n",
    "# to do that, here is one:\n",
    "jax.config.update(\"jax_debug_nans\", False)  # Change this flag and re-run the cell\n",
    "jnp.divide(0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX enforces single precision! There are simple ways around it though.\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "x = jax.random.uniform(key, (1000,), dtype=jnp.float64)\n",
    "print(x.dtype)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBXNhzemTUvVqvaZ1aOpnF",
   "collapsed_sections": [],
   "mount_file_id": "1aDiop5c-V90dbJAUkJjgfpwZHa4vvdOa",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
